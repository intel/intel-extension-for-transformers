{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intel Extension for Transformers provides a comprehensive suite of Langchain-based extension APIs, including advanced retrievers, embedding models, and vector stores. These enhancements are carefully crafted to expand the capabilities of the original Langchain API, ultimately boosting overall performance. This extension is specifically tailored to enhance the functionality and performance of RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install intel extension for transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install intel-extension-for-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "%cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/\n",
    "!pip install -r requirements.txt\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LLM with Langchain-extension API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've revamped the Chroma API, enabling users to adjust and fine-tune their settings even after the chatbot has been initialized, offering a more adaptable and user-friendly experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o Intel_AR_WR.pdf https://d1io3yog0oux5.cloudfront.net/_897efe2d574a132883f198f2b119aa39/intel/db/888/8941/file/412439%281%29_12_Intel_AR_WR.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot code with Langchain APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "loader = PyPDFLoader(\"./Intel_AR_WR.pdf\")\n",
    "langchain_documents = loader.load_and_split()\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "knowledge_base = Chroma.from_documents(documents=langchain_documents, embedding=embeddings, persist_directory='./out')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "pipe = HuggingFacePipeline(pipeline=pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128))\n",
    "retriever = VectorStoreRetriever(vectorstore=knowledge_base, search_type='mmr', search_kwargs={'k':1, 'fetch_k':5})\n",
    "retrievalQA = RetrievalQA.from_llm(llm=pipe, retriever=retriever)\n",
    "result = retrievalQA({\"query\": \"What is IDM 2.0?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatbot code with ITREX Langchain extension APIs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.documents import Document\n",
    "from intel_extension_for_transformers.langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from intel_extension_for_transformers.langchain.vectorstores import Chroma\n",
    "from intel_extension_for_transformers.neural_chat.pipeline.plugins.retrieval.parser.parser import DocumentParser\n",
    "\n",
    "document_parser = DocumentParser()\n",
    "input_path=\"./Intel_AR_WR.pdf\"\n",
    "data_collection=document_parser.load(input=input_path)\n",
    "documents = []\n",
    "for data, meta in data_collection:\n",
    "    doc = Document(page_content=data, metadata={\"source\":meta})\n",
    "    documents.append(doc)\n",
    "# load Intel/bge-base-en-v1.5-sts-int8-static from local\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"./bge-base-en-v1.5-sts-int8-static\")\n",
    "knowledge_base = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory='./output')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "pipe = HuggingFacePipeline(pipeline=pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128))\n",
    "retriever = VectorStoreRetriever(vectorstore=knowledge_base, search_type='mmr', search_kwargs={'k':1, 'fetch_k':5})\n",
    "retrievalQA = RetrievalQA.from_llm(llm=pipe, retriever=retriever)\n",
    "result = retrievalQA({\"query\": \"What is IDM 2.0?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the execution time, using ITREX Langchain extension APIs can get better performance.\n",
    "\n",
    "| APIs   |   Execution Time   |\n",
    "|-------|-------|\n",
    "| Langchain  | 106.094 sec  |\n",
    "| ITREX Langchain Extension | 81.429 sec  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've specifically designed `ChildParentRetriever` to address challenges in long-context retrieval scenarios. Our strategy involves initially splitting the user-uploaded files into larger chunks, termed 'parent chunks'. Then, these parent chunks are further divided into smaller 'child chunks'. Both child and parent chunks are interconnected using a unique identification ID. This approach enhances the likelihood and precision of matching the user query with a relevant, concise context chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from intel_extension_for_transformers.neural_chat.pipeline.plugins.retrieval.retrieval_agent import document_transfer, document_append_id\n",
    "from intel_extension_for_transformers.neural_chat.pipeline.plugins.retrieval.parser.parser import DocumentParser\n",
    "from intel_extension_for_transformers.langchain.retrievers import ChildParentRetriever\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512)\n",
    "document_parser = DocumentParser()\n",
    "input_path=\"./Intel_AR_WR.pdf\"\n",
    "data_collection=document_parser.load(input=input_path)\n",
    "langchain_documents = document_transfer(data_collection)\n",
    "child_documents = text_splitter.split_documents(langchain_documents)\n",
    "langchain_documents = document_append_id(langchain_documents)\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "knowledge_base = Chroma.from_documents(documents=langchain_documents, embedding=embeddings, persist_directory='./parent')\n",
    "child_knowledge_base = Chroma.from_documents(documents=child_documents, embedding=embeddings, persist_directory='./child')\n",
    "retriever = ChildParentRetriever(vectorstore=knowledge_base, parentstore=child_knowledge_base)\n",
    "docs=retriever.get_relevant_documents(\"What is IDM 2.0?\")\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing with result using default Langchain retriever, ITREX Langchain extension APIs can get better result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Retrieval Type   |   Retrieval Result   |\n",
    "|-------|-------|\n",
    "| default  | The Smart Capital strategy helps the company leverage various sources of capital to support investments in manufacturing capacity and fund their IDM 2.0 strategy.  |\n",
    "| ITREX Langchain Extension | Smart Capital for IDM 2.0 includes aggressive building out of manufacturing shell space, which gives flexibility in how and when to bring additional capacity online based on milestone triggers such as product readiness, market conditions, and customer commitments. It also involves government incentives, Strategic Capacity Investments (SCIP), customer commitments, and external foundries.   |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
