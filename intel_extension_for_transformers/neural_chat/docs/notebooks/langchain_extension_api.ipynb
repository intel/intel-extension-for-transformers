{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intel Extension for Transformers provides a comprehensive suite of Langchain-based extension APIs, including advanced retrievers, embedding models, and vector stores. These enhancements are carefully crafted to expand the capabilities of the original langchain API, ultimately boosting overall performance. This extension is specifically tailored to enhance the functionality and performance of RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install intel extension for transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install intel-extension-for-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install Requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "%cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/\n",
    "!pip install -r requirements.txt\n",
    "%cd ../../../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run LLM with Langchain-extension API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've revamped the Chroma API, enabling users to adjust and fine-tune their settings even after the chatbot has been initialized, offering a more adaptable and user-friendly experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://d1io3yog0oux5.cloudfront.net/_897efe2d574a132883f198f2b119aa39/intel/db/888/8941/file/412439%281%29_12_Intel_AR_WR.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from intel_extension_for_transformers.langchain.vectorstores import Chroma\n",
    "from intel_extension_for_transformers.neural_chat.pipeline.plugins.retrieval.parser.parser import DocumentParser\n",
    "\n",
    "document_parser = DocumentParser()\n",
    "input_path=\"./412439%281%29_12_Intel_AR_WR.pdf\"\n",
    "data_collection=document_parser.load(input=input_path)\n",
    "documents = []\n",
    "for data, meta in data_collection:\n",
    "    doc = Document(page_content=data, metadata={\"source\":meta})\n",
    "    documents.append(doc)\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "knowledge_base = Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory='./output')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "pipe = HuggingFacePipeline(pipeline=pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128))\n",
    "retriever = VectorStoreRetriever(vectorstore=knowledge_base)\n",
    "retrievalQA = RetrievalQA.from_llm(llm=pipe, retriever=retriever)\n",
    "result = retrievalQA({\"query\": \"What is IDM 2.0?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've specifically designed `ChildParentRetriever` to address challenges in long-context retrieval scenarios. Our strategy involves initially splitting the user-uploaded files into larger chunks, termed 'parent chunks'. Then, these parent chunks are further divided into smaller 'child chunks'. Both child and parent chunks are interconnected using a unique identification ID. This approach enhances the likelihood and precision of matching the user query with a relevant, concise context chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from intel_extension_for_transformers.neural_chat.pipeline.plugins.retrieval.retrieval_agent import document_transfer, document_append_id\n",
    "from intel_extension_for_transformers.neural_chat.pipeline.plugins.retrieval.parser.parser import DocumentParser\n",
    "from intel_extension_for_transformers.langchain.retrievers import ChildParentRetriever\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=512)\n",
    "document_parser = DocumentParser()\n",
    "input_path=\"./412439%281%29_12_Intel_AR_WR.pdf\"\n",
    "data_collection=document_parser.load(input=input_path)\n",
    "langchain_documents = document_transfer(data_collection)\n",
    "child_documents = text_splitter.split_documents(langchain_documents)\n",
    "langchain_documents = document_append_id(langchain_documents)\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "knowledge_base = Chroma.from_documents(documents=langchain_documents, embedding=embeddings, persist_directory='./parent')\n",
    "child_knowledge_base = Chroma.from_documents(documents=child_documents, embedding=embeddings, persist_directory='./child')\n",
    "retriever = ChildParentRetriever(vectorstore=knowledge_base, parentstore=child_knowledge_base)\n",
    "docs=retriever.get_relevant_documents(\"What is IDM 2.0?\")\n",
    "print(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
