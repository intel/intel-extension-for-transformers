{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative AI: Customize Chatbot with new LLM Models\n",
    "NeuralChat is a customizable chat framework designed to create user own chatbot within few minutes on multiple architectures. This notebook will introduce how to use your new LLM model to customize chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the new Model ðŸ’»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create `new_llm_model.py` in ***intel_extension_for_transformers/neural_chat/models/***, and define `NewModel` class derived from `BaseModel` in `base_model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .base_model import BaseModel, register_model_adapter\n",
    "import logging\n",
    "from fastchat.conversation import Conversation\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NewModel(BaseModel):\n",
    "    def match(self, model_path: str):\n",
    "        \"\"\"\n",
    "        Check if the provided model_path matches the current model.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to a model.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the model_path matches, False otherwise.\n",
    "        \"\"\"\n",
    "        return \"new_model\" in model_path.lower()\n",
    "\n",
    "    def get_default_conv_template(self, model_path: str) -> Conversation:\n",
    "        \"\"\"\n",
    "        Get the default conversation template for the given model path.\n",
    "\n",
    "        Args:\n",
    "            model_path (str): Path to the model.\n",
    "\n",
    "        Returns:\n",
    "            Conversation: A default conversation template.\n",
    "        \"\"\"\n",
    "        new_conv_template = Conversation(name=\"new_model\")\n",
    "        return new_conv_template\n",
    "\n",
    "register_model_adapter(NewModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model Adapter ðŸ’¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model adapter in ***intel_extension_for_transformers/neural_chat/chatbot.py*** at line 51. Modify the identification token like `new_llm_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"llama\" in config.model_name_or_path.lower():\n",
    "    from .models.llama_model import LlamaModel\n",
    "    adapter = LlamaModel()\n",
    "# create new model adapter here\n",
    "elif \"new_llm_model\" in config.model_name_or_path:\n",
    "    from .models.new_llm_model import NewModel\n",
    "    adapter = NewModel()\n",
    "else:\n",
    "    raise ValueError(\"NeuralChat Error: Unsupported model name or path, \\\n",
    "                        only supports FLAN-T5/LLAMA/MPT/GPT/BLOOM/OPT/NEURAL-CHAT now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Chatbot ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customize chatbot with the new llm model. Change the `model_name_or_path` to the real path and make sure the token \"new_llm_model\" is in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat.config import PipelineConfig\n",
    "config = PipelineConfig(model_name_or_path=\"/path/to/new_llm_model\")\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
