{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning or RAG for external knowledge?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demystify these two methods:\n",
    "\n",
    "**Finetuning:** This is the process of taking a pre-trained LLM and further training it on a smaller, specific dataset to adapt it for a particular task or to improve its performance. By finetuning, we are adjusting the model’s weights based on our data, making it more tailored to our application’s unique needs.\n",
    "\n",
    "**RAG:** This approach integrates the power of retrieval (or searching) into LLM text generation. It combines a retriever system, which fetches relevant document snippets from a large corpus, and an LLM, which produces answers using the information from those snippets. In essence, RAG helps the model to “look up” external information to improve its responses.\n",
    "\n",
    "Both of the two methods can help answer specific domain knowledge based on external information. There are some factors that impact the performance of them:\n",
    "\n",
    "1.  Dynamic vs. Static Data\n",
    "    - RAG: RAG excels in dynamic data environments. It continuously queries external sources, ensuring that the information remains up-to-date without frequent model retraining.\n",
    "\n",
    "    - Fine-Tuning: Fine-tuned models become static data snapshots during training and may quickly become outdated in dynamic data scenarios. Furthermore, fine-tuning does not guarantee recall of this knowledge, making it unreliable.\n",
    "    \n",
    "    - Conclusion: RAG offers agility and up-to-date responses in rapidly evolving data landscapes, making it ideal for projects with dynamic information needs.\n",
    "\n",
    "2. External Knowledge\n",
    "    - RAG: RAG is designed to augment LLM capabilities by retrieving relevant information from knowledge sources before generating a response. It's ideal for applications that query databases, documents, or other structured/unstructured data repositories. RAG excels at leveraging external sources to enhance responses.\n",
    "\n",
    "    - Fine-Tuning: While it's possible to fine-tune an LLM to learn external knowledge, it may not be more practical for frequently changing data sources. Usually, training and evaluating models can be difficult and time-consuming.\n",
    "\n",
    "    - Conclusion: RAG is likely the better option if your application heavily relies on external data sources due to its flexibility and ability to adapt to changing information.\n",
    "\n",
    "3. Model Customization\n",
    "    - RAG: RAG primarily focuses on information retrieval and may not inherently adapt its linguistic style or domain-specificity based on the retrieved information. It excels at incorporating external knowledge but may not fully customize the model's behavior or writing style.\n",
    "    \n",
    "    - Fine-Tuning: Fine-tuning allows you to adapt an LLM's behavior, writing style, or domain-specific knowledge to specific nuances, tones, or terminologies. It offers deep alignment with particular styles or expertise areas.\n",
    "\n",
    "    - Conclusion: Fine-tuning offers a more direct route if your application demands specialized writing styles or deep alignment with domain-specific vocabulary and conventions.\n",
    "\n",
    "\n",
    "Both approaches have their own strengths and weaknesses, you can unlock the full potential of your language model and create more reliable AI applications by choosing the right approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are an example for finetuning and RAG on a Q&A scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommend to use Python 3.9 or higher version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install intel-extension-for-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "%cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/\n",
    "!pip install -r requirements.txt\n",
    "%cd ../../../\n",
    "!pip uninstall torch -y\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### 1. prepare dataset\n",
    "\n",
    "Convert your dataset into chat-format, which is \"jsonl\" file, and each line has fields like:\n",
    "\n",
    "```python\n",
    "{\n",
    "\t\"messages\": [\n",
    "        {\n",
    "\t\t\"from\": \"human\",\n",
    "\t\t\"content\": \"What is cnvrg.io?\"\n",
    "        }, \n",
    "        {\n",
    "\t\t\"from\": \"chatbot\",\n",
    "\t\t\"content\": \"Cnvrg.io is a comprehensive ML/AI platform that provides tools and services to simplify the machine learning and artificial intelligence development process.\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. start finetuning\n",
    "\n",
    "We employ the [LoRA approach](https://arxiv.org/pdf/2106.09685.pdf) to finetune the LLM efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finetune the model on your chat-format dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "\n",
    "data_path = \"cnvrg_dataset\"\n",
    "llama2_model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=llama2_model_name_or_path,\n",
    "    use_fast_tokenizer=False,\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    dataset_name=data_path,\n",
    "    max_seq_length=1024,\n",
    "    max_source_length=512,\n",
    "    preprocessing_num_workers=4,\n",
    "    validation_split_percentage=0,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_peft_finetuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=8,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "finetune_args = FinetuningArguments(\n",
    "    lora_alpha=64,\n",
    "    lora_rank=16,\n",
    "    lora_dropout=0.05,\n",
    "    lora_all_linear=True,\n",
    "    do_lm_eval=True,\n",
    "    task=\"chat\"\n",
    ")\n",
    "\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "        model_args=model_args,\n",
    "        data_args=data_args,\n",
    "        training_args=training_args,\n",
    "        finetune_args=finetune_args,\n",
    ")\n",
    "\n",
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. inference with the finetuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat.models.model_utils import load_model, predict_stream\n",
    "from transformers import set_seed\n",
    "set_seed(27)\n",
    "\n",
    "\n",
    "base_model_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "peft_model_path = \"./llama_peft_finetuned_model\"\n",
    "\n",
    "load_model(model_name=base_model_path,\n",
    "        tokenizer_name=base_model_path,\n",
    "        peft_path=peft_model_path,\n",
    "        )\n",
    "\n",
    "template = \"\"\"\n",
    "### System:\n",
    "- You are a helpful assistant chatbot trained by Intel.\n",
    "- You answer questions.\n",
    "- You are excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.\n",
    "- You are more than just an information source, you are also able to write poetry, short stories, and make jokes.</s>\n",
    "### User:\n",
    "{}</s>\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "\n",
    "query = \"What is cnvrg.io?\"\n",
    "\n",
    "params = {\n",
    "        \"prompt\": template.format(query),\n",
    "        \"model_name\": base_model_path,\n",
    "        \"use_cache\": True,\n",
    "        \"repetition_penalty\": 1.0,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_k\": 10,\n",
    "        \"top_p\": 0.75,\n",
    "        \"num_beams\": 0,\n",
    "        \"max_new_tokens\": 128\n",
    "        }\n",
    "\n",
    "for new_text in predict_stream(**params):\n",
    "    print(new_text, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. prepare dataset\n",
    "\n",
    "the format for RAG, refer to: https://github.com/intel/intel-extension-for-transformers/blob/main/intel_extension_for_transformers/neural_chat/assets/docs/sample.jsonl\n",
    "\n",
    "For the example as follows, you can define the content of `doc` to be \"The cnvrg.io was founded by Yochay Ettun and Leah Forkosh Kolben.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. build RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "plugins.retrieval.enable = True\n",
    "plugins.retrieval.args['embedding_model'] = \"hkunlp/instructor-large\"\n",
    "plugins.retrieval.args['process'] = False\n",
    "\n",
    "plugins.retrieval.args[\"input_path\"] = './cnvrg_docs_rag/'\n",
    "plugins.retrieval.args[\"persist_dir\"] = \"./test_dir\"\n",
    "plugins.retrieval.args[\"response_template\"] = \"check the result\"\n",
    "plugins.retrieval.args['search_type'] = \"similarity_score_threshold\"\n",
    "plugins.retrieval.args['append'] = False\n",
    "plugins.retrieval.args['search_kwargs'] = {\"score_threshold\": 0.8, \"k\": 1}\n",
    "config = PipelineConfig(model_name_or_path=\"meta-llama/Llama-2-7b-hf\", plugins=plugins)\n",
    "\n",
    "chatbot = build_chatbot(config)\n",
    "\n",
    "response = chatbot.predict(\"Who are the founders of cnvrg.io?\")\n",
    "print('response',response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
