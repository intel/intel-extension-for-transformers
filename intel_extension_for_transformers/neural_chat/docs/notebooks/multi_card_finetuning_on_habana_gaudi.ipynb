{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune Your Chatbot on Habana Gaudi \n",
    "\n",
    "NeuralChat is a customizable chat framework designed to create user own chatbot within few minutes on multiple architectures. This notebook will introduce how to finetune your chatbot with the customized data on multi Intel Habana Gaodi Processors.\n",
    "\n",
    "## Prepare Environment\n",
    "In order to streamline the process, users can construct a Docker image employing a Dockerfile, initiate the Docker container, and then proceed to execute inference or finetuning operations.\n",
    "\n",
    "IMPORTANT: Please note Habana's Gaudi processors(HPU) requires docker environment for running. User needs to manually execute below steps to build docker image and run docker container for inference on Habana HPU. The Jupyter notebook server should be started in the docker container and then run this Jupyter notebook.\n",
    "\n",
    "To run finetuning on Habana HPU, please execute below steps\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "cd intel-extension-for-transformers/neural_chat/docker/finetuning/\n",
    "DOCKER_BUILDKIT=1 docker build --network=host --tag chatbot_finetuning:latest ./ -f Dockerfile  --target hpu --build-arg BASE_NAME=\"base-installer-ubuntu22.04\" --build-arg ARTIFACTORY_URL=\"vault.habana.ai\" --build-arg VERSION=\"1.10.0\" --build-arg REVISION=\"494\" --build-arg PT_VERSION=\"2.0.1\" --build-arg OS_NUMBER=\"2204\"\n",
    "docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host chatbot_finetuning:latest\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Dataset\n",
    "We select 3 kind of datasets to conduct the finetuning process for different tasks.\n",
    "\n",
    "1. Text Generation (General domain instruction): We use the [Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca) from Stanford University as the general domain dataset to fine-tune the model. This dataset is provided in the form of a JSON file, [alpaca_data.json](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json). In Alpaca, researchers have manually crafted 175 seed tasks to guide `text-davinci-003` in generating 52K instruction data for diverse tasks.\n",
    "\n",
    "2. Summarization: An English-language dataset [cnn_dailymail](https://huggingface.co/datasets/cnn_dailymail) containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail, is used for this task.\n",
    "\n",
    "3. Code Generation: To enhance code performance of LLMs (Large Language Models), we use the [theblackcat102/evol-codealpaca-v1](https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune Your Chatbot\n",
    "You could resort to `gaudi_spawn.py` to automatically complete the setting for the multiple card on habana. Then, you can train your chatbot with Alpaca dataset.\n",
    "```bash\n",
    "python gaudi_spawn.py \\\n",
    "        --world_size 8 --use_mpi finetune_clm.py \\\n",
    "        --model_name_or_path \"decapoda-research/llama-7b-hf\" \\\n",
    "        --bf16 True \\\n",
    "        --train_file \"/path/to/alpaca_data.json\" \\\n",
    "        --dataset_concatenation \\\n",
    "        --per_device_train_batch_size 2 \\\n",
    "        --per_device_eval_batch_size 2 \\\n",
    "        --gradient_accumulation_steps 4 \\\n",
    "        --evaluation_strategy \"no\" \\\n",
    "        --save_strategy \"steps\" \\\n",
    "        --save_steps 2000 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --learning_rate 1e-4  \\\n",
    "        --logging_steps 1 \\\n",
    "        --do_train \\\n",
    "        --num_train_epochs 3 \\\n",
    "        --overwrite_output_dir \\\n",
    "        --log_level info \\\n",
    "        --output_dir ./llama_peft_finetuned_model \\\n",
    "        --peft lora \\\n",
    "        --use_fast_tokenizer false \\\n",
    "        --device \"habana\" \\\n",
    "        --use_habana \\\n",
    "        --use_lazy_mode \\\n",
    "```\n",
    "\n",
    "Train your chatbot on the summarization task.\n",
    "```bash\n",
    "python gaudi_spawn.py \\\n",
    "        --world_size 8 --use_mpi finetune_clm.py \\\n",
    "        --model_name_or_path \"decapoda-research/llama-7b-hf\" \\\n",
    "        --bf16 True \\\n",
    "        --dataset_name \"cnn_dailymail\" \\\n",
    "        --dataset_config_name \"3.0.0\" \\\n",
    "        --dataset_concatenation \\\n",
    "        --per_device_train_batch_size 2 \\\n",
    "        --per_device_eval_batch_size 2 \\\n",
    "        --gradient_accumulation_steps 4 \\\n",
    "        --evaluation_strategy \"no\" \\\n",
    "        --save_strategy \"steps\" \\\n",
    "        --save_steps 2000 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --learning_rate 1e-4  \\\n",
    "        --logging_steps 1 \\\n",
    "        --do_train \\\n",
    "        --num_train_epochs 3 \\\n",
    "        --overwrite_output_dir \\\n",
    "        --log_level info \\\n",
    "        --output_dir ./llama_peft_finetuned_model \\\n",
    "        --peft lora \\\n",
    "        --use_fast_tokenizer false \\\n",
    "        --device \"habana\" \\\n",
    "        --use_habana \\\n",
    "        --use_lazy_mode \\\n",
    "```\n",
    "\n",
    "Train your chatbot on the code generation task:\n",
    "```bash\n",
    "python gaudi_spawn.py \\\n",
    "        --world_size 8 --use_mpi finetune_clm.py \\\n",
    "        --model_name_or_path \"decapoda-research/llama-7b-hf\" \\\n",
    "        --bf16 True \\\n",
    "        --dataset_name \"theblackcat102/evol-codealpaca-v1\" \\\n",
    "        --dataset_concatenation \\\n",
    "        --per_device_train_batch_size 2 \\\n",
    "        --per_device_eval_batch_size 2 \\\n",
    "        --gradient_accumulation_steps 4 \\\n",
    "        --evaluation_strategy \"no\" \\\n",
    "        --save_strategy \"steps\" \\\n",
    "        --save_steps 2000 \\\n",
    "        --save_total_limit 1 \\\n",
    "        --learning_rate 1e-4  \\\n",
    "        --logging_steps 1 \\\n",
    "        --do_train \\\n",
    "        --num_train_epochs 3 \\\n",
    "        --overwrite_output_dir \\\n",
    "        --log_level info \\\n",
    "        --output_dir ./llama_peft_finetuned_model \\\n",
    "        --peft lora \\\n",
    "        --use_fast_tokenizer false \\\n",
    "        --device \"habana\" \\\n",
    "        --use_habana \\\n",
    "        --use_lazy_mode \\\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
