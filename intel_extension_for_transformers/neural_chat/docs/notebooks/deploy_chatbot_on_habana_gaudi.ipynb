{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeuralChat is a customizable chat framework designed to create user own chatbot within few minutes on multiple architectures. This notebook is used to demostrate how to deploy a talking chatbot as a service on Habana's Gaudi processors(HPU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to streamline the process, users can construct a Docker image employing a Dockerfile, initiate the Docker container, and then proceed to execute inference or finetuning operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** Please note Habana's Gaudi processors(HPU) requires docker environment for running. User needs to manually execute below steps to build docker image and run docker container for inference on Habana HPU. The Jupyter notebook server should be started in the docker container and then run this Jupyter notebook. \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "cd intel-extension-for-transformers/docker/inference/\n",
    "DOCKER_BUILDKIT=1 docker build --network=host --tag chatbothabana:latest  ./ -f Dockerfile  --target hpu --build-arg BASE_NAME=\"base-installer-ubuntu22.04\" --build-arg ARTIFACTORY_URL=\"vault.habana.ai\" --build-arg VERSION=\"1.10.0\" --build-arg REVISION=\"494\" --build-arg PT_VERSION=\"2.0.1\" --build-arg OS_NUMBER=\"2204\"\n",
    "docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host chatbothabana:latest\n",
    "```\n",
    "\n",
    "To run finetuning on Habana HPU, please execute below steps\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "cd intel-extension-for-transformers/docker/finetuning/\n",
    "DOCKER_BUILDKIT=1 docker build --network=host --tag chatbot_finetuning:latest ./ -f Dockerfile  --target hpu\n",
    "docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none -v /dev/shm:/dev/shm  -v /absolute/path/to/llama2:/llama2 -v /absolute/path/to/alpaca_data.json:/dataset/alpaca_data.json --cap-add=sys_nice --net=host --ipc=host chatbot_finetuning:latest\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Client-Server Architecture for Performance and Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start Local Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó Please notice that the server is running on the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "from intel_extension_for_transformers.neural_chat import NeuralChatServerExecutor\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def start_service():\n",
    "    server_executor = NeuralChatServerExecutor()\n",
    "    server_executor(config_file=\"./server/config/neuralchat.yaml\", log_file=\"./log/neuralchat.log\")\n",
    "multiprocessing.Process(target=start_service).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Text Chat Service "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_chat import TextChatClientExecutor\n",
    "executor = TextChatClientExecutor()\n",
    "result = executor(\n",
    "    prompt=\"Tell me about Intel Xeon Scalable Processors.\",\n",
    "    server_ip=\"127.0.0.1\", # master server ip\n",
    "    port=8000 # master server entry point \n",
    "    )\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Voice Chat Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_chat import VoiceChatClientExecutor\n",
    "executor = VoiceChatClientExecutor()\n",
    "result = executor(\n",
    "    audio_input_path='./assets/audio/sample.wav',\n",
    "    audio_output_path='./results.wav',\n",
    "    server_ip=\"127.0.0.1\", # master server ip\n",
    "    port=8000 # master server entry point \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "# Play input audio\n",
    "print(\"     Play Input Audio ......\")\n",
    "IPython.display.display(IPython.display.Audio(\"./assets/audio/sample.wav\"))\n",
    "# Play output audio\n",
    "print(\"     Play Output Audio ......\")\n",
    "IPython.display.display(IPython.display.Audio(\"./assets/audio/welcome.wav\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access Finetune Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_chat import FinetuingClientExecutor\n",
    "executor = FinetuingClientExecutor()\n",
    "tuning_status = executor(\n",
    "    server_ip=\"127.0.0.1\", # master server ip\n",
    "    port=8000 # master server port (port on socket 0, if both sockets are deployed)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
