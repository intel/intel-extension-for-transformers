{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Talking Chatbot! This notebook provides instructions for setting up the Talking Chatbot system on Intel XEON Scalable Processors. You can also deploy this text chatbot on various other platforms, such as Habana's Gaudi processors (HPU), Intel Data Center GPU and Client GPU, Nvidia Data Center GPU and Client GPU, by making minor configuration adjustments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install intel-extension-for-transformers\n",
    "!git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "%cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup the backend server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó Please notice that the server is running on the background. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install numactl\n",
    "!conda install astunparse ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses -y\n",
    "!conda install jemalloc gperftools -c conda-forge -y\n",
    "!conda install -q -y pyg -c pyg\n",
    "!conda install -q -y pytorch cudatoolkit=11.3 -c pytorch\n",
    "!pip install -U torch torchaudio --no-cache-dir\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/examples/deployment/talkingbot/server/backend/talkingbot.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from intel_extension_for_transformers.neural_chat import NeuralChatServerExecutor\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def start_service():\n",
    "    server_executor = NeuralChatServerExecutor()\n",
    "    server_executor(config_file=\"talkingbot.yaml\", log_file=\"neuralchat.log\")\n",
    "multiprocessing.Process(target=start_service).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup frontend\n",
    "\n",
    "## üì∏ Frontend Screenshots\n",
    "\n",
    "![project-screenshot](https://i.imgur.com/aMQjHB5.png)\n",
    "![project-screenshot](https://i.imgur.com/49kpqzr.png)\n",
    "![project-screenshot](https://i.imgur.com/2pgsyGg.png)\n",
    "![project-screenshot](https://i.imgur.com/DYNPdSE.png)\n",
    "![project-screenshot](https://i.imgur.com/9518rja.png)\n",
    "![project-screenshot](https://i.imgur.com/NQedVvT.png)\n",
    "\n",
    "\n",
    "<h2>üßê Features</h2>\n",
    "\n",
    "Here're some of the project's features:\n",
    "\n",
    "- Start a Talking ChatÔºöVoice Chat and Bot Conversation.\n",
    "- Start with a TemplateÔºöDialogue with the character avatar and voice corresponding to the voice chat robot.\n",
    "- Start with a CustomizationÔºöFeel free to mix and match voices and avatars to customize your own chatbot.\n",
    "- Upload: Upload avatar/voice/knowledge base, customize exclusive chatbot.\n",
    "- Regenerate: Regenerates the current conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏èGet it Running\n",
    "\n",
    "cd to the talkingbot frontend folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./examples/deployment/talkingbot/server/frontend/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify .env file and set the following environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_URL = 'http://x.x.x.x:8888/v1/talkingbot'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'x.x.x.x' refers to the backend ip, please use the specific ip address instead of '127.0.0.1'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute `npm install` to install the corresponding dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the frontend using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!npm run dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The host and post of frontend are defined in `dev` near the beginning of `package.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you cannot chat using voice, please add the host of frontend `http://x.x.x.x` in `chrome://flags/` and enable it."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
