{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the Text Chatbot! This notebook provides instructions for setting up the Text Chatbot system with caching plugin on Intel XEON Scalable Processors.\n",
    "When LLM service encounters higher traffic levels, the expenses related to LLM API calls can become substantial. Additionally, LLM services might exhibit slow response times. Hence, we leverage GPTCache to build a semantic caching plugin for storing LLM responses.\n",
    "\n",
    "Caching plugin offers the following primary benefits:\n",
    "\n",
    "- **Decreased expenses**: Caching plugin effectively minimizes expenses by caching query results, which in turn reduces the number of requests and tokens sent to the LLM service.\n",
    "- **Enhanced performance**: Caching plugin can also provide superior query throughput compared to standard LLM services.\n",
    "- **Improved scalability and availability**: Caching plugin can easily scale to accommodate an increasing volume of of queries, ensuring consistent performance as your application's user base expands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Backend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install intel-extension-for-transformers\n",
    "!git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "%cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/\n",
    "!pip install -r requirements.txt\n",
    "!sudo apt install numactl\n",
    "!conda install astunparse ninja pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses -y\n",
    "!conda install jemalloc gperftools -c conda-forge -y\n",
    "!pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Startup the backend server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùó Please be aware that the server is running in the background. You can download the 'textbot.yaml' configuration file locally. This 'textbot.yaml' file enables caching and security checker plugins by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/examples/deployment/textbot/backend_with_cache/textbot.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from intel_extension_for_transformers.neural_chat import NeuralChatServerExecutor\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def start_service():\n",
    "    server_executor = NeuralChatServerExecutor()\n",
    "    server_executor(config_file=\"textbot.yaml\", log_file=\"neuralchat.log\")\n",
    "multiprocessing.Process(target=start_service).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup frontend\n",
    "\n",
    "Hugging Face Space helps to make some amazing ML applications more accessible to the community. Inspired by this, we can create a chatbot frontend on Hugging Face Spaces. Alternatively, you can also deploy the frontend on your own server.\n",
    "\n",
    "## Deploy on Huggingface Space\n",
    "\n",
    "### Create a new space on Huggingface\n",
    "To create a new application space on Hugging Face, visit the website at [https://huggingface.co/new-space](https://huggingface.co/new-space) and follow the below steps to create a new space.\n",
    "\n",
    "![Create New Space](https://i.imgur.com/QyjqUd6.png)\n",
    "\n",
    "The new space is like a new project that supports GitHub-style code repository management.\n",
    "\n",
    "### Check configuration\n",
    "We recommend using Gradio as the Space SDK, keeping the default values for the other settings.\n",
    "\n",
    "For detailed information about the configuration settings, please refer to the [Hugging Face Spaces Config Reference](https://huggingface.co/docs/hub/spaces-config-reference).\n",
    "\n",
    "### Setup application\n",
    "We strongly recommend utilizing the provided textbot frontend code as it represents the reference implementation already deployed on Hugging Face Space. To establish your application, simply copy the code files from this directory(intel_extension_for_transformers/neural_chat/examples/textbot/frontend) and adjust their configurations as necessary (e.g., backend service URL in the `app.py` file like below).\n",
    "\n",
    "![Update backend URL](https://i.imgur.com/rQxPOV7.png)\n",
    "\n",
    "Alternatively, you have the option to clone the existing space from [https://huggingface.co/spaces/Intel/NeuralChat-GNR-1](https://huggingface.co/spaces/Intel/NeuralChat-GNR-1).\n",
    "\n",
    "![Clone Space](https://i.imgur.com/76N8m5B.png)\n",
    "\n",
    "Please also update the backend service URL in the `app.py` file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy frontend on your server\n",
    "\n",
    "### Install the required Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ./examples/deployment/textbot/frontend/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the frontend\n",
    "\n",
    "Launch the chatbot frontend on your server using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ./examples/deployment/textbot/frontend/\n",
    "!nohup python app.py &"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will run the chatbot application in the background on your server. The port is defined in `server_port=` at the end of the `app.py` file.\n",
    "\n",
    "Once the application is running, you can find the access URL in the trace log:\n",
    "\n",
    "```log\n",
    "INFO | gradio_web_server | Models: meta-llama/Llama-2-7b-chat-hf\n",
    "INFO | stdout | Running on local URL:  http://0.0.0.0:7860\n",
    "```\n",
    "The URL to access the chatbot frontend is http://SERVER_IP_ADDRESS:7860. Please remember to replace SERVER_IP_ADDRESS with your server's actual IP address.\n",
    "\n",
    "![URL](https://i.imgur.com/La3tJ8d.png)\n",
    "\n",
    "Please update the backend service URL in the `app.py` file.\n",
    "\n",
    "![Update backend URL](https://i.imgur.com/gRtZHrJ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: Caching vs. No Caching"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
