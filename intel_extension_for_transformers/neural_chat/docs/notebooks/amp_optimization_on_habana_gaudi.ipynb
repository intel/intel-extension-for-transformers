{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMP Optimization of Chatbot on Habana's Gaudi processors(HPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** Please note Habana's Gaudi processors(HPU) requires docker environment for running. User needs to manually execute below steps to build docker image and run docker container for inference on Habana HPU. The Jupyter notebook server should be started in the docker container and then run this Jupyter notebook. \n",
    "\n",
    "```bash\n",
    "git clone https://github.com/intel/intel-extension-for-transformers.git\n",
    "cd ./intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/docker/\n",
    "docker build --build-arg UBUNTU_VER=22.04 -f Dockerfile -t neuralchat . --target hpu\n",
    "docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all neuralchat:latest\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BF16 Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(optimization_config=MixedPrecisionConfig())\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
