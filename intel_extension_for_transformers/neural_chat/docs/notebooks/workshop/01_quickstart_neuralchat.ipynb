{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c6ddfc4-ecd6-4f0a-9e32-5ff4ab014db7",
   "metadata": {},
   "source": [
    "# QuickStart: Intel® Extension For Transformers*: NeuralChat on 4th Generation Intel® Xeon® Scalable Processors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a628ae6-ddd2-41d3-acf1-855b6f31aef5",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae386d8-6138-478d-ae4d-8572a561967c",
   "metadata": {},
   "source": [
    "Follow the README to install the necessary requirements to run this tutorial. In summary, you will need to install the following:\n",
    "-  Intel(R) Extension for Transformers* from source (to get latest updates)\n",
    "-  NeuralChat requirements\n",
    "-  Retrieval Plugin Requirements\n",
    "-  Audio Plugin (TTS and ASR) Requirements\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733fa69-5e68-453c-8f6f-401ee096f4a7",
   "metadata": {},
   "source": [
    "Check hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d39d47-087f-4e2a-b766-b3707ee342e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!lscpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a640c2",
   "metadata": {},
   "source": [
    "Library imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c6d1e2-61f1-4ee4-98c7-4f6202e7f2ea",
   "metadata": {},
   "source": [
    "## Building a Simple Chatbot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9791bad",
   "metadata": {},
   "source": [
    "Building a chatbot only requires the 3 lines of code below! By default, the model is Intel's Neural-Chat-7B-V3-1 model. Without any optimizations, the model runs in FP32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5555f5b-84d1-4154-9388-8aa17041fec0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build chatbot\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig, GenerationConfig\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1')\n",
    "chatbot = build_chatbot(config)\n",
    "\n",
    "# Perform inference/generate a response\n",
    "\n",
    "gen_config = GenerationConfig(return_stats=True, format_version=\"v2\")\n",
    "results, _ = chatbot.predict_stream(\"Tell me about Intel Xeon Scalable Processors.\", config=gen_config)\n",
    "stream_text = \"\"\n",
    "for text in results:\n",
    "   stream_text += text\n",
    "print(stream_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db00b822-eef0-4b92-9a9a-9d7a8d148d6f",
   "metadata": {},
   "source": [
    "## Optimizing your Chatbot\n",
    "Enable mixed precision with bfloat16 (BF16). Using a lower precision data type will reduce memory usage and speed up runtime without sacrifice to accuracy, since BF16 has the same range as FP32, just less precision in terms of decimal places. Starting with the 4th Gen Intel® Xeon® Scalable Processors, there is an instruction set Advanced Matrix Extensions (AMX) which accelerates operators in BF16 and integer8 (INT8) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e832b1a-0f7a-4715-b245-85666729e206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build chatbot in BF16\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig, GenerationConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "mix_config = MixedPrecisionConfig(dtype=\"bfloat16\")\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1',\n",
    "                        optimization_config=mix_config)\n",
    "chatbot = build_chatbot(config)\n",
    "\n",
    "# Perform inference/generate a response\n",
    "gen_config = GenerationConfig(return_stats=True, format_version=\"v2\")\n",
    "results, _ = chatbot.predict_stream(\"Tell me about Intel Xeon Scalable Processors.\", config=gen_config)\n",
    "stream_text = \"\"\n",
    "for text in results:\n",
    "   stream_text += text\n",
    "print(stream_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff9ca77-149b-47cc-9a5b-a6f70877f09e",
   "metadata": {},
   "source": [
    "INT4 weight-only quantization can be used to further reduce memory and speed up performance without too much loss to accuracy. Note that the _compute_dtype_ is \"int8\" because AMX only supports down to INT8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a804023f-5a79-47ce-9b4b-61ea9746df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chatbot with INT4 weight-only quantization, computations in AMX INT8\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig\n",
    "from intel_extension_for_transformers.neural_chat.config import LoadingModelConfig\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1',\n",
    "                        optimization_config=WeightOnlyQuantConfig(compute_dtype=\"int8\", weight_dtype=\"int4_fullrange\"), \n",
    "                        loading_config=LoadingModelConfig(use_neural_speed=False))\n",
    "chatbot = build_chatbot(config)\n",
    "\n",
    "# Perform inference/generate a response\n",
    "gen_config = GenerationConfig(return_stats=True, format_version=\"v2\")\n",
    "results, _ = chatbot.predict_stream(\"Tell me about Intel Xeon Scalable Processors.\", config=gen_config)\n",
    "stream_text = \"\"\n",
    "for text in results:\n",
    "   stream_text += text\n",
    "print(stream_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d7df2d",
   "metadata": {},
   "source": [
    "## Swapping out Models: Llama2 Example\n",
    "You can swap out the Neural-Chat-7B model with another transformer model from [HuggingFace](https://huggingface.co/models), including the most popular LLMs. Pass in the model card for the _model_name_or_path_ argument. For example, this is how you can build a chatbot using Llama2 in FP32 and BF16. *NOTE* You may need to log in to HuggingFace to get access to this model. To do so, use the command _huggingface-cli login_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad36497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL: log in to HuggingFace to access Llama2\n",
    "#!huggingface-cli login --token <@TODO: enter in HF token here> --add-to-git-credential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df4fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build chatbot in BF16 using Llama2\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.transformers import MixedPrecisionConfig\n",
    "config = PipelineConfig(model_name_or_path='meta-llama/Llama-2-7b-chat-hf',\n",
    "                        optimization_config=MixedPrecisionConfig(dtype='bfloat16'))\n",
    "chatbot = build_chatbot(config)\n",
    "\n",
    "# Perform inference/generate a response\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc089c9-623f-43ec-a470-31e5f8d4cac4",
   "metadata": {},
   "source": [
    "## Customizing your Chatbot\n",
    "### Plugin: Retrieval\n",
    "Without the retrieval plugin, the output of the chatbot gives the wrong answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b5c7d-15e0-4438-b451-9172159398fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1')\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Who won Super Bowl 58 and what was the score?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65c5a6-1d33-43f6-8c61-92322b74e2cc",
   "metadata": {},
   "source": [
    "The retrieval plugin allows you to specify a file or a folder of files with information you want your chatbot to look up before outputting the final response. Here, _sample_workshop.txt_ contains the correct answer. For more information about the retrieval plugin and the file types supported, go to the [Retrieval README](https://github.com/intel/intel-extension-for-transformers/blob/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval/README.md).\n",
    "\n",
    "You can specify a single file or a a folder. In this example, the files will be placed inside a folder _docs_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaef691b-048a-4d5c-ada9-39acec7eeea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir docs\n",
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/docs/sample_workshop.txt\n",
    "!mv sample_workshop.txt ./docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e6474f-565b-4399-b250-a314447c2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat ./docs/sample_workshop.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3779e396-a963-4503-8a85-a0d897445c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build chatbot with retrieval\n",
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.retrieval.enable=True\n",
    "plugins.retrieval.args[\"input_path\"]=\"./docs/sample_workshop.txt\"\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1',\n",
    "                        plugins=plugins)\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Who won Super Bowl 58 and what was the score?\")\n",
    "print(response)\n",
    "\n",
    "plugins.retrieval.enable=False # disable retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2da2eef-e383-4e54-b9c1-4079cf053a33",
   "metadata": {},
   "source": [
    "### Plugin: ASR & TTS\n",
    "The ASR and TTS plugin enables voice chat for a more interactive experience. Instead of passing in text and getting text responses, you can pass in audio files and get audio files in response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fe3865-b23a-405d-8199-4a869f448ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/intel/intel-extension-for-transformers/main/intel_extension_for_transformers/neural_chat/assets/audio/sample.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103340b5-1b57-486c-9fba-3060de63ab42",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build chatbot with AST and TTS plugin\n",
    "from intel_extension_for_transformers.neural_chat import build_chatbot, PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat import plugins\n",
    "plugins.tts.enable = True\n",
    "plugins.tts.args[\"output_audio_path\"] = \"./response.wav\"\n",
    "plugins.asr.enable = True\n",
    "\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1',\n",
    "                        plugins=plugins)\n",
    "chatbot = build_chatbot(config)\n",
    "result = chatbot.predict(query=\"./sample.wav\")\n",
    "print(result)\n",
    "\n",
    "plugins.tts.enable = False # disable tts\n",
    "plugins.asr.enable = False # disable asr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7851393",
   "metadata": {},
   "source": [
    "Open the audio files using your own audio player to hear the query and response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25157d11-15c5-4d56-a4f4-5294db04a332",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(\"response.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a0779-8c26-4c62-a514-64bf9c58896f",
   "metadata": {},
   "source": [
    "### [Optional]: Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8419deb-8a06-41ba-af86-875d1d1c0fc7",
   "metadata": {},
   "source": [
    "We use the [Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca) from Stanford University as the general domain dataset to fine-tune the model. This dataset is provided in the form of a JSON file, [alpaca_data.json](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json). In Alpaca, researchers have manually crafted 175 seed tasks to guide `text-davinci-003` in generating 52K instruction data for diverse tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f5c30c-ecb4-4341-a586-4efdb3906bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -OL https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660a3a94-230a-497f-9ad9-2645b8f2d186",
   "metadata": {},
   "source": [
    "Finetune the model on Alpaca-format dataset to conduct text generation.\n",
    "\n",
    "We employ the [LoRA approach](https://arxiv.org/pdf/2106.09685.pdf) to finetune the LLM efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1008618d-a82e-4249-813e-ae46b95c64d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "model_args = ModelArguments(model_name_or_path='Intel/neural-chat-7b-v3-1')\n",
    "data_args = DataArguments(train_file=\"alpaca_data.json\")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./finetuned_model_path',\n",
    "    do_train=True,\n",
    "    do_eval=False,\n",
    "    num_train_epochs=3,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True\n",
    ")\n",
    "finetune_args = FinetuningArguments()\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "            model_args=model_args,\n",
    "            data_args=data_args,\n",
    "            training_args=training_args,\n",
    "            finetune_args=finetune_args,\n",
    "        )\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701abb4e-41fa-4aa4-81e0-2abc3f15f093",
   "metadata": {},
   "source": [
    "Load the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4112b3b3-ef0d-4157-8c23-cf3ed5d59e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_extension_for_transformers.neural_chat import build_chatbot\n",
    "from intel_extension_for_transformers.neural_chat import PipelineConfig\n",
    "from intel_extension_for_transformers.neural_chat.config import LoadingModelConfig\n",
    "\n",
    "config = PipelineConfig(model_name_or_path='Intel/neural-chat-7b-v3-1',\n",
    "                      loading_config=LoadingModelConfig(peft_path=\"./finetuned_model_path\"))\n",
    "chatbot = build_chatbot(config)\n",
    "response = chatbot.predict(query=\"Tell me about Intel Xeon Scalable Processors.\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e46d1-3f94-423c-abcb-4a533777e26c",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed the NeuralChat quickstart. Now go build your own custom chatbots!\n",
    "Visit [notebooks directory](https://github.com/intel/intel-extension-for-transformers/blob/c30353fcb0e5ceab440a7508b5980ccebcac8750/intel_extension_for_transformers/neural_chat/docs/full_notebooks.md) to see more examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itrex",
   "language": "python",
   "name": "itrex"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
