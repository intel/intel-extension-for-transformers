{"doc": "Effective Post-Training Quantization for Large Language Models\nIn this blog, we describe a post-training quantization technique for large language models with enhanced SmoothQuant approach. We also illustrate the usage and demonstrate the accuracy benefits. This method has been integrated into Intel Neural Compressor.\nIn this blog, we demonstrate an enhanced SmoothQuant approach to post-training quantization to improve large language models. This method has been integrated into Intel Neural Compressor, an open-source Python library of popular model compression techniques like quantization, pruning (sparsity), distillation, and neural architecture search. It is compatible with popular frameworks such as TensorFlow, the Intel Extension for TensorFlow, PyTorch, the Intel Extension for PyTorch, ONNX Runtime, and MXNet.", "doc_id": 0}