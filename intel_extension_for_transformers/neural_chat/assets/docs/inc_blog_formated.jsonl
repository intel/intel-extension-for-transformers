{"doc": "Effective Post-Training Quantization for Large Language Models\nIn this blog, we describe a post-training quantization technique for large language models with enhanced SmoothQuant approach. We also illustrate the usage and demonstrate the accuracy benefits. This method has been integrated into Intel Neural Compressor.\nIn this blog, we demonstrate an enhanced SmoothQuant approach to post-training quantization to improve large language models. This method has been integrated into Intel Neural Compressor, an open-source Python library of popular model compression techniques like quantization, pruning (sparsity), distillation, and neural architecture search. It is compatible with popular frameworks such as TensorFlow, the Intel Extension for TensorFlow, PyTorch, the Intel Extension for PyTorch, ONNX Runtime, and MXNet.", "doc_id": 0}
{"doc": "Large Language Models\nLarge language models (LLM) are trained on massive data sets and can have billions of weights. Their advanced network structures and larger number of parameters enable them to master the intrinsic complexity of natural language. Once trained, a LLM can be fine-tuned for a wide variety of downstream natural language processing (NLP) and natural language generation (NLG) tasks like conversational chatbots (e.g., ChatGPT), machine translation, text classification, fraud detection, and sentiment analysis.", "doc_id": 1}
{"doc": "LLM Deployment Challenges\nLLM perform very well on NLP and NLG tasks but training and deploying such large models is complicated by the AI and Memory Wall: compute improves 3.1x while memory bandwidth only improves 1.4x every two years. Also, distributed systems are required to train LLM, which adds a network bandwidth challenge. After training, models are often deployed on systems with limited compute and memory resources. As such, reducing the size of LLM via post-training quantization is critical to make low-latency inference possible.", "doc_id": 2}
{"doc": "Quantization for LLM\nQuantization is a common compression operation to reduce the memory footprint of a model and improve inference performance, which would make LLM deployment easier. Quantization converts the floating-point matrix to an integer matrix:\nwhere X_fp32, S and Z are the input matrix, scale factor, and integer zero point, respectively.\nOur SmoothQuant documentation explains why per-channel quantization could not be applied for activation, even though it could lead to lower quantization loss. However, the quantization error loss of activation plays an important role in the accuracy loss of model quantization. To reduce the quantization loss of activations, lots of methods have been proposed, e.g.: SPIQ, Outlier Suppression, and SmoothQuant. These three methods share a similar idea to shift the difficulty from activation quantization to weight quantization, but they differ in how much difficulty is transferred.", "doc_id": 3}
{"doc": "Enhancing SmoothQuant\nSmoothQuant introduces a hyperparameter \u03b1 as a smoothing factor to calculate the per-channel scale and balance the quantization difficulty of activation and weight:\nwhere j is the input channel index.\nFor most models, such as OPT and BLOOM, \u03b1 = 0.5 is a well-balanced value to split the difficulty of weight and activation quantization. A larger \u03b1 value could be used on models with more significant activation outliers to migrate more quantization difficulty to weights.\nThe original SmoothQuant aims to split the quantization difficulty of weight and activation by using a fixed value \u03b1 for an entire model. However, as the distributions of activation outliers vary not only across different models but also across different layers within a model, we propose to obtain layer-wise optimal \u03b1 values with the ability to tune automatically using Intel Neural Compressor.\nOur method consists of five major steps (the pseudocode is shown below):\n1. Hook input and output values of all layers using register_forward_hook.\n2. Generate a list of \u03b1 values given user-defined \u03b1 range and step sizes.\n3. Recalculate smoothing factor given an \u03b1 value and adjust parameters (weights and activations).\n4. Perform per-channel quantization_dequantization of weights and per-tensor quantization_dequantization of inputs to predict the layer-wise output corresponding to the given \u03b1 value.\n5. Calculate the mean-squared loss with respect to the actual output value, recover the adjusted parameters and save the layer-wise optimal \u03b1 values.", "doc_id": 4}
{"doc": "Multiple criteria (e.g., min, max, and mean) are supported to determine the \u03b1 value of an input LayerNorm operation of a transformer block. In our experiments, an \u03b1 range of [0.3, 0.7] with a step_size of 0.05 is found to be well-balanced for the majority of models.\nTwo remarkable features of our method are that it is fully automated and it supports more fusing patterns than the original approach. A sample code of performing SmoothQuant \u03b1 auto-tuning on the BLOOM-1b7 model is provided below:\nThe user only needs to pass a model_name and a data loader. Please note that we rely on Torch JIT to analyze the model. The user could set torchscript to True when loading a Hugging Face model or set the return_dict to False. Please refer to Intel Neural Compressor documentation for more information.", "doc_id": 5}
{"doc": "Results\nA major advantage of our enhancement is improved accuracy. After evaluations over various popular LLMs, INT8 SmoothQuant auto-tuning provides better last-token accuracy with respect to the original INT8 SmoothQuant and FP32 baseline.\nYou can see that our enhancement shows 5.4% and 1.6% higher accuracy than the default SmoothQuant on the OPT-1.3b and BLOOM-1b7 models, respectively. The quantized model is also up to 4x smaller than the FP32 model, which significantly reduces the memory footprints.\nPlease see our GitHub repository for more comprehensive results. You are also welcomed to create pull requests or leave your comments on GitHub issues. We look forward to your feedback and suggestions.", "doc_id": 6}
{"doc": "Intel is still in an exclusive club as the only data center CPU vendor to have MLPerf inference results on a broad set of models, not only submitting our own results but also closely collaborating with Dell, HP, and Quanta to deliver 4th Gen Xeon processor submissions, further showcasing AI performance scalability and portability on servers powered by Intel\u00ae Xeon processors.", "doc_id": 7}
{"doc": "4th Gen Xeon processors deliver remarkable gains of up to 5x across MLPerf workloads compared to 3rd Gen Intel\u00ae Xeon\u00ae Scalable processors \u2013 based on v1.1 to v2.x results from last August \u2013 due in large part to specialized AI hardware accelerators, including the Intel\u00ae Advanced Matrix Extension (Intel\u00ae AMX).", "doc_id": 8}
{"doc": "But there\u2019s more! Compared to our 4th Gen Xeon preview results last August, we were able to improve performance by 1.2x and 1.4x for Offline and Server scenarios respectively, underscoring how we are continuously optimizing the AI software stack to realize full utilization of the unique capabilities of our Xeon CPUs. Furthermore, we showcase additional up to 9x (offline) and 16x (server) performance improvement thru hardware-algorithm co-design in our open submission.", "doc_id": 9}
{"doc": "The introduction and unprecedented adoption of ChatGPT is indicative of the speed in which artificial intelligence and deep learning is moving, and how it\u2019ll soon be built into nearly every enterprise application. Given diverse use case requirements and the rapidly evolving nature of AI, it\u2019s even more critical to understand the performance capabilities of the Xeon CPUs that are already running your business.", "doc_id": 10}
{"doc": "4th Gen Intel\u00ae Xeon\u00ae Scalable processors feature an all-new level of built-in AI accelerator for deep learning, a comprehensive set of data preprocessing, modeling, and deployment tools and optimizations for the most popular AI frameworks and libraries. This powerful combination of hardware and software is designed to address the needs of any AI professional, from data scientists who want to customize models for large-scale data center deployments to application engineers who want to quickly integrate models into applications and deploy at the edge.", "doc_id": 11}
{"doc": "Our goal for the MLPerf Inference v3.0 submissions remains the same as always: Demonstrate how 4th Gen Xeon processors can run any AI workload \u2013 including vision, language processing, recommendation, speech and audio translation \u2013 and deliver remarkable gains, of up to 5x across these tasks, compared to 3rd Gen Intel\u00ae Xeon\u00ae Scalable processors \u2013 based on v1.1 to v2.x results from last year \u2013 due in large part to specialized AI hardware accelerators, including the Intel\u00ae Advanced Matrix Extension (Intel\u00ae AMX).", "doc_id": 12}
{"doc": "To simplify the use of these new accelerator engines and extract the best performance, we remain laser-focused on software. Our customers tend to use the mainstream distributions of the most popular AI frameworks and tools. Intel\u2019s AI experts have been working for years with the AI community to co-develop and optimize a broad range of open and free-to-use tools, optimized libraries, and industry frameworks to deliver the best out-of-the-box performance regardless of the Xeon generation.\nAnd that\u2019s where the additional performance kicker comes in. Compared to our 4th Gen Xeon preview results last August, we were able to improve performance by 1.2x and 1.4x for Offline and Server scenarios respectively, which underscores how we are continuously optimizing the AI software stack to realize full utilization of the unique capabilities of our Xeon CPUs. We\u2019ve also closely collaborated with our OEM partners \u2013 namely HPE, Dell, and Quanta \u2013 to deliver their own submissions, further showcasing AI performance scalability and portability on servers powered by Intel\u00ae Xeon processors.", "doc_id": 13}
{"doc": "Furthermore, Intel is developing tools and capabilities (e.g., OpenVINO, AI Kit, etc.) to further co-optimize algorithm-hardware to deliver additional performance while improving the developer experience by making them easy for them. To showcase these capabilities, Intel has made open submissions for BERT and RetinaNet models that demonstrates the trade-off between model performance and accuracy while automating end-to-end process of model optimizations. The submissions demonstrate impressive gains such as 9x in offline and 16x in server configurations.", "doc_id": 14}
{"doc": "Bottom line: Xeon CPUs can run every AI code, enabling you to deploy a single hardware and software platform from the data center to the factory floor. If you need acceleration beyond that to meet your service level agreements (SLAs), we can help there as well, but that\u2019s a story for another blog.", "doc_id": 15}
{"doc": "Intel Closed Submission Results for MLPerf v3.0 Inference\nIntel\u2019s submission covers all six data center benchmarks and demonstrated the leading CPU performance and software efficiency across the data center benchmark suite. See the task and framework coverage in Table 1 below. Summary results of data center submissions can be found at MLCommons results page.", "doc_id": 16}
{"doc": "In Figure 1, we summarize the Gen-to-Gen performance ratios between 3rd and 4th Gen Intel\u00ae Xeon\u00ae Scalable processors (codenamed Ice Lake and Sapphire Rapids respectively). In Figure 2, we show the performance improvements over our preview submission in v2.1, which is achieved through several software optimizations including Intel\u00ae oneAPI Deep Neural Network Library (oneDNN), and workload-specific optimizations.", "doc_id": 17}
{"doc": "The v3.0 submission features an average improvement of 1.2x for Offline and 1.4x for Server over v2.1 \u201cPreview\u201d submission on Sapphire Rapids. Compared to v1.1 submission on Ice Lake, this submission on Sapphire Rapids represents over 5x improvement in Offline, and a 10x improvement in Offline performance.", "doc_id": 18}
{"doc": "Figure 1 Comparison between MLPerf v1.1 submission on Ice Lake and v3.0 submission on Sapphire Rapids. *3D-UNET model changed from v1.1 to v3.0. **RetinaNet is a new benchmark in v3.0", "doc_id": 19}
{"doc": "Figure 2 Comparison between preview submission (v2.1) on Sapphire Rapids vs current submission (v3.0). The observed performance due to software optimizations (See section on workload optimizations for details). *RNNT v2.1 performance is internal and was not submitted to MLPerf v2.1 Inference submission", "doc_id": 20}
{"doc": "Now let\u2019s dive deeper into the software engineering behind the scenes for how we made it happen. The optimization techniques that we describe benefit the model classes in general, beyond the specific models listed. You can find the implementation details in our latest MLPerf inference v3.0 submission.", "doc_id": 21}
{"doc": "Software Optimization Highlights in MLPerf v3.0 Submission Results\nWe use Intel\u00ae Deep Learning Boost (Intel\u00ae DL Boost) technology, Intel\u00ae Advanced Matrix Extension (Intel\u00ae AMX), including Vector Neural Network Instructions (VNNI), in our INT8-based submissions on ResNet50-v1.5, RetinaNet, 3D UNET, DLRM, and BERT. We use INT8 and bfloat16 (mixed precision) in our Recurrent Neural Network Transducer (RNN-T) submissions.", "doc_id": 22}
{"doc": "Apart from low precision inference, the following optimization techniques are used in the submissions. This is not an exhaustive list since software optimization for inference is a broad area with many exciting research and innovations in industry and academia.", "doc_id": 23}
{"doc": "3D-UNet\nOptimizations performed with 3D-UNET closed division implementation Instancenorm is one of the normalization methods. Unlike BatchNorm, Instancenorm normalizes each instance on the batch independently. Since tensor has two memory formats, channel first and channel last, we need to consider optimization methods separately for these two cases.", "doc_id": 24}
{"doc": "Channel First (NCDHW)\nWe refer to the DHW direction as the reduce direction. For the channel first tensor, the memory is continuous in the reduce direction, so we can directly load the data into the register along the reduce direction to calculate the mean. And in the process of this calculation, each instance does not interfere with each other, so the tensor can be divided into N\u00d7C blocks for parallel calculation. The specific calculation method is shown in Figure 4.", "doc_id": 25}
{"doc": "Channel Last (NDHWC)\nFor the channel last tensor, since the memory is continuous in the channel direction, we can only load data along the channel direction.\n\nTo improve efficiency, we divide the reduce direction into blocks, such as D blocks, and calculate the mean and variance in parallel for each block, so that N\u00d7D means and variances are obtained. The specific steps are shown in the Figure 5 and 6.", "doc_id": 26}
{"doc": "BERT\nOptimizations performed with BERT closed division implementation implemented Fused MHA with AMX GEMM. Each core computes a complex depicted in Figure 7, maximizing both AMX efficiency and parallelism.", "doc_id": 27}
{"doc": "Memory and Cache Management\nAMX Linear optimization targets maintaining most of the data inside L2. We maximize weights reuse by minimizing shared states of L2. Specifically, we shard each weight in L2 as depicted in figure 6, compared to suboptimal situation (figure for B matrix load. Each sub-block of GEMM moves to the next shard of weight, which will be automatically copied to local L2 by snooping protocol, evicting stale data, ideally.", "doc_id": 28}
{"doc": "DLRM\nIn the DLRM closed submission, we leverage Intel AMX instruction to speed up MLP and interaction with oneDNN and customized kernels MLP operator is fused with following elementwise operator, such as relu, sigmoid. oneDNN also supports hybrid datatype fuse. For example, top mlp5 is fused with sigmoid, the weight, and input of this fused operator are s8, while output is fp32. Meanwhile, the other MLP operators are fused with relu, datatype of their output is s8. Row wise quantization is also supported in oneDNN to improve accuracy. The interaction operator is fused with embedding operator in a customized kernel to speed up memory bound operation. We optimize this kernel and reduce the number of instructions. The prefetch function provided by Intel x86 is also leveraged to overlap compute and communication so that the bandwidth utilization can reach peak.", "doc_id": 29}
{"doc": "Other major optimizations we have done so far are:\n\nAdjust cache locality for whole model to fit Sapphire Rapids\u2019 cache.\nFuse pre-processing and post-processing OPs.\nLeverage the intrinsic functions from Intel SVML library for speedup.", "doc_id": 30}
{"doc": "ResNet50\nWe utilized graph compiler technology to generate a performance-oriented kernel with various optimizations aimed at improving compute efficiency and reducing memory overhead, as outlined below:", "doc_id": 31}
{"doc": "Optimizations from Computational Graph Level\nWe have enabled the use of large partition to include multiple stages in the same outer loop along with batch axis. The layers within stage 1 and stage 2 are combined into the same outer loop and run with batch size 1, the layers within stage 3 are combined into another outer loop and run with batch size 2, while the layers within stage 4 are running separately, as the weight size becomes larger to be held in L2 cache for multiple layers.", "doc_id": 32}
{"doc": "We have improved the handling of padding attributes by promoting them into convolution\u2019s preceding op. This reduces the extra runtime overhead of memory copy during computational graph transformation, as shown in Figure 11.\nWe have made improvements to the residual-add operation for all bottlenecks with direct INT8, without the need for extra conversion overhead between FP32 and INT8 data types.", "doc_id": 33}
{"doc": "RetinaNet\nOptimizations performed with RetinaNet closed division implementation RetinaNet optimizations range from weight invariant model architecture modification to customized post-processing subroutines.", "doc_id": 34}
{"doc": "Avoiding Memory Expensive Concatenation\nFor each of regression and classification head, we eliminate the concatenation of the 5 tensors generated based on the output of feature pyramid network, saving significant memory ops. Instead, we keep the tensors in a tuple, with post-processing adapted accordingly to work on a tuple instead of one big tensor for each of regression and classification outputs.", "doc_id": 35}
{"doc": "Skipping Sigmoid Computation\nSelection of candidate bounding boxes for IOU calculation relies on the logits from the classification scores passing a set threshold (usually set to c=0.05). We postpone this sigmoid compute, utilizing monotonicity of the sigmoid, and applying a modified threshold directly on input classification scores.", "doc_id": 36}
{"doc": "Detection Boxes Post-processing\nWe utilized custom OpenMP parallelization to threshold and collect IDs of detection boxes meeting the above threshold. Unlike the default post-processing implementation, which uses the expensive masking complimentary routines of torch\u2019s greater and where methods.", "doc_id": 37}
{"doc": "Other optimizations include passing quantized int8 inputs directly to the graph, reducing framework overhead. We also use a histogram observation method to collect quantization statistics, helping to preserve accuracy while keeping activations in signed int8 format.", "doc_id": 38}
{"doc": "Optimization for LSTM\nWe shard inputs and split weights between the cores to maximize the reuse of L2 cache. We also adjust the order of computation by fusing the first Linear of LSTM Cells along layer axis to reuse the shared weight inside LSTM Layer. We also leverage AVX512-FP16 instructions to speed up post-ops in LSTM with rational polynomial approximated Sigmoid and Tanh implementation.", "doc_id": 39}
{"doc": "Optimization for Other Linear Layers\nFor other Linear layers, we split the first Linear of Joint Network into two separated Linear and an elementwise accumulation, fuse the two Linear, elementwise Add and ReLU to reduce memory footprint.", "doc_id": 40}
{"doc": "Intel Open Submission Result for MLPerf v3.0 Inference\nIntel submitted models to the Open category with additional algorithmic optimizations, in addition to the submissions to the Closed division mentioned above. The Open division models include BERT and RetinaNet models. Hardware-algorithm co-optimization techniques help further improve the performance of these benchmark models on 4th Gen Intel\u00ae Xeon\u00ae processors.", "doc_id": 41}
{"doc": "Pruning and Compression with Intel Neural Compressor\nNeural network pruning (briefly known as pruning or sparsity) is one of the most promising model compression techniques. It removes the least important parameters in the network and achieves compact architectures with minimal accuracy drop and maximal inference acceleration. As current state-of-the-art models have increasingly more parameters, pruning plays a crucial role in enabling them to run on devices whose memory footprints and computing resources are limited.", "doc_id": 42}
{"doc": "We implement INT8 sparse GEMM kernel to accelerate operations between dense input and sparse weights leveraging AVX512-VNNI instructions. Figure 1 illustrates how the kernel operates. Given a block-wise sparse weight, we broadcast the non-zero weight block to form a VNNI-format block A. Based on the mask in the sparse weight, we re-organize the corresponding input as another VNNI-format block B. Then, the kernel uses VNNI to produce the intermediate output given A and B, and add bias C as the final output.", "doc_id": 43}
{"doc": "Optimization with Joint Pruning Quantization and Distillation (JPQD)\nIntel developed JPQD as a single joint-optimization pipeline to improve transformer inference performance by pruning, quantization, and distillation in parallel during transfer learning of a pretrained transformer. JPQD alleviates the developer complexity of sequential optimization of different compression techniques. The output of JPQD is a structurally pruned, quantized model in OpenVINO IR, which is ready to deploy with OpenVINO runtimes optimized for a range of Intel devices. JPQD feature is available in OpenVINO suite of tools and can be invoked via Hugging Face Optimum-Intel.", "doc_id": 44}
{"doc": "In this submission, we applied JPQD on 3 pretrained BERT models, BERT-large, BERT-base, and MobileBERT, dialing structured sparsity to attain target F1 accuracy for SQuADv1.1, as well as MLPerf p99.9% and p99% target latency. We have submitted 4 optimized models (i.e., JPQD-BERT-large-99.9, JPQD-BERT-large-99, JPQD-BERT-base-99, JPQD-MobileBERT-99) to showcase flexibility of JPQD technology to optimize models for accuracy vs performance. Optimized models achieved up to 9X throughput improvement in offline scenario whereas server configuration achieved up to 16.6X improvement.", "doc_id": 45}
{"doc": "RetinaNet Optimization using BootstrapNAS Automated Neural Architecture Search\nIntel developed BootstrapNAS, a Neural Architecture Search (NAS)-based model optimization solution available in the Neural Network Compression Framework (NNCF). It uses a pre-trained model as input to automatically generate a weight-sharing super-network, and once this super-network has been trained, it searches for high-performing subnetworks. We used BootstrapNAS to discover a 4th Gen Intel Xeon-optimized RetinaNet model. As illustrated in the figure below, we achieved an additional 1.64x improvement in MLPerf\u2019s Server configuration and 1.52x in the Offline configuration. BootstrapNAS models are more efficient and improve the mean average precision (mAP) compared to the baseline models. More details about BootstrapNAS are available in our paper and submission results here.", "doc_id": 46}
{"doc": "Ease of use for Intel Submission of MLPerf Inference v3.0 Benchmarks\nMLPerf is an industry standard benchmark to measure platform AI capabilities for different usage scenarios. To save enabling effort for customers we also provided containers for four PyTorch based workloads. This allows users to enable MLPerf benchmark with minimal effort. Please refer to the README.md of each workload at https://github.com/mlcommons/inference_results_v3.0/tree/master/closed/Intel/code to pull MLPerf Intel containers and evaluate the AI workload performance on Sapphire Rapids.", "doc_id": 47}
{"doc": "Looking Ahead\nIntel is uniquely positioned to provide an ecosystem that caters to the demands of the ever-expanding frontiers of artificial intelligence. Our MLPerf v3.0 submissions underscores our industry commitment to providing recipes to our customers that unleash the full capabilities of Intel Xeon Scalable processors. We\u2019ll continue to innovate and make it easy to build and deploy Intel AI solutions at scale, along with delivering significant generational performance gains on follow-on products like Emerald Rapids and Granite Rapids.\n\nAll workload optimizations presented herein are either upstreamed to the respective frameworks or in the process.", "doc_id": 48}
{"doc": "Notices & Disclaimers\nPerformance varies by use, configuration and other factors. Learn more at our Performance Index page.\nPerformance results are based on testing as of dates shown in configurations and may not reflect all publicly available updates. See backup for configuration details. No product or component can be absolutely secure.\nYour costs and results may vary.\nIntel technologies may require enabled hardware, software or service activation.\n\u00a9 Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others.", "doc_id": 49}
{"doc": "TensorFlow 2.9.1 was the first release to include, by default, optimizations driven by the Intel\u00ae oneAPI Deep Neural Network (oneDNN) library, for 3rd Gen Intel \u00ae 3rd Xeon\u00ae processors (Cascade Lake). Since then, Intel and Google have continued our collaboration to introduce new TensorFlow optimizations for the next generation of Intel Xeon processors.", "doc_id": 50}
{"doc": "These optimizations accelerate TensorFlow models using the new matrix-based instructions set, Intel\u00ae Advanced Matrix Extension (AMX). The Intel AMX instructions are designed to accelerate deep learning operations such as matrix multiplication and convolutions that use Google\u2019s bfloat16 and 8-bit low precision data types. Low precision data types are widely used and provide significant improvement over the default 32-bit floating format without significant loss in accuracy.", "doc_id": 51}
{"doc": "We are happy to announce that these features are now available as a preview in the nightly build of TensorFlow on Github, and also in the Intel optimized build. TensorFlow developers can now use Intel AMX on the 4th Gen Intel\u00ae Xeon\u00ae Scalable processor (formerly known as Sapphire Rapids) using the existing mixed precision support available in TensorFlow. We are excited by the results - several popular AI models run up to 19x faster by moving from 3rd Gen to 4th Gen Intel Xeon processors using Intel AMX.", "doc_id": 52}
{"doc": "Intel\u2019s Advanced Matrix Extension (AMX) Accelerations in 4th Gen Intel Xeon Processor\nThe Intel\u00ae Advanced Matrix Extension (AMX) is an X86-based extension which introduces a new programming framework for dot products of two matrices. Intel AMX serves as an AI acceleration engine and builds on capabilities such as AVX-512 (for optimized vector operations) and Deep Learning Boost (through Vector Neural network Instructions for optimized resource utilization/caching and for lower precision AI optimizations) in previous generations of Intel Xeon processors.", "doc_id": 53}
{"doc": "In Intel AMX, a new type of 2-dimensional register file, called \u201ctiles\u201d, and a set of 12 new X86 instructions to operate on the tiles, are introduced. New instruction TDPBF16PS performs a dot product of bfloat16 tiles, and TDPBSSD performs dot product of signed 8-bit integer tiles. Other instructions include tile configuration and data movement to the Intel AMX unit. Further details can be found in the document published by Intel.", "doc_id": 54}
{"doc": "How to take advantage of AMX optimizations on 4th Gen Intel Xeon.\nIntel AMX optimizations are included in the official TensorFlow nightly releases. The latest stable release 2.11 includes preliminary support, however full support will be available in a subsequent stable release.", "doc_id": 55}
{"doc": "Users running TensorFlow on Intel 4th gen Intel Xeon can take advantage of the optimizations with minimal changes:\na) For bfloat16 mixed precision, developers can accelerate their models using Keras mixed precision API, as explained here. You can easily invoke auto mixed precision by including these lines in your code, that\u2019s it! \n```\nfrom tensorflow.keras import mixed_precision\npolicy = mixed_precision.Policy('mixed_bfloat16')\nmixed_precision.set_global_policy(policy)\n```\nb) Using Intel AMX with 8-bit quantized models requires the models to be quantized to use int8. Any existing standard models, for example RN50, BERT, SSD-RN34 that have been previously quantized with Intel Neural Compressor will run with no changes needed.", "doc_id": 56}
{"doc": "Performance improvements\nThe following charts show performance improvement on a 2-socket, 56-core 4th Gen Intel Xeon using Intel AMX low precision on various popular vision and language models, where the baseline is a 2-socket, 40-core 3rd Gen Intel Xeon with FP32 precision. We use Intel Optimization for TensorFlow* preview and the launch_benchmark script from Model Zoo for Intel\u00ae Architecture .", "doc_id": 57}
{"doc": "Here in the chart, inference with mixed precision models on a 4th Gen Intel Xeon was 1.9x to 9.6x faster than FP32 models on a 3rd Gen Intel Xeon. (BS=x indicates a large batch size, depending on the model)", "doc_id": 58}
{"doc": "Training models with auto-mixed-precision on a 4th Gen Intel Xeon was 2.3x to 5.5x faster than FP32 models on a 3rd Gen Intel Xeon.\nSimilarly, quantized model inference on a 4th Gen Intel Xeon was 3.3x to 19x faster than FP32 precision on a 3rd Gen Intel Xeon.\n\nIn addition to the above popular models, we have tested 100s of other models to ensure that the performance gain is observed across the board.", "doc_id": 59}
{"doc": "Next Steps\nWe are working to continuously tune and improve the Intel AMX optimizations in future releases of TensorFlow. We encourage users to optimize their AI models with Intel AMX on Intel 4th Gen processors to get a significant performance boost; not just for inference, but also for pre-training, fine tuning and transfer learning. We would like to hear from you, please provide feedback through the TensorFlow Github page or the oneAPI Deep Neural Network library GitHub page.", "doc_id": 60}
{"doc": "Acknowledgements\nThe results presented in this blog is the work of many people including the TensorFlow and oneDNN teams at Intel and our collaborators in Google\u2019s TensorFlow team.\nFrom Intel: Md Faijul Amin, Mahmoud Abuzaina, Gauri Deshpande, Ashiq Imran, Kanvi Khanna, Geetanjali Krishna, Sachin Muradi, Srinivasan Narayanamoorthy, Bhavani Subramanian, Yimei Sun, Om Thakkar, Jojimon Varghese, Tatyana Primak, Shamima Najnin, Mona Minakshi, Haihao Shen, Shufan Wu, Feng Tian, Chandan Damannagari.\nFrom Google: Eugene Zhulenev, Antonio Sanchez, Emilio Cota.", "doc_id": 61}
{"doc": "Since models are growing in the number of parameters, it increases the computational requirements at the same time. However, during inference, you want to allocate as less compute resources as possible for a service to run 24/7, simply to keep the operational costs low. Therefore, an active field of research is to reduce the model compute requirements in terms of computing for inference while keeping the original model performance.", "doc_id": 62}
{"doc": "This post presents one example of how to reduce the model size. It is based on the paper QuaLA-MiniLM: a Quantized Length Adaptive MiniLM. The paper was published in 2022 by Intel and accepted for NeurIPS 2022. It focuses on the problem of computational costs during the inference time of transformer-based language models.", "doc_id": 63}
{"doc": "The QuaLA-MiniLM model achieves a speedup of up to 8.8 times compared to the original model. To transform the original model to the optimized version, there are 3 steps necessary:\n1. MiniLM Distillation\n2. Length Adaptive Transformer\n3. Quantization\nEach step is presented in more detail below.", "doc_id": 64}
{"doc": "MiniLM Distillation\nMiniLM is based on a teacher-student model. It uses deep self-attention distillation to generate a smaller version of the Language Model (LM) with only a small loss in performance.", "doc_id": 65}
{"doc": "The first step, the student to learn the self-attention module of the last layer of the teacher model which contains rich linguistic information. This can be achieved by reducing the KL divergence between the student and teacher self-attention module. It is called Attention Transfer (see the image below).", "doc_id": 66}
{"doc": "The second step is called Value-Relation Transfer. Basically, it uses the values of the self-attention module of the teacher and the student as additional guidance during training. The value relation is the resulting loss of the KL-divergence between the scaled dot-product between the values of the student and teacher.", "doc_id": 67}
{"doc": "Following the original paper, if the difference between the number of the teacher and student for layers and the hidden size is more than 50%, it makes sense to have a teacher assistant model first and train the student supervised by the teacher assistant.", "doc_id": 68}
{"doc": "Length Adaptive Transformer (LAT)\nLength Adaptive Transformer is the second building block to reduce the computational footprint of a language model. It builds on the foundation of the PoWER-BERT model. PoWER-BERT in the original form uses only the embeddings for tokens with the highest significance. The significance is the amount of attention imposed by a one-word vector on the other word vectors. Keep in mind that PoWER-BERT has the same model parameters as BERT. However, the technique of PoWER-BERT reduces the inference time. There are cases where PoWER-Bert can be more accurate and more efficient than BERT. Too large models tend to overfit. You find a visualization of PoWER-Bert below.", "doc_id": 69}
{"doc": "Length Adaptive Transformer expands this idea with LengthDrop and LayerDrop. LengthDrop randomly removes tokens during the training phase. Before each Stochastic Gradient Decent (SGD) update, every layer is a length defined based on the previous length with a LengthDrop probability p. LengthDrop probability p describes the probability of an output token being dropped. It can be compared to dropout in classical Neural Networks. The image below illustrates what Drop-and-Restore looks like.", "doc_id": 70}
{"doc": "In addition, there is also LayerDrop which drops complete layers during the training phase. This enforces the tokens to be layer agnostic. Reducing the number of layers would improve the computational requirements, too. However, more importantly, it allows for the dropped tokens to be restored in the last layer.", "doc_id": 71}
{"doc": "However, to make it work in reality, there are a few more tricks necessary. One is the sandwich rule and inplace distillation. The sandwich rule describes an update process. In every training step, the full model is updated with the supervised loss function. Additionally, in every training loop, a set of randomly sampled sub-models (the sandwiches) and the smallest possible sub-model are trained using knowledge distillation from the full model. That means that the sub-models are trained to predict the output of the full model with a cross-entropy loss. To be clear: This means there is a training overhead at this point. The length-adaptive transformer is only more efficient during inference time.", "doc_id": 72}
{"doc": "After the training process, there is not only a single model but a whole set of models (all possible sub-models). Therefore, these sub-models can be analyzed and compared. The paper suggests using an evolutionary search to find a model with the best trade-off between accuracy and computational requirements or limitations, respectively.", "doc_id": 73}
{"doc": "Quantization\nQuantization is a well-known method used for a long time now. Basically, it is possible to reduce the precision used for the model weights from e.g. 32bit float to an 8bit representation. This not only reduces memory consumption but at the same time reduces computational effort.", "doc_id": 74}
{"doc": "There are different techniques for quantization, the paper is using zero-point quantization. Zeropoint quantization is shifting the distribution in the interval [-127, 127] by normalizing and shifting the zero-point. This affine transformation ensures that all 8 bits are used and therefore as much information as possible can be preserved.", "doc_id": 75}
{"doc": "Summary \u2014 QuaLA-MiniLM\nQuaLA-MiniLM combines these three elements from above to speed up a language model drastically. But to achieve the best possible compromise between accuracy and computational performance, all steps already focus on the final task (see the image below). Therefore, a task-specific dataset is necessary for this optimization.\nThis means, this increase in efficiency comes with two main drawbacks:\n1. It needs more computational power during training.\n2. It is a task-specific model, not a generic language model.", "doc_id": 76}
{"doc": "However, the results show are very promising. In this specific case, QuaLA-MiniLM speeds up the model compared to the original BERT model by a factor of 8.8 with a drop in accuracy by less than 1 percent. The latency times reported in the table below were tested on an Intel Xeon 8280 system.", "doc_id": 77}
{"doc": "Are you interested in efficiency optimization for large transformer models? Then have a look at my other articles. And if you have questions or comments, feel free to add them to the comment section. Thanks!", "doc_id": 78}
{"doc": "Hugging Face Transformers provide thousands of pretrained deep learning (DL) models to perform natural language processing (NLP) tasks such as text classification, information extraction, question answering, summarization, translation, text generation, and more in over 100 languages. It aims to make cutting-edge NLP accessible to everyone by providing APIs to quickly download and use its pretrained models on a given text, fine-tune them for your datasets, and then share them with the community on the model hub.", "doc_id": 79}
{"doc": "The AI ecosystem is evolving rapidly. More and more specialized hardware are emerging. Optimum Intel is an extension of Transformers that provides a set of performance optimization tools to efficiently train and run models on targeted hardware. As such, Optimum enables users to efficiently use any of these platforms with the same ease inherent to transformers. Recently, Intel\u2019s Neural Coder (powered by Intel Neural Compressor) has enabled the support of automatic enabling of INT8 quantization with the Optimum Intel API for Hugging Face Transformers model codes. It supports automatic enabling of post-training static and dynamic quantization for Transformers models.", "doc_id": 80}
{"doc": "Intel Neural Compressor is an open-source Python library for model compression that reduces the model size and increases DL inference performance on CPUs or GPUs. It supports post-training static and dynamic quantization of PyTorch models. It supports automatic accuracy-driven tuning strategies for users to easily generate quantized model. Users can easily apply static, dynamic, and aware-training quantization approaches while giving an expected accuracy criterion.", "doc_id": 81}
{"doc": "Neural Coder is a novel component of Intel Neural Compressor to further simplify the deployment of DL models via one-click automated code changes for device compatibility and optimization enabling. It performs automated benchmarking of optimization approaches to determine the best out-of-box performance. Neural Coder uses static program analysis and heuristics to help users take advantage of Intel DL Boost and hardware features to improve performance. This one-click enabling boosts developer productivity while making it easier to take advantage of acceleration.", "doc_id": 82}
{"doc": "Neural Coder supports all example codes that adopt Transformer\u2019s Trainer class, which means it can support 18 PyTorch tasks including common NLP tasks. With Neural Coder\u2019s one-click acceleration, users can enjoy a significant performance boost (~1.7x for the 3rd Gen Intel Xeon Scalable processor) for Hugging Face Transformers models on Intel Architecture with minimum accuracy loss, usually less than 1%.", "doc_id": 83}
{"doc": "The table below shows the data for several popular finetuned NLP models that run with run_glue.py for text classification example. The data was collected on the 3rd Gen Intel Xeon Scalable Processors. The \u201cPerformance Gain\u201d denotes the ratio between the inference speed of the accelerated model (by Neural Coder\u2019s one-click enabling of Optimum Intel API) and that of the original model. The \u201cAccuracy Delta\u201d denotes the change in model accuracy after acceleration. We can see that these fine-tuned Hugging Face Transformers models are significantly faster on Intel Architecture with minimal accuracy loss.", "doc_id": 84}
{"doc": "Getting Started\nTo use Neural Coder for Hugging Face Transformers models, Neural Coder provides several possibilities, including a Python Launcher and as a JupyterLab extension.\nPython Launcher\nLet\u2019s say we are running an NLP model with run_glue.py for the text classification task, a popular Hugging Face Transformers examples. We generally run the model with a Python command-line like this, with personalized parameters (e.g., model name, dataset name):\npython run_glue.py --model_name_or_path bert-base-cased --task_name mrpc --do_eval --output_dir result\n\nWith Neural Coder\u2019s Launcher, users can easily enjoy inference optimizations with Hugging Face\u2019s Optimum Intel API (default optimization: INT8 post-training static quantization by Intel Neural Compressor) by simply adding an inline prefix: -m neural_coder\nto the Python command line, and keep everything else unchanged:\npython -m neural_coder run_glue.py --model_name_or_path bert-base-cased --task_name mrpc --do_eval --output_dir result\nThis will run run_glue.py with Optimum Intel automatically enabled, while everything else remains exactly the same as the original code. Users can check out the optimized code run_glue_optimized.py auto-generated by the Launcher in the same folder to learn more about the Optimum Intel code enabling API.", "doc_id": 85}
{"doc": "JupyterLab Extension\nJupyterLab users can leverage Neural Coder\u2019s plugin jupyter-lab-neural-compressor to perform automatic quantization and benchmark evaluation for Hugging Face Transformers models. For more details, please refer to this documentation. JupyterLab users can search for our plugin \u201cjupyter-lab-neural-compressor\u201d in the Extension Manger and install it with just one click:\nUpon running the Neural Coder plugin on a HuggingFace notebook script, users can enjoy automatic enabling of INT8 quantization as shown in the below image, without manual coding:", "doc_id": 86}
{"doc": "Conclusion\nWith Neural Coder\u2019s one-click enabling of INT8 quantization, HuggingFace Transformers model users can easily accelerate the inference of their DL models without coding manually. Neural Coder gives a productivity boost for HuggingFace Transformers models, and it demonstrates significant inference acceleration on Intel architectures with minimal accuracy loss.", "doc_id": 87}
{"doc": "Visual Studio Code (VS Code) is a source code editor made by Microsoft with the Electron Framework for Windows, Linux, and macOS. Features include support for debugging, syntax highlighting, intelligent code completion, snippets, code refactoring, and embedded Git. VS Code is built with extensibility in mind. It offers many extensions to customize almost every part of VS Code. Neural Coder is one such extension. It is a novel component of Intel Neural Compressor that simplifies deployment of deep learning (DL) models via one-click automated code changes for device compatibility and optimization.", "doc_id": 88}
{"doc": "Intel Neural Compressor is an open-source Python library for model compression that reduces model size and improves DL inference performance on CPUs or GPUs. It supports post-training static and dynamic quantization of PyTorch models and automatic accuracy-driven tuning strategies for users to easily generate quantized models. The users can easily apply static, dynamic, and aware-training quantization approaches while giving an expected accuracy criterion.", "doc_id": 89}
{"doc": "Neural Coder can perform automated benchmarking of optimization approaches to determine the best out-of-box performance. It uses static program analysis and heuristics to help users take advantage of Intel DL Boost and hardware features to improve performance. This one-click enabling boosts developer productivity while making it easier to take advantage of acceleration. We provide here a detailed step-by-step guide to using the Neural Coder extension in VS Code.", "doc_id": 90}
{"doc": "First, if you\u2019re not already using VS Code on a Linux system, open VS Code Extension and link to a remote Linux server via SSH.\nSecond, search for Neural Coder extension in the VS Code extensions marketplace. Just click \u201cInstall\u201d when you see the icon below. (Note: The installation location should be the SSH remote server to which you are connected, if you\u2019re using VS Code on a Windows system.)\nThird, click the settings icon and select \u201cExtension Settings\u201d to enter the path to your preferred Python version.\nFourth, open the deep learning code that you want to quantize and evaluate. You should see a new icon in the upper right pane and sidebars on the left showing your operation history.\nFifth, click the Neural Coder button at the top right and select the optimization (quantization) that you want to run on your code. Select \u201cINC Enable INT8 (Static),\u201d \u201cINC Enable INT8 (Dynamic),\u201d or \u201cINC Enable BF16\u201d then wait loading to complete.", "doc_id": 91}
{"doc": "Post-Training Quantization\nPTQ is an effective model compression approach to quantize a model without additional training. It requires calibration using a representative dataset to determine the quantization parameters (e.g., scale, zero point) that will satisfy the required accuracy. Intel Neural Compressor provides easy-to-use quantization support with accuracy-aware tuning on Intel platforms. In a quantized model, INT8 operations can improve inference efficiency by up to 4x over FP32 operations via Intel Deep Learning Boost (DL Boost) on Intel Xeon Scalable processors with Intel Advanced Matrix Extensions (AMX).", "doc_id": 92}
{"doc": "Stable Diffusion\nStable Diffusion is a latent text-to-image diffusion model that can generate photorealistic images given any text input. This popular model is released as open source in Hugging Face diffusers. The diagram below illustrates the default inference flow of Stable Diffusion in FP32 from a given user prompt to an output image (see this Jupyter notebook):", "doc_id": 93}
{"doc": "We can see from the flow that there is a loop surrounding \u201ctext conditioned latent UNet\u201d driven by a scheduler algorithm. The PNDMScheduler with 50 iterations of latent denoising are used by default. Profiling indicates that UNet is the most time-consuming step of the workflow.", "doc_id": 94}
{"doc": "Optimizing Stable Diffusion Inference\nLet\u2019s try quantizing UNet to see how it can improve inference efficiency. Below is the updated inference flow with INT8 UNet:", "doc_id": 95}
{"doc": "We describe the instructions and sample code to quantize UNet for Stable Diffusion using the technologies provided by Intel Neural Compressor. First, install Optimum Intel: pip install optimum[neural-compressor]", "doc_id": 96}
{"doc": "Second, add a few lines of code to enable quantization, as shown below for the key quantization APIs (e.g., IncQuantizer):\n# pip install diffusers[\"torch\"] transformers\nfrom diffusers import StableDiffusionPipeline\nfrom optimum.intel.neural_compressor import IncQuantizer, IncOptimizer\n...\npipe = StableDiffusionPipeline.from_pretrained('fp32_model')\n\n# prepare calibration and eval func\nquantizer = IncQuantizer(config,\n                eval_func=eval_func,\n                calib_dataloader=DataLoader(...),\n                calib_func=calibration_func\n            )\n\n# quantize model\nmodel = getattr(pipe, 'unet')\noptimizer = IncOptimizer(model, quantizer=quantizer)\nopt_model = optimizer.fit()\n\n# run pipeline using opt model \nsetattr(pipe, 'unet', opt_model)\nimage = pipe('sample prompt',\n             guidance_scale=7.5,\n             num_inference_steps=50,\n             generator=generator,\n             num_images_per_prompt=1,\n             ).images[0]\n                   \nimage.save(\"int8.png\")\nYou can review the complete code for more details. An INT8 model is generated after running the example. Note that we use the same seed to generate the image to avoid the potential randomness of text-to-image generation.", "doc_id": 97}
{"doc": "We next compare the quality of the images generated by the FP32 and INT8 models, with the ground truth image for the same text prompt. To evaluate image quality, we use FID, a popular metric to measure the similarity of two sets of images (or two images). It has been shown to correlate well with subjective human judgment of visual quality.", "doc_id": 98}
{"doc": "As you can see, the INT8-generated image looks more like the ground truth image. The FID score of the INT8 and ground truth images is 246, which is lower than the FP32 and ground truth score (333). Moreover, Intel Neural Compressor delivers twice the performance on Intel Xeon Scalable processors compared to the basic quantization capability provided by default PyTorch.", "doc_id": 99}
{"doc": "Concluding Remarks\nWe demonstrate the effectiveness of post-training quantization in Stable Diffusion using Optimum Intel with Intel Neural Compressor to achieve 4x model size reduction after quantizing UNet. We plan to explore other model compression techniques (e.g., knowledge distillation) to further improve inference efficiency while maintaining the similar quality.\nPlease visit Intel Neural Compressor and AI Kit pages for more details. You are also welcomed to create pull requests or submit issues through GitHub. We look forward to your feedback and suggestions!", "doc_id": 100}
{"doc": "Large transformer language models (LM) that scale up to billions of parameters have demonstrated state-of-the-art performance across a wide variety of natural language processing (NLP) tasks. The real-world deployment of such models however remains limited due to their slow speed and heavy compute demands.", "doc_id": 101}
{"doc": "Researchers from Intel Corporation and Intel Labs address this issue in the new paper Fast DistilBERT on CPUs, proposing a pipeline and hardware-aware extreme compression technique for creating and running fast transformer models on CPUs. The approach achieves impressive speed ups and SOTA performance in production environments.", "doc_id": 102}
{"doc": "The team summarizes their main contributions as follows:\n1. Propose a hardware-aware extreme compression technique for fast transformer models on CPUs.\n2. Create an efficient transformer inference runtime for sparse & quantized transformer models.\n3. Demonstrate new SOTA performance under typical constraints in common production environments.", "doc_id": 103}
{"doc": "To apply the proposed model compression technique to transformer-based LMs, the researchers first use specialized sparse GEMM operators to accelerate sparse transformer models and extend Zafrir et al.\u2019s model compression infrastructure to create sparse pretrained LMs with block-wise structured sparsity. They then fine-tune the models with knowledge distillation while fine-tuning the block-wise sparse pre-trained LM to downstream tasks to bridge the accuracy gap caused by compression methods. Finally, they apply post-training quantization with automatic accuracy-aware tuning to optimize the model.", "doc_id": 104}
{"doc": "For software acceleration, the team develops a transformer inference engine on a CPU with advanced runtime, graph optimization and sparse GEMM operators.", "doc_id": 105}
{"doc": "In their empirical study, the team trained a block-wise sparse pretrained DistilBERT model and applied the proposed approach while fine-tuning it on the question-answering SQuADv1.1 benchmark. Their resulting Fast DistilBERT model surpasses the runtime performance of Neural Magic\u2019s state-of-the-art DeepSparse model by up to 50 percent with only a minimal loss in accuracy and achieves 4.1x better performance than ONNX Runtime.", "doc_id": 106}
{"doc": "The novel combination of block-wise structured sparsity, knowledge distillation and quantization that define this hardware-aware model compression technique enables transformer-based LMs to run efficiently on CPUs. The researchers plan to apply their method to other common transformer models to further test its inference efficiency.\nThe paper Fast DistilBERT on CPUs has been accepted by the 36th Conference on Neural Information Processing Systems (NeurIPS 2022) and is available on arXiv.", "doc_id": 107}
{"doc": "Stable Diffusion is a state-of-the-art latent text-to-image diffusion model that generates photorealistic images from text. It has recently become popular for creating realistic and futuristic images based on the user\u2019s imagination. However, training Stable Diffusion is expensive, taking 150K hours on 256 Nvidia A100 GPU cards as described in model card.", "doc_id": 108}
{"doc": "In this blog, we describe how to create personalized Stable Diffusion models through few-shot fine-tuning. We just use one image to fine-tune stable diffusion on a single CPU and demonstrate the inference of text-to-image. To the best of our knowledge, this is the first demonstration of an end-to-end stable diffusion workflow from fine-tuning to inference on a CPU.\nWe plan to introduce low-precision optimizations using Intel Neural Compressor in future articles to boost inference performance.", "doc_id": 109}
{"doc": "Personalized Stable Diffusion adds new concepts (also called objects) to be recognized by the model while maintaining the capabilities of the pretrained model on text-to-image generation. Here is a sample concept \u201cdicoo\u201d from the Stable Diffusion concepts library:\nTextual Inversion is a technique to understand new concepts from a small number of images in a way that can later be used in text-to-image pipelines, where new text objects are learned in the embedding space of the text encoder in the pipeline while the other model parameters are frozen. This technique meets the requirements for creating a personalized Stable Diffusion.", "doc_id": 110}
{"doc": "Next, we use \u201cdicoo\u201d as an example to demonstrate the few-shot fine-tuning to create our own Stable Diffusion. First, we prepare the images similar to other fine-tuning tasks. We are using only one image, so we select one random image from the concepts library. We leveraged the fine-tuning script provided by Textual Inversion with one prepared image under train_data_dir and \u201cdicoo\u201d for placeholder_token (as a new object):", "doc_id": 111}
{"doc": "Hugging Face\u2019s diffusers library provides a high-level device abstraction, so it\u2019s relatively easy to migrate the default model script and run the fine-tuning on a CPU. Moreover, we are enabling automatic mixed-precision to accelerate fine-tuning by using BFloat16 on Intel processors. Finally, we generate our own Stable Diffusion on a single CPU in less than three hours. And now, let\u2019s test it!", "doc_id": 112}
{"doc": "We use the prompt \u201ca lovely <dicoo> in red dress and hat, in the snowy and brightly night, with many brightly buildings\u201d and generate the below image:\nAs you can see the generated image looks pretty good. The new object \u201cdicoo\u201d is well-captured, while the other objects and relevant text semantics remain. We consider this a successful personalized Stable Diffusion!", "doc_id": 113}
{"doc": "We released all the code and scripts under Intel\u00ae Neural Compressor. Please check it out now and create your own Stable Diffusion!\nAs mentioned early, our next step is to enable low-precision optimizations to accelerate the inference on Intel platforms. Please star this project if you like it so you will receive notifications about our latest optimizations.\nPlease visit the Intel Neural Compressor and AI Kit pages and feel free to reach us if you have any questions!", "doc_id": 114}
{"doc": "Intel\u00ae Extension for TensorFlow*\nWe are excited to announce Intel\u00ae Extension for TensorFlow*, an open-source solution to run TensorFlow applications on Intel AI hardware. Intel\u00ae Extension for TensorFlow* is a high-performance deep learning extension implementing the TensorFlow* PluggableDevice interface. Through seamless integration with TensorFlow framework, it allows Intel XPU (GPU, CPU, etc.) devices readily accessible to TensorFlow developers. With the Intel Extension, developers can train and infer TensorFlow models on Intel AI hardware with zero code change.", "doc_id": 115}
{"doc": "Intel\u00ae Extension for TensorFlow* is built on top of oneAPI software components. Most performance-critical graphs and operators are highly optimized by using Intel\u00ae oneAPI Deep Neural Network (oneDNN), an open-source, cross-platform performance library for Deep Learning applications. Other operators are implemented with SYCL*, one API\u2019s core language for programming accelerators and multiprocessors. The plug-in also maintains a local Eigen math library which ports Eigen to the SYCL* language so that it can generate SYCL* kernel programs to implement device operators. Intel\u00ae Extension for TensorFlow implements all the TensorFlow GPU operators, which ensures good performance and sufficient model coverage for Intel GPUs.", "doc_id": 116}
{"doc": "Intel\u00ae Extension for TensorFlow* Features\nIntel\u00ae Extension for TensorFlow* maintains the same user experience with TensorFlow public Python API, such as device management API (tf.config.list_physical_devices), model execution API (v1\u2019s Session graph API and v2\u2019s eager and graph API), Keras Mixed precision API, profiler API (tf.profiler.experimental.start ). Thus, you can run existing application code on Intel GPU without any changes. Currently, Intel\u00ae Extension for TensorFlow* supports both Intel CPU and Intel GPU on Linux* and WSL (Windows Subsystem for Linux*).", "doc_id": 117}
{"doc": "Intel\u00ae Extension for TensorFlow* provides good performance with the default configuration. It also offers several features through simple Python APIs or environment variables for advanced users to get additional performance:\n\nAdvanced Auto Mixed Precision\nIntel\u00ae Extension for TensorFlow* is fully compatible with Keras mixed precision API in TensorFlow. It also provides an advanced auto mixed precision feature. For example, you can just set two environment variables to get the performance benefit from low-precision data type FP16/BF16 without changing the application code.\nexport ITEX_AUTO_MIXED_PRECISION=1&nbsp;&nbsp;\nexport ITEX_AUTO_MIXED_PRECISION_DATA_TYPE=\"BFLOAT16\" # or \"FLOAT16\"\n\nTF32 Math Mode\nIntel\u00ae Iris\u00ae Xe GPU Matrix Engine in Ponte Vecchio Xe-HPC GPU natively supports the TF32 math mode. Intel\u00ae Extension for TensorFlow* enables TF32 math mode on Ponte Vecchio with one line:\nexport ITEX_FP32_MATH_MODE=\"TF32\"", "doc_id": 118}
{"doc": "Int8 Quantization\nIntel\u00ae Extension for TensorFlow* leverages Intel\u00ae Neural Compressor to provide a compatible TensorFlow INT8 quantization solution. You may reuse the existing TensorFlow INT8 models or write minimal lines of code to generate a new TensorFlow INT8 model as below:\n```\nfrom neural_compressor.experimental import Quantization, common\nquantizer = Quantization()\nquantizer.model = fp32_model\ndataset = quantizer.dataset('dummy', shape=(1, 224, 224, 3))\nquantizer.calib_dataloader = common.DataLoader(dataset)\nint8_model = quantizer.fit()\n```\n\nAuto-Tuned CPU Launcher [Experimental]\nIntel\u00ae Extension for TensorFlow* provides an auto-tuned CPU Launcher to help user to get better performance when running the TensorFlow applications on the CPU. Tensorflow application performance on CPU is highly influenced by various configurations, such as workload instance number, thread number per instance, thread affinity, memory allocator, NUMA memory placement policy, etc. There is no single configuration that is optimal for all topologies. The CPU launcher automates these configuration settings to simplify model deployment workflow. Users may use the following command:\nLatency mode: python -m intel_extension_for_tensorflow.python.launch --latency_mode infer_resnet50.py\nThroughput mode: python -m intel_extension_for_tensorflow.python.launch --throughput_mode infer_resnet50.py\nAggressive Fusion through oneDNN Graph [Experimental]", "doc_id": 119}
{"doc": "Intel\u00ae Extension for TensorFlow* can offload performance-critical graph partitions to oneDNN library through oneDNN Graph API to get more aggressive graph optimizations such as Multi-Head Attention (MHA) fusion for Transformers. This feature is experimental and under active development. Currently, users can enable this feature with one environment variable.", "doc_id": 120}
{"doc": "Enabling Intel\u00ae Extension for TensorFlow*\nThis section uses an example to show how to enable TensorFlow applications on Intel GPU with Intel\u00ae Extension for TensorFlow*.\nInstallation\n1. Pre-Installation Requirements\nInstall Intel GPU driver\n    \u2022  If running an Intel\u00ae Data Center GPU Flex Series, please find drivers here\n    \u2022  For other Intel GPUs (experimental), please find GPU driver installation guides here\nInstall Intel\u00ae oneAPI Base Toolkit : \n    \u2022  Install Intel\u00ae oneAPI Base Toolkit Packages 2022.3\n2. Setup environment variables\nsource /opt/intel/oneapi/setvars.sh\n3. Install the latest Tensorflow and Intel\u00ae Extension for Tensorflow*\npip install tensorflow==2.10.0\npip install --upgrade intel-extension-for-tensorflow[gpu]", "doc_id": 121}
{"doc": "Check Physical Devices\nYou can use tf.config.list_physical_devices() to check all available physical devices:\n```\nimport tensorflow as tf\ntf.config.list_physical_devices()\n```\nOutput\n```\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n&nbsp;PhysicalDevice(name='/physical_device:XPU:0', device_type='XPU')]\n```\nYou can see both CPU and XPU in a machine with Intel GPU installed. \u201cXPU\u201d is the device type registered by Intel\u00ae Extension for TensorFlow* to represent Intel GPU, as the counterpart of \u201cGPU\u201d in TensorFlow.", "doc_id": 122}
{"doc": "Using Intel GPU in TensorFlow Model\nTensorFlow PluggableDevice takes priority over native devices, so if you don\u2019t explicitly specify the device through tf.device(), it will run on Intel GPU by default without any code change. If explicitly specify device, only one line change is required: \n```\nimport tensorflow as tf   # TensorFlow registers PluggableDevices here.\ntf.config.list_physical_devices()  # XPU device is visible to TensorFlow.\n[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XPU:0', device_type='XPU')]\n\n#Section 1 Run implicitly \na = tf.random.normal(shape=[5], dtype=tf.float32)  # Runs on XPU.\nb = tf.nn.relu(a)         # Runs on XPU .\n\n#Section 2 Run with explicit device setting \nwith tf.device(\"/XPU:0\"):  # Users can also use 'with tf.device' syntax.\n  c = tf.nn.relu(a)        # Runs on XPU.\nwith tf.device(\"/CPU:0\"):\n  c = tf.nn.relu(a)        # Runs on CPU.\n\n#Section 3 Run with graph mode\n@tf.function  # Defining a tf.function\ndef run():\n  d = tf.random.uniform(shape=[100], dtype=tf.float32) \n  e = tf.nn.relu(d)        \nrun()  # PluggableDevices also work with tf.function and graph mode. Runs on XPU\n```\nDevelopers can visit the online document website, and then get started with a tour of Intel\u00ae Extension for TensorFlow* examples.", "doc_id": 123}
{"doc": "Summary and Future Work\nIntel\u00ae Extension for TensorFlow* is a high-performance deep learning extension plugin based on TensorFlow PluggableDevice interface to bring the first Intel GPU product Intel\u00ae Data Center GPU Flex Series 170 into TensorFlow ecosystem for AI workload acceleration.\nFor product quality CPU support, we recommend you to use TensorFlow and Intel\u00ae Optimization for TensorFlow*. Intel\u00ae Extension for TensorFlow* provides experimental CPU support.\nWe are continuously enhancing Intel\u00ae Extension for TensorFlow* on functionality and performance. With further product development, more Intel devices will be added to the XPU-supported list. We are also exploring more features, such as:\n\nExtending the support for OpenXLA and JAX.\nAdding an XPU heterogeneous execution runtime to efficiently schedule TensorFlow applications on both CPU and GPU to get better system hardware utilization\nWe welcome the community to evaluate the new AI solution and contributions to Intel\u00ae Extension for TensorFlow*.", "doc_id": 124}
{"doc": "Intel Neural Compressor is an open-source Python* library for model compression that reduces the model size and increases the speed of deep learning (DL) inference on CPUs or GPUs (Figure 1). It provides unified interfaces across multiple DL frameworks for popular network compression technologies, such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help the user quickly find the best quantized model. It also implements different weight pruning algorithms to generate pruned models using a predefined sparsity goal and supports knowledge distillation from the teacher model to the student model. Intel Neural Compressor provides APIs for a range of frameworks including TensorFlow*, PyTorch*, and MXNet* in addition to ONNX* runtime for greater interoperability across frameworks. We will focus on the benefits of using the tool with a PyTorch model.", "doc_id": 125}
{"doc": "Intel Neural Compressor extends PyTorch quantization by providing advanced recipes for quantization and automatic mixed precision, and accuracy-aware tuning. It takes a PyTorch model as input and yields an optimal model. The quantization capability is built on the standard PyTorch quantization API and makes its own modifications to support fine-grained quantization granularity from the model level to the operator level. This approach gives better accuracy without additional hand-tuning.", "doc_id": 126}
{"doc": "It further extends the PyTorch automatic mixed precision feature on 3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors with support for INT8 in addition to BF16 and FP32. It first converts all the quantizable operators from FP32 to INT8, and then converts the remaining FP32 operators to BF16, if BF16 kernels are supported on PyTorch and accelerated by the underlying hardware (Figure 2).", "doc_id": 127}
{"doc": "Intel Neural Compressor also supports an automatic accuracy-aware tuning mechanism for better quantization productivity. It first queries the framework for the quantization capabilities, such as quantization granularity (per_tensor or per_channel), quantization scheme (symmetric or asymmetric), quantization data type (u8 or s8), and calibration approach (min-max or KL divergence) (Figure 3). Then it queries the supported data types for each operator. With these queried capabilities, the tool generates a whole tuning space of different sets of quantization configurations and starts the tuning iterations. For each set of quantization configurations, it performs calibration, quantization, and evaluation. Once the evaluation meets the accuracy goal, the tool terminates the tuning process and produces a quantized model.", "doc_id": 128}
{"doc": "Pruning is mainly focused on unstructured and structured weight pruning and filter pruning. Unstructured pruning uses a magnitude algorithm to prune weights during training when their magnitude is below a predefined threshold. Structured pruning implements experimental tile-wise sparsity kernels to boost the performance of the sparsity model. Filter pruning implements a gradient-sensitivity algorithm that prunes the head, intermediate layers, and hidden states in the model according to the importance score calculated by the gradient.", "doc_id": 129}
{"doc": "Intel Neural Compressor also implements a knowledge distillation algorithm to transfer knowledge from a large \u201cteacher\u201d model to a smaller \u201cstudent\u201d model without loss of validity (Figure 4). The same input is fed to both models, and the student model learns by comparing its results to both the teacher and the ground-truth label.", "doc_id": 130}
{"doc": "The following example shows how to quantize a natural language processing model with Intel Neural Compressor:\n```\n# config.yaml\nmodel:\n  name: distilbert\n  framework: pytorch_fx\ntuning:\n  accuracy_criterion:\n    relative: 0.01\n\n# main.py\nimport torch\nimport numpy as np\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer\n)\n\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n)\n\n# Calibration dataloader\nclass CalibDataLoader(object):\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.sequence = \"Shanghai is a beautiful city!\"\n        self.encoded_input = self.tokenizer(\n            self.sequence,\n            return_tensors='pt'\n        )\n        self.label = 1 # negative sentence: 0; positive sentence: 1\n        self.batch_size = 1\n\n    def __iter__(self):\n        yield self.encoded_input, self.label\n\n# Evaluation function\ndef eval_func(model):\n    output = model(**calib_dataloader.encoded_input)\n    print(\"Output: \", output.logits.detach().numpy())\n    emotion_type = np.argmax(output.logits.detach().numpy())\n    return 1 if emotion_type == calib_dataloader.label else 0\n\n# Enable quantization\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization('./config.yaml')\nquantizer.model = model\nquantizer.calib_dataloader = CalibDataLoader()\nquantizer.eval_func = eval_func\nq_model = quantizer.fit()\n```\nNote that the generated mixed-precision model may vary, depending on the capabilities of the low precision kernels and the underlying hardware (e.g., INT8/BF16/FP32 mixed-precision model on 3rd Gen Intel Xeon Scalable Processors).", "doc_id": 131}
{"doc": "Performance Results\nIntel Neural Compressor has validated 400+ examples with a performance speedup geomean of 2.2x on an Intel Xeon Platinum 8380 Processor with minimal accuracy loss (e.g., Table 1). More details for validated models are available here.", "doc_id": 132}
{"doc": "The vision of Intel Neural Compressor is to improve productivity and solve the issues of accuracy loss by an auto-tuning mechanism and an easy-to-use API when applying popular neural network compression approaches. We are continuously improving this tool by adding more compression recipes and combining those techniques to produce optimal models. We invite users to try Intel Neural Compressor and provide feedback and contributions via the GitHub repo.", "doc_id": 133}
{"doc": "Alibaba and Intel have collaborated to improve developer productivity by combining three key components:\n1. The Neural Coder component of Intel Neural Compressor\n2. The Alibaba Cloud Machine Learning Platform for AI (PAI)\n3. The BladeDISC open-source compiler for ML workloads", "doc_id": 134}
{"doc": "Neural Coder simplifies deployment of deep learning (DL) models via one-click, automated code changes (e.g., to switch accelerator devices or enable optimizations). It uses static program analysis and heuristics to help users take advantage of Intel DL Boost and hardware features to improve performance. This one-click enabling boosts developer productivity while making it easier to take advantage of acceleration. This can all be done through a convenient JupyterLab GUI extension to Neural Coder.", "doc_id": 135}
{"doc": "PAI provides end-to-end machine learning (ML) services, including data processing, feature engineering, model training, model prediction, and model evaluation. The Data Science Workshop (DSW) of PAI is an integrated development environment in the cloud. It integrates JupyterLab and provides plug-ins for customized development. BladeDISC is one of the key components of PAI-Blade inference accelerator. BladeDISC provides general, transparent, and easy performance optimization for TensorFlow and PyTorch workloads on CPU and GPU backends.", "doc_id": 136}
{"doc": "Neural Coder has been integrated into PAI-DSW, and includes BladeDISC as one of its optimization backends (Figure 1). This simplifies access to the inference acceleration that BladeDISC provides. For example, the DL script of Hugging Face\u2019s Albert Model can be one-click optimized using the \u201cAlibaba Blade-DISC\u201d option of the Neural Coder extension (Figure 2).", "doc_id": 137}
{"doc": "The \u201cAuto Benchmark\u201d option shown in Figure 1 tells Neural Coder to run benchmarks to inform automatic code optimization. It compares before and after performance and provides a benchmark log below the code cell of the Jupyter notebook (Figure 3).", "doc_id": 138}
{"doc": "Intel and Alibaba collaborated to enhance DL productivity by providing a no-code solution to enable Alibaba Blade-DISC optimization. The integration of Neural Coder into Alibaba Cloud PAI DSW greatly simplifies the deployment of Alibaba Blade-DISC optimization, and can help users accelerate their DL models with just one click.", "doc_id": 139}
{"doc": "DL practitioners can try out this extension in Alibaba Cloud PAI DSW platform by creating a DSW g7 CPU instance in PAI. Alternatively, you can try the standalone version of Neural Coder extension in JupyterLab. We also encourage you to check out Intel\u2019s other AI Tools and Framework optimizations and learn about the unified, open, standards-based oneAPI programming model that forms the foundation of Intel\u2019s AI Software Portfolio.", "doc_id": 140}
{"doc": "Intel Neural Compressor is an open-source Python library for model compression that reduces the model size and increases the speed of deep learning inference for deployment on CPUs or GPUs. It provides unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, knowledge distillation and neural architecture search.", "doc_id": 141}
{"doc": "In this blog, we demonstrate how to use Intel Neural Compressor to distill and quantize a BERT-Mini model to accelerate inference while maintaining the accuracy. This example uses the SST-2 dataset, which is part of the General Language Understanding Evaluation benchmark set. Our distilled and quantized BERT-Mini model shows:\n1. A 18x speedup over BERT-Base (PyTorch) while keeping similar accuracy.\n2. An accuracy improvement of 6.9% on the SST-2 dataset over default BERT-Mini. This improvement is mostly due to GPT-2 data augmentation, which is planned for Intel Neural Compressor.", "doc_id": 142}
{"doc": "Orchestration of Compression Methods\nArbitrary meaningful combinations of supported compression methods under one-shot or multi-shot are supported in Intel Neural Compressor. Such orchestration brings extra benefits in terms of accuracy or performance. Possible combinations could be distillation during pruning and quantization-aware training, or distillation and then post-training quantization.", "doc_id": 143}
{"doc": "Distillation\nKnowledge distillation (KD) from a large model to a much simpler architecture shows promising results for reducing model size and computational load while preserving much of the original model\u2019s accuracy (Tang et al., 2019 and Wasserblat et al., 2020). A typical KD setup has two stages (Figure 1). In the first stage, a large, cumbersome, and accurate teacher neural network for a specific downstream task is trained. In the second stage, a smaller and simpler student model is trained to mimic the behavior of the teacher. This model is easier to deploy in environments with limited resources. To simplify the system, we only use a single loss generated for each training batch by calculating the mean-squared error (MSE) distance between the target predictions produced by the student and teacher models.", "doc_id": 144}
{"doc": "Quantization\nQuantization is a widely used compression technique that can reduce model size while also improving inference and training latency. The full-precision model weights are converted to low-precision. The inference performance of the quantized model can improve by reducing memory bandwidth and accelerating computations with low precision instructions, with little degradation in model accuracy. Intel provides several lower-precision instructions (e.g., 8- or 16-bit multipliers) that benefit training and inference. Please see this article on lower numerical precision inference and training in deep learning for more details.", "doc_id": 145}
{"doc": "Example\nThe code snippets below show how to train a BERT-Mini model on the SST-2 dataset through distillation, and then leverage quantization to accelerate inference while maintaining accuracy using Intel Neural Compressor. The complete code is available in the noBERT-Mini SST-2 notebook.\nDistillation\n```\nfrom transformers import Trainer\nimport torch.nn.functional as F\nclass RegressionTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        outputs = model(**inputs)  \n        logits = outputs.logits\n        loss = F.mse_loss(logits, labels)\n        return (loss, outputs) if return_outputs else loss\ntrainer = RegressionTrainer(\n    student_model,\n    args=training_args,\n    train_dataset=ds[\"train\"],\n    eval_dataset=ds[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics_for_regression,\n)\ntrainer.train()\ntrainer.save_model(model_output_dir)\n```\nQuantization\n```\nfrom neural_compressor.experimental import Quantization, common\nos.environ[\"GLOG_minloglevel\"] = \"2\"\nmodel_input = \"onnx_bert_mini/model.onnx\"\nmodel_output = \"bert-mini_engine_int8\"\nquantizer = Quantization(\"bert-mini_engine_static.yaml\")\nquantizer.model = common.Model(model_input)\nquantizer.eval_dataloader = common.DataLoader(DatasetForINC(ds[\"validation\"], engine_order=True), 1)\nquantizer.calib_dataloader = common.DataLoader(DatasetForINC(ds[\"validation\"], engine_order=True), 1)\nq_model = quantizer()\nq_model.save(model_output)\n```", "doc_id": 146}
{"doc": "Results\nThe example above first trains a BERT-Mini model on the SST-2 dataset through distillation to obtain comparable accuracy to BERT-Base. Next, it leverages quantization and the reference natural language processing (NLP) engine to improve the inference performance of the BERT-Mini model while maintaining its accuracy. The reference NLP engine is a framework for high-performance NLP model inference. This engine uses hardware and software optimizations to boost performance of extremely compressed NLP models:\n1. The baseline accuracy of BERT-Base on the SST-2 dataset is 94.0%. BERT-Mini using knowledge distillation is able to achieve 92.8% accuracy, which is an improvement over BERT-Mini without distillation (85.9%).\n2. With quantization and the reference NLP engine, BERT-Mini (Reference NLP Engine INT8) achieves a 18x speedup compared to BERT-Base (PyTorch FP32) \u2014 0.90 vs. 16.94 millisecond latency \u2014 while maintaining comparable accuracy (92.8% vs. 94%).", "doc_id": 147}
{"doc": "Future Work\nWe demonstrated how to use Intel Neural Compressor to optimize a BERT-Mini model on the SST-2 dataset through orchestration of distillation and quantization to accelerate inference while maintaining the accuracy. We will try incorporating the pruning approach to further accelerate this model\u2019s inference in the future. We invite users to explore this example and send us feedback through Github issues. We also encourage you to check out Intel\u2019s other AI Tools and Framework optimizations and learn about the unified, open, standards-based oneAPI programming model that forms the foundation of Intel\u2019s AI Software Portfolio.", "doc_id": 148}
{"doc": "Intel Neural Compressor is an open-source Python library for model compression. It reduces the model size and increases the speed of deep learning (DL) inference for deployment on CPUs or GPUs. It provides unified interfaces across multiple DL frameworks for popular network compression technologies like quantization, pruning, knowledge distillation, and neural architecture search (NAS). In this blog, we introduce a super-network-based NAS approach called dynamic neural architecture search (DyNAS) that is >4x more sample efficient than typical one-shot, predictor-based NAS approaches.", "doc_id": 149}
{"doc": "NAS\nNAS has seen rapid growth in the machine learning research community. It automates the discovery of optimal deep neural network architectures in domains like computer vision and natural language processing. While there have been many recent advancements, there is still a significant focus on making the search more efficient to reduce the computational cost incurred when validating discovered architectures.", "doc_id": 150}
{"doc": "Super-Networks\nThe computational overhead of evaluating deep neural network architectures during the search process can be costly due to the training and validation cycles. Novel weight-sharing approaches known as one-shot or super-networks offer a way to mitigate the training overhead. These approaches train a task-specific super-network architecture with a weight-sharing mechanism that allows the sub-networks to be treated as unique individual architectures. This enables sub-network model extraction and validation without a separate training cycle (Figure 1).", "doc_id": 151}
{"doc": "DyNAS Methodology\nEvolutionary algorithms, specifically genetic algorithms, have a history of usage in NAS and continue to gain popularity as an efficient way to explore the architecture objective space. We will show how evolutionary algorithms can be paired with lightly trained objective predictors in an iterative cycle to accelerate multi-objective architectural exploration. Specifically, we use a bi-level optimization approach denoted as DyNAS (Figure 2).", "doc_id": 152}
{"doc": "In the first phase of the search, a small population of sub-networks is randomly sampled from the super-network and evaluated (validation measurement) to provide the initial training set for the inner predictor loop. After the predictors are trained, a multi-objective evolutionary search is performed in the predictor objective space. After this extensive search is performed, the best performing sub-network configurations are selected to be the next iteration\u2019s validation population. The cycle continues until the user-defined evaluation count is met.", "doc_id": 153}
{"doc": "NAS API in Intel Neural Compressor\nThere are two ways to utilize the NAS API. One way is to use a YAML configuration file, e.g.:\n```\nagent = NAS('config.yaml')\nresults = agent.search()\n```\nThe config.yaml file will look like the following, where the approach section specifies which NAS approach to use:\n```\nnas:\n  approach: dynas\n  search: \n    search_algorithm: 'nsga2'\n  dynas:\n    supernet: 'ofa_resnet50\u2019\nmetrics: ['acc', 'macs']\n\u2026\n```", "doc_id": 154}
{"doc": "The other way is to use the NASConfig class, which is demonstrated in the code snippet below, which demonstrates how to perform a multi-objective NAS on a MobileNetV3 one-shot weight-sharing super-network for the image classification task on ImageNet-ilsvrc2012:\n```\nfrom neural_compressor.conf.config import NASConfig\nfrom neural_compressor.experimental.nas import NAS\nconfig = NASConfig(approach='dynas', search_algorithm='nsga2')\nconfig.dynas.supernet = 'ofa_mbv3_d234_e346_k357_w1.2'\nconfig.dynas.metrics = ['acc', 'macs']\nconfig.dynas.population = 50\nconfig.dynas.num_evals = 250\nconfig.dynas.results_csv_path = 'search_results.csv'\nconfig.dynas.batch_size = 64\nconfig.dynas.dataset_path = '/datasets/imagenet-ilsvrc2012' #example\nagent = NAS(config)\nresults = agent.search()\n```", "doc_id": 155}
{"doc": "It uses the Intel Neural Compressor DyNAS search approach. In this example, we use NSGA-II as the search algorithm, accuracy (ImageNet Top-1 Accuracy (%)) and macs (Multiply-and-accumulates as measured from FVCore) as the search metrics.\nResults from the search process will be pairs of model architectures and metrics of such architectures (Figure 3). For analysis, pareto fronts of the search result can be drawn with the code below. For more details about this example, please refer to MobileNetV3 supernet NAS notebook example.\n```\n### Plot Search Results in the Multi-Objective Space\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.cm import ScalarMappable\nfig, ax = plt.subplots(figsize=(7,5))\nnumber_of_evals = 500\ndf_dynas = pd.read_csv('results_mbnv3w1p2_macs.csv')[:number_of_evals]\ndf_dynas.columns = ['config', 'date', 'lat', 'macs', 'top1']\ncm = plt.cm.get_cmap('viridis_r')\ncount = [x for x in range(len(df_dynas))]\nax.scatter(df_dynas['macs'].values, df_dynas['top1'].values, marker='^', alpha=0.8, c=count, \n           cmap=cm, label='Discovered DNN Model', s=10)\nax.set_title(f'Intel\u00ae Neural Compressor\\nDynamic NAS (DyNAS)\\nSupernet:{config.dynas.supernet}')\nax.set_xlabel('MACs', fontsize=13)\nax.set_ylabel('Top-1 Accuracy (%)', fontsize=13)\nax.legend(fancybox=True, fontsize=10, framealpha=1, borderpad=0.2, loc='lower right')\nax.grid(True, alpha=0.3)\n# Eval Count bar\nnorm = plt.Normalize(0, len(df_dynas))\nsm = ScalarMappable(norm=norm, cmap=cm)\ncbar = fig.colorbar(sm, ax=ax, shrink=0.85)\ncbar.ax.set_title(\"         Evaluation\\n  Count\", fontsize=8)\nfig.tight_layout(pad=2)\nplt.show();\n```", "doc_id": 156}
{"doc": "Future Work\nWe plan to add more NAS approaches in Intel Neural Compressor. We invite users to try this DyNAS example and send us feedback through GitHub issues.\nWe encourage you to check out Intel\u2019s other AI Tools and Framework optimizations and learn about the unified, open, standards-based oneAPI programming model that forms the foundation of Intel\u2019s AI Software Portfolio.", "doc_id": 157}
{"doc": "In this blog, we illustrate one of its new features to help you do easy quantization on broader models.\nPyTorch provides a FX toolkit for developers to transform a torch.nn.Module into a torch.fx.GraphModule. With the generated GraphModule, FX can execute static quantization by automatically inserting quantize and dequantize operations.\nIt\u2019s useful to convert an imperative model into a graph model because the latter gives better performance with multiple optimization options such as post-training static quantization. However, FX cannot handle dynamic control flows automatically, and there are many cases that will block the model transformation because of dynamic control flows.", "doc_id": 158}
{"doc": "Our Solution\nOur solution, namely fine-grained FX, helps models with dynamic control flows on ease-of-use quantization. It is integrated into the pytorch_fx backend of Intel Neural Compressor and supports three popular quantization methods: post-training dynamic and static quantization, and quantization-aware training.\nPyTorch recommends post-training dynamic quantization for NLP models because its real-time variable scales and zero-points shows stable accuracy after quantization. Post-training static quantization performs quantization based on fixed scales and zero-points. It supports continuous quantization modules, avoiding redundant quantization and dequantization operations. Theoretically, static quantization has a better performance than dynamic quantization. Quantization-aware training for static quantization requires an additional training process to adjust model weights to reduce quantization loss. It can provide high accuracy based on the best performance of static quantization.\nBecause the imperative model consists of several blocks, fine-grained FX will aggressively and recursively detect these blocks for module transformation. Two examples are shown below: natural language processing (Figure 1) and object detection (Figure 2). The dark green blocks are detected as suitable for module transformation because they are the largest blocks without any control flow. We leverage the FX toolkit on these blocks and do quantization automatically. By reassembling these processed blocks using the original control flows, the resulting model maintains the same behavior and provides higher performance by leveraging INT8.", "doc_id": 159}
{"doc": "Adopting Our Solution\nIntel Neural Compressor Examples\nWe provide two kinds of examples for natural language processing models based on Hugging Face Transformers. You can easily replace the input model with your own and quantize it based on fine-grained FX:\nPost-training static quantization for text classification\nQuantization-aware training for text classification", "doc_id": 160}
{"doc": "Hugging Face Optimum-Intel Examples\nOptimum-Intel is an extension of Transformers that enable the use of popular compression techniques such as quantization and pruning via Intel Neural Compressor. All tasks in Optimum-Intel support fine-grained FX: language modeling, multiple choice, question answering, summarization, text classification, token classification, and translation. We also uploaded several INT8 models into the Hugging Face model hub that can be easily initialized and leveraged with Intel Neural Compressor, e.g.:\n```\nfrom neural_compressor.utils.load_huggingface import OptimizedModel\nint8_model = OptimizedModel.from_pretrained(\n    'Intel/distilbert-base-uncased-finetuned-sst-2-english-int8-static',\n)\n```\n\nFuture Work\nThe vision of fine-grained FX is to improve the productivity of PyTorch quantization, especially of the static quantization approach. We are continuously uploading INT8 models to the Hugging Face model hub for quick deployment. We invite users to try Intel Neural Compressor and Hugging Face Optimum-Intel and share your models on the model hub. We also encourage you to check out Intel\u2019s other AI Tools and Framework optimizations and learn about the unified, open, standards-based oneAPI programming model that forms the foundation of Intel\u2019s AI Software Portfolio.", "doc_id": 161}
{"doc": "For PyTorch, it is not always trivial to apply DL optimizations such as INT8 quantization correctly in the Python code. Not only do users have to insert the corresponding API code correctly in their code, they also need to identify the correct variable name of the calibration dataloader and of the model on which the quantization is to be performed. Users might also need to construct the evaluation function for tuning. To address this issue, Intel Neural Compressor v1.13 provides an experimental auto-quantization feature that allows users to enable quantization without coding. The feature leverages DL optimization rules and static program analysis (Python code syntax analysis, static type inference, call graph parsing) that can automatically insert the necessary API code into user scripts.", "doc_id": 162}
{"doc": "How to One-Click Enable My Model\nFeature Highlight\nWe\u2019re delighted to share Neural Coder, a toolkit that was recently published in Intel Neural Compressor. Neural Coder, as a code-free solution, automatically enables quantization algorithms in a PyTorch model script and evaluates for the best model performance. Supported features include post-training static quantization, post-training dynamic quantization, and mixed precision. See this user guide for more information.\nThe following example code shows how to enable quantization algorithms and performance evaluation on a pretrained ResNet50 model for ImageNet using Neural Coder:\n```\nfrom neural_coder import auto_quant\nauto_quant(code=\"https://github.com/pytorch/examples/blob/main/imagenet/main.py\", args=\"-a resnet50 --pretrained -e /path/to/imagenet/\")\n```", "doc_id": 163}
{"doc": "Python API for PyTorch Programmers\nNeural Coder can be used as a standalone Python library. It offers one-click acceleration of DL scripts via automatic platform conversions and optimization code insertions. It subsequently benchmarks applicable optimization sets acquired from the automated enabling to determine best performance.\n\nThis feature leverages static program analysis and heuristic DL optimization rules to simplify the use of DL optimization APIs, which improves developer productivity and facilitates DL acceleration. See this document for more information.", "doc_id": 164}
{"doc": "Intel Neural Compressor Bench GUI\nWe have also integrated Neural Coder into the Intel Neural Compressor Bench GUI for easy access. First, create a new project and upload a PyTorch script (Figure 1).\nNext, choose an optimization approach (Figure 2).\nPost-training dynamic and static quantization (with the FX backend) are supported in the current Intel Neural Compressor release, with new features under development (Figures 3 and 4). Currently, we construct the evaluate function as a dummy version, so while the performance boost can be demonstrated through the optimization, tuning is bypassed at this stage. However, we will soon support the construction of \u201creal\u201d evaluation functions for most popular model zoos.", "doc_id": 165}
{"doc": "Static quantization quantifies model weights and activations. It allows the user to fuse activations into previous layers when possible. Unlike dynamic quantization, where scales and zeros are collected during inference, static quantization scales and zeros are determined prior to inference using calibration data sets. Hence, static quantization is theoretically faster than dynamic quantization. As a result, static quantization models are more conducive to inference than dynamic quantization models.", "doc_id": 166}
{"doc": "Dynamic quantization weighs the weights of the neural network as integers, but the activation is dynamically quantized during inference (Figure 5). Compared to floating-point neural networks, the size of the dynamic quantization model is much smaller due to the weights being stored as low-bit wide integers. Compared to other quantization techniques, dynamic quantization does not require any data for calibration or fine-tuning.", "doc_id": 167}
{"doc": "Users can benchmark the optimizations against the original input model to compare before and after performance of specific optimizations (Figure 6). The GUI design is kept the same as running benchmarks on TensorFlow/ONNX models, so users can enjoy a smooth experience if they have ever used this functionality for TensorFlow/ONNX models.", "doc_id": 168}
{"doc": "Future Work\nWe welcome any feedback on Neural Coder. Other features in Neural Coder are planned and are going to be integrated in the next Intel Neural Compressor release, e.g.: stock PyTorch INT8 enabling similar to Intel Neural Compressor INT8, support for random broad models, and more. We also encourage you to check out Intel\u2019s other AI Tools and Framework optimizations and learn about the unified, open, standards-based oneAPI programming model that forms the foundation of Intel\u2019s AI Software Portfolio.", "doc_id": 169}
{"doc": "Overview\nAI inference can often be a slow, memory-crushing process due to the need for precision coupled with model computational complexity.\nThis session looks at a way to solve these issues using quantization: the process of converting data in FP32 to a smaller precision (like int8) while maintaining accuracy and performance and saving memory bandwidth.\nAI software engineers Neo Zhang and Severine Habert introduce the tools and techniques to quantize your AI models easily and quickly, including:\n1. An overview of Intel\u00ae Neural Compressor and Intel\u00ae Deep Learning Boost\n2. A demonstration showcasing an end-to-end pipeline to train a TensorFlow* model with a small Keras* dataset, followed by speeding it up using quantization\n3. Performance comparisons of FP32 and int8 models by the same script\n\nGet the Software\nThe Intel Neural Compressor is available as part of the Intel\u00ae AI Analytics Toolkit\u2014eight tools and frameworks to accelerate end-to-end data science and analytics pipelines.", "doc_id": 170}
{"doc": "Intel\u00ae Neural Compressor is an open-source python library for model compression that reduces the model size and increases the speed of deep learning inference for deployment on CPUs or GPUs. It provides unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help the user quickly find the best-quantized model. It also implements different weight pruning algorithms to generate pruned models using a predefined sparsity goal, and supports knowledge distillation to distill the knowledge from the teacher model to the student model. Intel\u00ae Neural Compressor provides APIs for a range of deep learning frameworks including TensorFlow, PyTorch, and MXNet in addition to ONNX runtime for greater interoperability across frameworks. This blog is focused on the benefits of using the tool with a PyTorch model.", "doc_id": 171}
{"doc": "Quantization\nIntel\u00ae Neural Compressor extends PyTorch quantization by providing advanced recipes for quantization and automatic mixed precision, and accuracy-aware tuning. It takes a PyTorch model as input and yields an optimal model accordingly.", "doc_id": 172}
{"doc": "Fine-grained Quantization\nIntel\u00ae Neural Compressor\u2019s quantization capability is built upon the standard PyTorch quantization API and makes its own modifications to support fine-grained quantization granularity from the model level to the operator level. This approach gives users the ability to get better accuracy without additional hand-tuned work.", "doc_id": 173}
{"doc": "Advanced Automatic Mixed-Precision\nIntel\u00ae Neural Compressor further extends the scope of the PyTorch Automatic Mixed Precision (AMP) feature on 3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors. Compared with vanilla PyTorch AMP implementation, the tool also supports INT8 in addition to BF16 and FP32. As shown in the diagram below, it first converts all the quantizable operators from FP32 to INT8, and then converts the remaining FP32 operators to BF16 operators if BF16 kernels are supported on PyTorch and accelerated by underlying HW.", "doc_id": 174}
{"doc": "Accuracy-aware Tuning\nIntel\u00ae Neural Compressor also supports an automatic accuracy-aware tuning mechanism for better quantization productivity. As the first step, the tool queries the framework for the quantization capabilities. , such as quantization granularity (per_tensor or per_channel), quantization scheme (symmetric or asymmetric), quantization data type (u8 or s8), and calibration approach (min-max or KL divergence). Then it queries the supported data types for each operator. With these queried capabilities, the tool generates a whole tuning space of different sets of quantization configurations and starts the tuning iterations. For each set of quantization configurations, it performs the calibration, quantization, and evaluation. Once the evaluation meets the accuracy goal, the tool terminates the tuning process and produces a quantized model.", "doc_id": 175}
{"doc": "Pruning\nThe pruning part of Intel\u00ae Neural Compressor is mainly focused on unstructured and structured weight pruning, and filter pruning. Unstructured pruning uses a magnitude pruning algorithm, which prunes the weights during training when their magnitude is below a predefined threshold. Structured pruning implements experimental tile-wise sparsity kernels to boost the performance of the sparsity model. And filter pruning implements a gradient-sensitivity pruning algorithm, which prunes the head, intermediate layers, and hidden states in the NLP model according to the importance score calculated by the gradient.", "doc_id": 176}
{"doc": "Distillation\nIntel\u00ae Neural Compressor also implements a knowledge distillation algorithm to transfer knowledge from a large \u201cteacher\u201d model to a smaller \u201cstudent\u201d model without loss of validity. As indicated below, the same input is fed to both models, and the student model learns by comparing its results to both the teacher and the ground-truth label.", "doc_id": 177}
{"doc": "Example\nBelow is one example to show how to quantize an NLP model with Intel\u00ae Neural Compressor.\n```\n# config.yaml\nmodel:\n  name: distilbert\n  framework: pytorch_fx\ntuning:\n  accuracy_criterion:\n    relative: 0.01\n\n# main.py\nimport torch\nimport numpy as np\nfrom transformers import (\n    AutoModelForSequenceClassification,\n    AutoTokenizer\n)\n\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n)\n\n# Calibration dataloader\nclass CalibDataLoader(object):\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.sequence = \"Shanghai is a beautiful city!\"\n        self.encoded_input = self.tokenizer(\n            self.sequence,\n            return_tensors='pt'\n        )\n        self.label = 1 # negative sentence: 0; positive sentence: 1\n        self.batch_size = 1\n\n    def __iter__(self):\n        yield self.encoded_input, self.label\n\n# Evaluation function\ndef eval_func(model):\n    output = model(**calib_dataloader.encoded_input)\n    print(\"Output: \", output.logits.detach().numpy())\n    emotion_type = np.argmax(output.logits.detach().numpy())\n    return 1 if emotion_type == calib_dataloader.label else 0\n\n# Enable quantization\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization('./config.yaml')\nquantizer.model = model\nquantizer.calib_dataloader = CalibDataLoader()\nquantizer.eval_func = eval_func\nq_model = quantizer.fit()\n```\nNote that the generated mixed-precision model may vary, depending on the capabilities of the low precision kernels and underlying hardware (e.g., INT8/BF16/FP32 mixed-precision model on 3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors).", "doc_id": 178}
{"doc": "Performance Results\nIntel\u00ae Neural Compressor has validated 400+ examples with a performance speedup geomean of 2.2x on an Intel\u00ae Xeon\u00ae Platinum 8380 Processor with minimal accuracy loss. More details for validated models are available here.\n*Test configuration: Test by Intel as of 6/10/2022 processor: 2S Intel(R) Xeon(R) Platinum 8380 CPU @ 2.30GHz, 40-core/80-thread, Turbo Boost on, Hyper-Threading on; memory: 256GB (16x16GB DDR4 3200MT/s); storage: Intel\u00ae SSD *1; NIC: 2x Ethernet Controller 10G X550T; BIOS: SE5C6200.86B.0022.D64.2105220049(ucode:0xd0002b1)\uff1bOS: Ubuntu 20.04.1 LTS; Kernel: 5.4.0\u201342-generic; Batch Size: 1; Core per Instance: 4;", "doc_id": 179}
{"doc": "Summary and Future Work\nThe vision of Intel\u00ae Neural Compressor is to improve productivity and solve the issues of accuracy loss by an auto-tuning mechanism and an easy-to-use API when applying popular neural network compression approaches. We are continuously improving this tool by adding more compression recipes and combining those techniques to produce optimal models. We invite users to try Intel\u00ae Neural Compressor and send us feedback through Github issues. We also welcome any contributions to Intel\u00ae Neural Compressor Github repo.", "doc_id": 180}
{"doc": "The mission of Hugging Face is to democratize good machine learning and maximize its positive impact across industries and society. Not only do we strive to advance Transformer models, but we also work hard on simplifying their adoption.\nToday, we're excited to announce that Intel has officially joined our Hardware Partner Program. Thanks to the Optimum open-source library, Intel and Hugging Face will collaborate to build state-of-the-art hardware acceleration to train, fine-tune and predict with Transformers.\nTransformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.", "doc_id": 181}
{"doc": "Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.\n\u201c*We\u2019re excited to work with Hugging Face to bring the latest innovations of Intel Xeon hardware and Intel AI software to the Transformers community, through open source integration and integrated developer experiences.*\u201d, says Wei Li, Intel Vice President & General Manager, AI and Analytics.\nIn recent months, Intel and Hugging Face collaborated on scaling Transformer workloads. We published detailed tuning guides and benchmarks on inference (part 1, part 2) and achieved single-digit millisecond latency for DistilBERT on the latest Intel Xeon Ice Lake CPUs. On the training side, we added support for Habana Gaudi accelerators, which deliver up to 40% better price-performance than GPUs.\nThe next logical step was to expand on this work and share it with the ML community. Enter the Optimum Intel open source library! Let\u2019s take a deeper look at it.", "doc_id": 182}
{"doc": "Get Peak Transformers Performance with Optimum Intel\nOptimum is an open-source library created by Hugging Face to simplify Transformer acceleration across a growing range of training and inference devices. Thanks to built-in optimization techniques, you can start accelerating your workloads in minutes, using ready-made scripts, or applying minimal changes to your existing code. Beginners can use Optimum out of the box with excellent results. Experts can keep tweaking for maximum performance.\nOptimum Intel is part of Optimum and builds on top of the Intel Neural Compressor (INC). INC is an open-source library that delivers unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, and knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help users quickly build the best quantized model.\nWith Optimum Intel, you can apply state-of-the-art optimization techniques to your Transformers with minimal effort. Let\u2019s look at a complete example.", "doc_id": 183}
{"doc": "Case study: Quantizing DistilBERT with Optimum Intel\nIn this example, we will run post-training quantization on a DistilBERT model fine-tuned for classification. Quantization is a process that shrinks memory and compute requirements by reducing the bit width of model parameters. For example, you can often replace 32-bit floating-point parameters with 8-bit integers at the expense of a small drop in prediction accuracy.\nWe have already fine-tuned the original model to classify product reviews for shoes according to their star rating (from 1 to 5 stars). You can view this model and its quantized version on the Hugging Face hub. You can also test the original model in this Space.\nLet\u2019s get started! All code is available in this notebook.\nAs usual, the first step is to install all required libraries. It\u2019s worth mentioning that we have to work with a CPU-only version of PyTorch for the quantization process to work correctly.\n```\npip -q uninstall torch -y \npip -q install torch==1.11.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu\npip -q install transformers datasets optimum[neural-compressor] evaluate --upgrade\n```", "doc_id": 184}
{"doc": "Then, we prepare an evaluation dataset to assess model performance during quantization. Starting from the dataset we used to fine-tune the original model, we only keep a few thousand reviews and their labels and save them to local storage.\nNext, we load the original model, its tokenizer, and the evaluation dataset from the Hugging Face hub.\n```\nfrom datasets import load_dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nmodel_name = \"juliensimon/distilbert-amazon-shoe-reviews\"\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\neval_dataset = load_dataset(\"prashantgrao/amazon-shoe-reviews\", split=\"test\").select(range(300))\n```", "doc_id": 185}
{"doc": "Next, we define an evaluation function that computes model metrics on the evaluation dataset. This allows the Optimum Intel library to compare these metrics before and after quantization. For this purpose, the Hugging Face evaluate library is very convenient!\n```\nimport evaluate\ndef eval_func(model):\n    task_evaluator = evaluate.evaluator(\"text-classification\")\n    results = task_evaluator.compute(\n        model_or_pipeline=model,\n        tokenizer=tokenizer,\n        data=eval_dataset,\n        metric=evaluate.load(\"accuracy\"),\n        label_column=\"labels\",\n        label_mapping=model.config.label2id,\n    )\n    return results[\"accuracy\"]\n```", "doc_id": 186}
{"doc": "We then set up the quantization job using a [configuration]. You can find details on this configuration on the Neural Compressor documentation. Here, we go for post-training dynamic quantization with an acceptable accuracy drop of 5%. If accuracy drops more than the allowed 5%, different part of the model will then be quantized until it an acceptable drop in accuracy or if the maximum number of trials, here set to 10, is reached.\n```\nfrom neural_compressor.config import AccuracyCriterion, PostTrainingQuantConfig, TuningCriterion\n\ntuning_criterion = TuningCriterion(max_trials=10)\naccuracy_criterion = AccuracyCriterion(tolerable_loss=0.05)\n# Load the quantization configuration detailing the quantization we wish to apply\nquantization_config = PostTrainingQuantConfig(\n    approach=\"dynamic\",\n    accuracy_criterion=accuracy_criterion,\n    tuning_criterion=tuning_criterion,\n)\n```\nWe can now launch the quantization job and save the resulting model and its configuration file to local storage.\n```\nfrom neural_compressor.config import PostTrainingQuantConfig\nfrom optimum.intel.neural_compressor import INCQuantizer\n\n# The directory where the quantized model will be saved\nsave_dir = \"./model_inc\"\nquantizer = INCQuantizer.from_pretrained(model=model, eval_fn=eval_func)\nquantizer.quantize(quantization_config=quantization_config, save_directory=save_dir)\n```", "doc_id": 187}
{"doc": "You can find the resulting model hosted on the Hugging Face hub. To load a quantized model hosted locally or on the \ud83e\udd17 hub, you can do as follows :\n```\nfrom optimum.intel.neural_compressor import INCModelForSequenceClassification\ninc_model = INCModelForSequenceClassification.from_pretrained(save_dir)\n```", "doc_id": 188}
{"doc": "We\u2019re only getting started\nIn this example, we showed you how to easily quantize models post-training with Optimum Intel, and that\u2019s just the beginning. The library supports other types of quantization as well as pruning, a technique that zeroes or removes model parameters that have little or no impact on the predicted outcome.\nWe are excited to partner with Intel to bring Hugging Face users peak efficiency on the latest Intel Xeon CPUs and Intel AI libraries. Please give Optimum Intel a star to get updates, and stay tuned for many upcoming features!", "doc_id": 189}
{"doc": "Deploy More Efficient Deep Learning Models\nIntel\u00ae Neural Compressor performs model compression to reduce the model size and increase the speed of deep learning inference for deployment on CPUs or GPUs. This open source Python* library automates popular model compression technologies, such as quantization, pruning, and knowledge distillation across multiple deep learning frameworks.\nUsing this library, you can:\n1. Converge quickly on quantized models though automatic accuracy-driven tuning strategies.\n2. Prune the least important parameters for large models.\n3. Distill knowledge from a larger model to improve the accuracy of a smaller model for deployment.\n4. Get started with model compression with one-click analysis and code insertion.\nIntel Neural Compressor is part of the end-to-end suite of Intel\u00ae AI and machine learning development tools and resources.", "doc_id": 190}
{"doc": "ONNX (Open Neural Network Exchange)\nONNX is an open format to represent both deep learning and traditional models. ONNX is developed and supported by a community of partners such as Microsoft, Facebook, and AWS. At a high level, ONNX is designed to express machine learning models while offering interoperability across different frameworks. ONNXRuntime is the runtime library that can be used to maximize performance of Intel hardware for ONNX inference.", "doc_id": 191}
{"doc": "Quantization\nQuantization is the replacement of floating-point arithmetic computations (FP32) with integer arithmetic (INT8). Using lower-precision data reduces memory bandwidth and accelerates performance.\n8-bit computations (INT8) offer better performance compared to higher-precision computations (FP32) because they enable loading more data into a single processor instruction. Using lower-precision data requires less data movement, which reduces memory bandwidth.", "doc_id": 192}
{"doc": "Intel\u00ae Deep Learning Boost (Intel\u00ae DL Boost)\nIntel\u00ae Deep Learning Boost (Intel\u00ae DL Boost) is a hardware acceleration feature available in second-generation Intel\u00ae Xeon\u00ae scalable processors to increase performance of deep learning workloads. Intel DL Boost Vector Neural Network Instructions (VNNI) delivers 3X performance improvement by combining three instructions into one for deep learning computations, thereby reducing memory bandwidth and maximizing compute efficiency and cache utilization.\nQuantization can introduce accuracy loss because fewer bits limit the precision and range of values. However, researchers have extensively demonstrated that weights and activations can be represented using 8-bit integers (INT8) without incurring significant loss in accuracy. Techniques such as post training quantization (PTQ) and quantization aware training (QAT) can recover loss in accuracy due to quantization. These techniques are available in an Intel supported open-source tool \u201cIntel\u00ae Neural Compressor.\u201d", "doc_id": 193}
{"doc": "Intel\u00ae Neural Compressor\nIntel\u00ae Neural Compressor (formerly known as Intel\u00ae Low Precision Optimization Tool) is an open-source Python tool, which delivers unified interface to support multiple deep learning frameworks. It can be used to apply key model optimization techniques, such as quantization, pruning, knowledge distillation to compress models. This tool makes it easy to implement accuracy-driven tuning strategies to help user create highly optimized AI models. It has support for multiple weight pruning algorithms, which generate pruned models with predefined sparsity goals. This tool can also be used to apply knowledge distillation to distill the knowledge from the teacher model to a student model.\nAs shown in Figure 2, Intel\u00ae Neural Compressor is built on the top of frameworks and relies on framework interfaces to execute model training/inference/quantization/evaluation.", "doc_id": 194}
{"doc": "Quantizing ONNX Models Using Intel\u00ae Neural Compressor\nIn this tutorial, we will show step-by-step how to quantize ONNX models with Intel\u00ae Neural Compressor.\nIntel\u00ae Neural Compressor takes FP32 model and YAML configuration file as two inputs. To construct the quantization process, users can either specify below settings via YAML or\nPython APIs:\n1. Calibration Dataloader (Needed for static quantization)\n2. Evaluation Dataloader\n3. Evaluation Metric", "doc_id": 195}
{"doc": "Conclusion\nBy leveraging Intel\u00ae Neural Compressor, we achieved less than 1% accuracy loss and gained significant speedup in INT8 model performance compared to the FP32 model. We continue expanding the quantized model scope and contribute to ONNX model zoo.\nPlease send your pull requests for review if you have improvements to Intel\u00ae Neural Compressor. If you have any suggestions or questions, please contact inc.maintainers@intel.com.", "doc_id": 196}
{"doc": "Introduction\nIntel\u00ae oneAPI AI Analytics Toolkit (AI Kit) gives data scientists, AI developers, and researchers familiar Python* tools and frameworks to accelerate end-to-end data science and analytics pipelines on Intel\u00ae architectures.\nThis article covers how to use quantization method to accelerate AI inference by Intel\u00ae AI Analytics Toolkit container on Alibaba Cloud. In same time, it shows the inference of quantized AI model to be accelerated more by Intel\u00ae Deep Learning Boost in Intel\u00ae Xeon\u00ae 2nd Generation Scalable Processor and later.\nThere is an end-to-end pipeline demo to show how to deploy Intel\u00ae AI Analytics Toolkit container on Alibaba Cloud; how to train and quantize an AI model by Intel\u00ae Optimization for Tensorflow* in Intel\u00ae AI Analytics Toolkit; the final acceleration result based on Intel\u00ae Deep Learning Boost in Alibaba Cloud.\nThere are involved 3 software components and 1 hardware technology to accelerate the AI inference:", "doc_id": 197}
{"doc": "Getting Started with Alibaba Cloud\nThis article assumes you are familiar with Alibaba Cloud environment. To learn more about working with Alibaba Cloud, please refer to Elastic Compute Service (ECS).\nSpecifically, this article assumes:\n1. You have an Alibaba Cloud account.\n2. You are familiar with creating instances within the Alibaba Cloud environment.\n3. To learn more about launching an instance see Create and manage an ECS instance by using the ECS console (express version).\n4. Developers are familiar with Python, AI model training and inference based on Tensorflow*.", "doc_id": 198}
{"doc": "Setting Up an Ubuntu* Instance\nCreating an Instance\n1. Log in to your Alibaba Cloud account.\n2. Navigate to the Elastic Compute Service dashboard.\n3. Click the Create ECS Instance button.\n\nBasic Configurations\nStep 1 \u2013 Choose Billing Method: Select the billing type to launch as instance. Suggested: Pay-As-You-Go.\nNote When you want to stop billing, please release the instance totally.\nStep 2 \u2013 Choose an Instance Type: Use the ecs.c6e.2xlarge instant type: 8 vCPUs, 16 GiB, Intel Xeon (Cascade Lake) Platinum 8269CY.\nNote For using Intel\u00ae Deep Learning Boost, please choose Intel\u00ae Cascade Lake or later Xeon CPU.\nStep 3 \u2013 Choose Image: Choose Ubuntu 20.04 64-bit.\nStep 4 \u2013 Choose Storage: Set the instance storage to a minimum of 40GB.\nClick Next\nNetworking\nStep 1 \u2013 Choose Bandwidth Billing: Recommend to Pay-By-Traffic.\nStep 2 \u2013 Choose Peak Bandwidth: Recommend maxing value 100M if Bandwidth Billing is Pay-By-Traffic.\nStep 3 \u2013 Choose Security Group: Enable Port 80, Port 22.\nNote Port 80 is used to access Jupyter Notebook in demo, and Port 22 is used to access the instance by SSH.\nClick Next\nSystem Configuration\nStep 1 \u2013 Set Logon Credentials: Choose Password.\nStep 2 \u2013 Set Login Password: Set a password.\nStep 3 \u2013 Set Confirm Password: Set the same password.\nStep 4 \u2013 Set Instance Name: Set to the name as you want.\nClick Preview\nPreview\nStep 1 \u2013 Set Automatic Release: It\u2019s recommended to set the automatic release time, to make sure the instance won\u2019t take more money after work.\nClick Create Instance\nWhen the instance is created, go to the Instance Dashboard, and check the status. This process may take a few minutes before the instance is running.", "doc_id": 199}
{"doc": "Connecting to the Alibaba Cloud Instance\nThis instruction below assume you are working from within a Linux* or Mac OS* environment. For more information or if you are working from a Windows* environment see Connect to a Linux instance by using a password.\nPrerequisites\nInstall an SSH client.\nYour Linux computer most likely includes an SSH client by default. You can check for an SSH client by typing ssh at the command line. If your computer doesn't recognize the command, the OpenSSH project provides a free implementation of the full suite of SSH tools. For more information, see the OpenSSH web page.\nConnecting using SSH\nIn Instance Dashboard, get the public IP address of the instance status.\nIn a command-line shell, use the ssh command to connect to the instance. The username is root. The password is the value to be set during create instance. `ssh root@xxx.xxx.xxx.xxx`", "doc_id": 200}
{"doc": "Check Intel\u00ae Deep Learning Boost\nAfter login the instance, use lscpu command to check the CPU info in Ubuntu. The flag: avx512_vnni means the CPU support Intel\u00ae Deep Learning Boost.\nDeploy Intel\u00ae AI Analytics Toolkit Container\nIntel\u00ae AI Analytics Toolkit supports to deploy by installation (online, offline) and container. The container is chosen in this article.\n\nInstall Docker in Ubuntu*\nInstall docker and other tools in Ubuntu*.\n`$apt update && apt install docker.io vim git`", "doc_id": 201}
{"doc": "Check docker service status\nMake sure docker service is active before going to next steps.\nStartup Container and Jupyter notebook\nIn host (Alibaba Cloud Instance), startup container with share folder and port mapping:\n\nMount the host folder \u201c/root/share\u201d to container folder \u201c/host\u201d.\nMap host port 80 to the container network port 8888.\nmkdir -p share\ndocker run -v /root/share:/host -p 80:8888 -it \"intel/oneapi-aikit:latest\"", "doc_id": 202}
{"doc": "Download Demo Code\nUse the opensource oneAPI samples demo code provided by Intel\u00ae.\nIn container, run following commands to download the demo code.\n```\napt update && apt install git vim\ncd /host\ngit clone https://github.com/oneapi-src/oneAPI-samples.git\n```", "doc_id": 203}
{"doc": "Prepare Jupyter Notebook Running Time\nIn container, run following commands to create Conda* virtual environment and install Intel\u00ae Optimization for Tensorflow*, Intel\u00ae Neural Compressor and other depended libraries from oneAPI local channel.\n```\nconda init bash\nsource ~/.bashrc\nconda env remove -n user_tensorflow\nconda create --name user_tensorflow -y\nconda activate user_tensorflow\nconda install -c ${ONEAPI_ROOT}/conda_channel tensorflow python-flatbuffers -y\nconda install -c ${ONEAPI_ROOT}/conda_channel neural-compressor -y\nconda install runipy notebook -y\n```", "doc_id": 204}
{"doc": "Run Demo in Jupyter Notebook\nStartup Jupyter notebook by running script run_jupyter.sh\n1. Open the url http://public_ip by Chrome. The public_ip is the IP address of the Instance.\nThe web page needs the password or token. Input the token value in previous step output, like:\n29fa5d5464a7be9c1de4f21afb35f400ddb5affde14709e3\n2. Click Jupyter file: inc_sample_tensorflow.jpynb to open it and startup Jupyter kernel.\nNote If there is \u201cNot Trusted\u201d flag in kernel status, please click \u201cNot Trusted\u201d. Then, in pop up window, click \u201cTrusted\u201d\n3. In menu, click Cell -> Run All. Run the demo in Jupyter notebook. It will take 10-30 minutes to run all the steps. It depended on the number of vCPU in the instance.", "doc_id": 205}
{"doc": "Demo\nThe demo includes 4 stages: Training, Quantization, Test Performance, Compare Performance. Please refer to the description in the Jupyter notebook file for detailed info.\nTraining\nThe demo uses AlexNet model to classify handwriting number defined in MNIST dataset. It will take a little more time to train the model. After it\u2019s finished, the model will be frozen and saved as PB format.\nNote The default number of epochs is 3. Increase epochs to get higher accuracy..", "doc_id": 206}
{"doc": "Quantization\nThe trained model will be quantized by Intel\u00ae Neural Compressor. This tool will apply different parameters & methods to quantize the model and find the best result. Finally, it will output the first INT8 model which match the requirement (better performance and less accuracy lost). The result will be saved as PB file: alexnet_int8_model.pb.", "doc_id": 207}
{"doc": "Test Performance\nUse same test script and setting to test the performance of FP32 and INT8 models. The test results include throughput, latency & accuracy and are saved to JSON format files.\nCompare Performance\nThere are figures to compare the performance: throughput, latency& accuracy.", "doc_id": 208}
{"doc": "Summary\nAccording to the result, it\u2019s easy to learn the INT8 model has obviously performance increase than FP32 model in throughput and latency. In same time, the accuracy lost is limited to small range as we expected.\nIt\u2019s the result to bind the Intel software (Intel\u00ae oneAPI AI Analytics Toolkit) and hardware (Intel\u00ae Deep Learning Boost) technology.\nIntel\u00ae oneAPI AI Analytics Toolkit includes more Intel\u00ae optimized framework, library, and tools: PyTorch*, Modin*, scikit-learn*, and XGBoost. They are optimized for Intel\u00ae Architecture and have good performance in related instance of Alibaba Cloud.", "doc_id": 209}
{"doc": "We are pleased to share that Intel Neural Compressor (INC) now has easy to use integration with SigOpt.\nIntel Neural Compressor (formerly known as Intel Low Precision Optimization Tool) is an open-source Python library running on Intel CPUs and GPUs, which delivers unified interfaces across multiple deep learning frameworks for popular network compression technologies, such as quantization, pruning, knowledge distillation. This tool supports automatic accuracy-driven tuning strategies to help users quickly find out the best quantized model. It also implements different weight pruning algorithms to generate pruned model with predefined sparsity goal and supports knowledge distillation to distill the knowledge from the teacher model to the student model.\nSigOpt is an Intelligent Experimentation platform which can be used to accelerate the performance for model development. In this case, SigOpt increases the performance gains for INC Quantization compression.", "doc_id": 210}
{"doc": "Preparation\nBefore using SigOpt strategy, signup or login to your SigOpt account.\n1. Each account has its own API token. Find your API token and then set the configuration item: sigopt_api_token.\n2. Create a new project and then set the project name into the configuration item: sigopt_project_id.\n3. Set the name for this experiment in configuration item sigopt_experiment_id. The default is nc-tune.", "doc_id": 211}
{"doc": "SigOpt Optimization Setup\nAfter logging in, you can use the API token to connect the local code and the online platform, corresponding to the configuration item sigopt_api_token, which can be obtained here.\nIn addition to the Optimization Loop, SigOpt has two important concepts: project and experiment. Create a project before experimenting, corresponding to sigopt_project_id and sigopt_experiment_name. Multiple experiments can be created in each project. After creating an experiment, run through these three simple steps in a loop:\n1. Receive a suggestion from SigOpt\n2. Evaluate your metric\n3. Report an observation to SigOpt\nWith INC\u2019s SigOpt strategy, the metrics add accuracy as a constraint and optimize for latency.", "doc_id": 212}
{"doc": "Neural Compressor Configuration\nThe following INC configuration will help get you started quickly. Note that the sigopt_api_token is necessary to use the SigOpt strategy, whereas the Basic strategy does not need the API token. Also, be sure to create the corresponding project name sigopt_project_id in your SigOpt account before using the SigOpt strategy.\n```\ntuning:\n  strategy:\n    name: sigopt\n    sigopt_api_token: YOUR-ACCOUNT-API-TOKEN\n    sigopt_project_id: PROJECT-ID\n    sigopt_experiment_name: nc-tune\n  accuracy_criterion:\n    relative: 0.01\n  exit_policy:\n    timeout: 0\n  random_seed: 9527\n```", "doc_id": 213}
{"doc": "Performance Benefits\nThe metric constraints from SigOpt help you easily self-define metrics and search for desirable outcomes. Also, the results of each experiment are recorded in your account. So, you can use the SigOpt data analysis function to analyze the results, such as drawing a chart, calculating F1 score, and more.\nWith INC\u2019s integrated SigOpt strategy, you can achieve faster compression while maintaining accuracy. The following results show how SigOpt increased the quantization speed for MobileNet_v1 and ResNet50_v1 with TensorFlow. While the basic strategy results in slower compression time.", "doc_id": 214}
{"doc": "Overview\nMost deep learning applications are using 32-bits of floating-point precision for inference. But low precision data types, especially int8, are getting more focus due to significant performance boost. One of the essential concerns on adopting low precision is how to easily mitigate the possible accuracy loss and reach predefined accuracy requirement.\n\nIntel\u00ae Neural Compressor aims to address the aforementioned concern by extending PyTorch with accuracy-driven automatic tuning strategies to help user quickly find out the best quantized model on Intel hardware, including Intel Deep Learning Boost (Intel DL Boost) and Intel Advanced Matrix Extensions (Intel AMX).\n\nIntel\u00ae Neural Compressor has been released as an open-source project at Github.\n\nFeatures\n1. Ease-of-use Python API: Intel\u00ae Neural Compressor provides simple frontend Python APIs and utilities for users to do neural network compression with few line code changes. Typically, only 5 to 6 clauses are required to be added to the original code.\n2. Quantization: Intel\u00ae Neural Compressor supports accuracy-driven automatic tuning process on post-training static quantization, post-training dynamic quantization, and quantization-aware training on PyTorch fx graph mode and eager model.\nThis tutorial mainly focuses on the quantization part. As for how to use Intel\u00ae Neural Compressor to do pruning and distillation, please refer to corresponding documents in the Intel\u00ae Neural Compressor github repo.", "doc_id": 215}
{"doc": "Intel\u00ae Neural Compressor (formerly known as Intel\u00ae Low Precision Optimization Tool) is an open-source Python library running on Intel CPUs and GPUs that provides popular neural network compression technologies, such as quantization, pruning, and knowledge distillation. It provides a single, unified interface to multiple deep learning frameworks, including TensorFlow, MXNet, PyTorch, and ONNX Runtime.\nIntel\u00ae Neural Compressor supports automated, accuracy-driven tuning strategies to help data scientists quickly find the best quantized model for their particular data model. It also implements a variety of weight pruning algorithms to generate pruned models with specific sparsity goals and supports knowledge distillation to move knowledge from teacher models to the student models.\nImplementing these techniques manually can be tedious and complex, requiring detailed knowledge of the underlying framework as well as how best to construct and tune the compression techniques. Neural Compressor automates much of the tedium, providing a quick and easy way to obtain optimal results in the framework and workflow you prefer.", "doc_id": 216}
{"doc": "Why Use Intel\u00ae Neural Compressor?\nDeep learning training and inference is resource intensive. It requires a lot of data, and high-precision data consumes much more space than lower-precision data.\nThis places a lot of pressure on memory sizing, and fast memory is expensive, but the greater challenge is the time it takes to compute on large, high-precision models. Even with infinite money, you can\u2019t buy more time, and you can only buy the fastest memory and CPUs that actually exist. Learning and inferencing is often iterative, particularly during model development, so the time taken accumulates with each train and test cycle. And while the computation is running, there is little for data scientists to do but wait.\nBut we can use resources more efficiently, particularly if total precision isn\u2019t actually required. There are multiple benefits if you can produce good results with a smaller, lower-precision dataset. A smaller dataset fits in less memory, lowering costs. It\u2019s faster to move the data across the memory bus, speeding up cycle times. And the model is faster to compute on because of how it fits into memory.\nIf you\u2019re renting compute instances by the hour in the public cloud, these savings can add up substantially. But even if you\u2019re running models on static infrastructure, faster throughput means faster results, and more opportunities to try more things. Or simply getting the answer you need quicker, so you can move on to what you really want to do instead of sitting around waiting for a model to finish training. These benefits are qualitatively better, not merely economic gains; it\u2019s just a lot more enjoyable to work on problems without having to wait around for slow infrastructure all the time.\nGetting good answers quickly is what we\u2019re trying to do, after all.", "doc_id": 217}
{"doc": "Quantization With Neural Compressor\nThe 32 bits of precision of a float32 datatype requires four times as much space as 8-bit precision of the int8 datatype. You can convert a high-precision float32 number to an int8 number using a process called quantization which takes samples of the original, smooth number to give you a rough approximation.\nThis is essentially how MP3s work to give you acceptable quality audio compared to the full fidelity of a live performance. It\u2019s good enough for listening to while you\u2019re going for a run, and much more convenient than carrying a band and all their equipment on your back.\nCERN, the European Organization for Nuclear Research, has used Neural Compressor to improve the performance of a 3D Generative Adversarial Network (GAN) used for measuring the energy of particles produced in simulated electromagnetic calorimeters. Quantization with Neural Compressor provided around 1.8x faster processing with 8 streams and 56 cores.\nAlibaba, meanwhile, achieved approximately 3x performance improvement by quantizing to int8 with Neural Compressor for its PAI Natural Language Processing (NLP) Transformer model which uses the PyTorch framework. The quantization resulted in only 0.4% accuracy loss, which was deemed acceptable for the workload goals.", "doc_id": 218}
{"doc": "Pruning With Intel\u00ae Neural Compressor\nPruning carefully removes non-critical information from a model to make it smaller. This is a different approach from the quantization method, but the goal is the same: get good enough results from a smaller model so that you can get those good results faster and with fewer resources.\nIntel\u00ae Neural Compressor supports a variety of pruning techniques including basic magnitude, gradient sensitivity, and pattern lock.\nPruning is traditionally quite complex, requiring manually tuning many iterations and a lot of expertise. Neural Compressor automates a selection of techniques and can combine them with quantization techniques\nPruning provides many of the same benefits as quantization, and because it is a different technique, the two approaches can be combined. Neural Compressor supports pruning followed by post-training quantization as well as pruning during quantization-aware training methods, providing extra efficiency.", "doc_id": 219}
{"doc": "Using the Power of Open Source Software\nIntel\u00ae Neural Compressor is open source, available at https://github.com/intel/neural-compressor so you can easily add the library to your toolbox. And if you\u2019re just getting into machine learning and learning how neural networks work, you can dig into the code to see how quantization and pruning actually works. Or just see working examples of what NumPy flatten() and reshape() are used for.\nOnce you have a model (an example mobilenet model) and some evaluation parameters (see an example here), using the library requires just a few lines of Python, as you can see below:\nLearn more about how to use Neural Compressor in your projects with the tutorials and detailed documentation included with the code.", "doc_id": 220}
{"doc": "Conclusion\nWhile the realm of deep learning and neural networks can be extremely complex, the benefits of Intel\u00ae Neural Compressor are based on the same principles we\u2019re already very familiar with in other parts of the technology stack. Lossy compression is an everyday staple in the music we listen to, the JPEG photographs we take with our cameras, and the streaming movies we watch. Using the same techniques to speed up deep learning processing is a natural extension of what we already do in many other places.\nIf we consider the reverse: taking twice as long to get a tiny improvement in precision that no customer will ever notice, not using these techniques seems wasteful in comparison.\nWhich is what makes Intel\u00ae Neural Compressor so compelling: why wouldn\u2019t you want use it?\nAnd beyond the pure performance and resource efficiency gains, there is a wealth of opportunity here for those new to deep learning who want to explore how these techniques work, both in theory and in practice. As a freely available piece of software that you can inspect and learn from, there\u2019s little reason not to at least give Neural Compressor a try.\nThe power of modern frameworks and the ease-of-use provided by modern tools like Neural Compressor puts these advanced techniques within reach of a broad range of software developers and data scientists. When combined with on-demand cloud computing infrastructure, it\u2019s easy to try it out for yourself to see if the claims are true. You can prove the results for yourself without a major investment in time and infrastructure just to test out vendor claims.\nAnd if it performs as well as other customers have seen, customers like CERN, Alibaba, and Tencent, why wouldn\u2019t you use it too?", "doc_id": 221}
{"doc": "Deep Learning (DL) and Artificial Intelligence (AI) are quickly becoming ubiquitous. Naveen Rao, Intel's Artificial Intelligence Products Group's GM, recently stated that \"there is a vast explosion of [AI] applications,\" and Andrew Ng calls AI \u201cthe new electricity\u201d. Deep learning applications already exist in the cloud, home, car, our mobile devices, and various embedded IoT (Internet of Things) devices. These applications employ Deep Neural Networks (DNNs), which are notoriously time, compute, energy, and memory intensive. Applications need to strike a balance between accuracy, speed, and power consumption\u2013taking into consideration the HW and SW constraints and the product eco-system.", "doc_id": 222}
{"doc": "At Intel\u00ae AI Lab, we have recently open-sourced Neural Network Distiller, a Python* package for neural network compression research. Distiller provides a PyTorch* environment for prototyping and analyzing compression algorithms, such as sparsity-inducing methods and low-precision arithmetic. We think that DNN compression can be another catalyst that will help bring Deep Learning innovation to more industries and application domains, to make our lives easier, healthier, and more productive.", "doc_id": 223}
{"doc": "The Case for Compression\nUser-facing Deep Learning applications place a premium on the user-experience, and interactive applications are especially sensitive to the application response time. Google\u2019s internal research found that even small service response delays have a significant impact on user engagement with applications and services. With more applications and services being powered by DL and AI, the importance of low-latency inference grows more important, and this is true for both cloud-based services and mobile applications.", "doc_id": 224}
{"doc": "Cloud applications processing batches of inputs, are more concerned about Total Cost of Ownership (TCO) than response latency. For example, a mortgage lending firm that uses Machine Learning (ML) and DL to evaluate the risk of mortgage applications, processes vast quantities of data. High throughput is valued more than low latency and keeping the energy bill under control, a main component of TCO, is a concern.", "doc_id": 225}
{"doc": "IoT edge devices usually collect vast amounts of sensor data to be analyzed by DL-powered applications. Directly transferring the data to the cloud for processing is often prohibitive due to the available network bandwidth or battery power. Therefore, employing DL at the edge, for example to preprocess sensor data to reduce its size, or to offer reduced functionality when the connection to the cloud is not available, can be very beneficial. DL, however, often requires compute, memory, and storage resources that are either not available to include or are expensive to add to IoT devices.", "doc_id": 226}
{"doc": "So, we see that whether we are deploying a deep learning application on a resource-constrained device or in the cloud, we can benefit from reducing the computation load, memory requirements, and energy consumption.\nOne approach to this is to design small network architectures from the get-go. SqueezeNet and MobileNets are examples of two architectures that explicitly targeted network size and speed. Another research effort by our colleagues at the Intel AI Lab shows a principled method for unsupervised structure learning of the deeper layers of DNNs, resulting in compact structures and equivalent performance to the original large structure.\nA different approach to achieving reduced network sizes is to start with a predetermined, well-performing DNN architecture and to transform it through some algorithmic steps into a smaller DNN. This is referred to as DNN compression.", "doc_id": 227}
{"doc": "Compression is a process in which a DNN\u2019s requirements for (at least one of) compute, storage, power, memory, and time resources are reduced while keeping the accuracy of its inference function within an acceptable range for a specific application. Often, these resource requirements are inter-related and reducing the requirement for one resource will also reduce another.\nIt is interesting to note that recent research has demonstrated empirically, that even small-by-design networks such as those mentioned above can be further compressed.\nIt shows that large-and-sparse models perform significantly better than their small-and-dense equivalents (the pairs of compared models have the same architecture and memory footprint).", "doc_id": 228}
{"doc": "Introducing Distiller\nWe\u2019ve built Distiller with the following features and tools, keeping both DL researchers and engineers in mind:\n1. A framework for integrating pruning, regularization, and quantization algorithms\n2. A set of tools for analyzing and evaluating compression performance\n3. Example implementations of state-of-the-art compression algorithms", "doc_id": 229}
{"doc": "Pruning and regularization are two methods that can be used to induce sparsity in a DNN\u2019s parameters tensors. Sparsity is a measure of how many elements in a tensor are exact zeros, relative to the tensor size. Sparse tensors can be stored more compactly in memory and can reduce the amount of compute and energy budgets required to carry out DNN operations. Quantization is a method to reduce the precision of the data type used in a DNN, leading again to reduced memory, energy and compute requirements. Distiller provides a growing set of state-of-the-art methods and algorithms for quantization, pruning (structured and fine-grained) and sparsity-inducing regularization\u2013leading the way to faster, smaller, and more energy-efficient models.", "doc_id": 230}
{"doc": "To help you concentrate on your research, we\u2019ve tried to provide the generic functionality, both high and low-level, that we think most people will need for compression research. Some examples:\n1. Certain compression methods dynamically remove filters and channels from Convolutional layers while a DNN is trained. Distiller will perform the changes in the configuration of the targeted layers, and in their parameter tensors as well. In addition, it will analyze the data-dependencies in the model and modify dependent layers as needed\n2. Distiller will automatically transform a model for quantization, by replacing specific layer types with their quantized counterparts. This saves you the hassle of manually converting each floating-point model you are using to its lower-precision form, and allows you to focus on developing the quantization method, and to scale and test your algorithm across many models", "doc_id": 231}
{"doc": "We\u2019ve included Jupyter notebooks that demonstrate how to access statistics from the network model and compression process. For example, if you are planning to remove filters from your DNN, you might want to run a filter-pruning sensitivity analysis and view the results in a Jupyter notebook:\nDistiller statistics are exported as Pandas DataFrames which are amenable to data-selection (indexing, slicing, etc.) and visualization.\nDistiller comes with sample applications that employ some methods for compressing image-classification DNNs and language models. We\u2019ve implemented a few compression research papers that can be used as a template for starting your own work. These are based on a couple of PyTorch\u2019s example projects and show the simplicity of adding compression to pre-existing training applications.", "doc_id": 232}
{"doc": "Only the Beginning\nDistiller is a research library for the community at large and is part of Intel AI Lab\u2019s effort to help scientists and engineers train and deploy DL solutions, publish research, and reproduce the latest innovative algorithms from the AI community. We are currently working on adding more algorithms, more features, and more application domains.\nIf you are actively researching or implementing DNN compression, we hope that you will find Distiller useful and fork it to implement your own research; we also encourage you to send us pull-requests of your work. You will be able to share your ideas, implementations, and bug fixes with other like-minded engineers and researchers\u2014a benefit to all! We take research reproducibility and transparency seriously, and we think that Distiller can be the virtual hub where researchers from across the globe share their implementations.\nFor more information about Distiller, you can refer to the documentation and code.\nGeek On.", "doc_id": 233}
{"doc": "Intel and Google* have been collaborating to deliver optimized machine learning implementations of compute-intensive TensorFlow* operations. For example, convolution filters that require large matrix multiplications.\nIn this session, Penporn Koanantakook of Google delivers an overview of the Intel and Google collaboration, which includes the Intel\u00ae Extension for TensorFlow* and other key AI developer tools\u2014Intel\u00ae oneAPI Deep Neural Network Library (oneDNN) and Intel\u00ae Neural Compressor.\nThis session covers:\n1. Optimizations that have been implemented, such as operation fusion, primitive caching, and vectorization of int8 and bfloat16 data types.\n2. A live demonstration of the Intel Neural Compressor automatically quantizing a network to improve performance by 4x with a 0.06% accuracy loss.\n3. An overview of the PluggableDevice mechanism in TensorFlow, co-architected by Intel and Google to deliver a scalable way for developers to add new device support as plug-in packages.\nNote This presentation was current as of TensorFlow v2.8. Starting with TensorFlow v2.9, the oneDNN optimizations are on by default, and no longer require the TF_ENABLE_ONEDNN_OPTS=1 variable setting.", "doc_id": 234}
{"doc": "ntroduction\nTo get good performance on your pre-trained model for inference, some inference optimizations are required.\nThis article will guide you how to optimize a pre-trained model for better inference performance, and also analyze the model pb files before and after the inference optimizations.\nThose optimizations include:\n\n1. Converting variables to constants\n2. Removing training-only operations like checkpoint saving\n3. Stripping out parts of the graph that are never reached\n4. Removing debug operations like CheckNumerics\n5. Folding batch normalization ops into the pre-calculated weights\n6. Fusing common operations into unified versions", "doc_id": 235}
{"doc": "Prerequisites\nThis article uses a published tutorial in the format of a Jupyter Notebook.\nReaders might need to get the samples listed below with the related oneAPI AI Analytics toolkit installation.\nInference Optimization Tutorial\nFor hands-on exercises, please follow the README for that published tutorial.\nThis article also guides readers to go through the results from the previous run.", "doc_id": 236}
{"doc": "Introduce the Benchmark\nThe ssd_inception_v2 pre-trained model from download.tensorflow.org is used in this article, and we leverage an inference benchmark script from the LPOT project to measure the performance of the pre-trained model.", "doc_id": 237}
{"doc": "Run the Benchmark with the Original Pre-trained Model\nBy running the command below with the downloaded benchmark script and the ssd_inception_v2 pre-trained model, users should be able to get performance throughput from this original pre-trained model.\n```\n$python tf_savemodel_benchmark.py --model_path pre-trained-models/ssd_inception_v2_coco_2018_01_28/saved_model --num_iter 200 --num_warmup 10 --disable_optimize\n```\nHere are output examples of performance throughput.\n```\n[{tf_savemodel_benchmark.py:144} INFO - Throughput: 15.671910\nThroughput: 15.67 fps\n```\nNote : We disable the optimization features from the benchmark script to get the actual performance number from the original model.", "doc_id": 238}
{"doc": "Optimize the Original Pre-trained Model\nUsers have different options to optimized their pretrained model, and those options includes The Intel\u00ae Low Precision Optimization Tool (Intel\u00ae LPOT),  and tools from TensorFlow github.  \nThe optimization from those options includes:\n1. Converting variables to constants\n2. Removing training-only operations like checkpoint saving\n3. Stripping out parts of the graph that are never reached\n4. Removing debug operations like CheckNumerics\n5. Folding batch normalization ops into the pre-calculated weights\n6. Fusing common operations into unified versions\nWe recomend to use LPOT for pre-trained model optimization on Intel Architectures.", "doc_id": 239}
{"doc": "Intel\u00ae Neural Compressor\nThe Intel\u00ae Neural Compressor (INC) is an open-source Python library that delivers a unified low-precision inference interface across multiple Intel-optimized Deep Learning (DL) frameworks on both CPUs and GPUs.\nINC also provides graph optimizations for fp32 pre-trained models with more optimizations (such as common subexpression elimination) than the TensorFlow optimization tool optimize_for_inference.\nHere are related to INC codes to convert a unfrozen saved model pb file into a optimized model pb file.\n```\nfrom neural_compressor.experimental import Graph_Optimization\ngraph_opt = Graph_Optimization()\ngraph_opt.model = 'pre-trained-models/ssd_inception_v2_coco_2018_01_28/saved_model'   # the path to saved_model dir\noutput = graph_opt() output.save('pre-trained-models/ssd_inception_v2_coco_2018_01_28/optimized_model')\n```\nThe unfrozen save model pb file is under \"pre-trained-models/ssd_inception_v2_coco_2018_01_28/saved_model\", and the optimized save model pb file is under \"pre-trained-models/ssd_inception_v2_coco_2018_01_28/optimized_model\". \nUsers could refer to fp32 optimization for more details.\nFp32 optimization feature from LPOT is required, so users must use LPOT v1.4 or greater.", "doc_id": 240}
{"doc": "Tools from TensorFlow github\nTensorFlow github provides tools for freezing and optimizing a pre-trained model. Freezing the graph can provide additional performance benefits. The freeze_graph tool, available as part of TensorFlow on GitHub, converts all the variable ops to const ops on the inference graph and outputs a frozen graph. With all weights frozen in the resulting inference graph, you can expect improved inference time. After the graph has been frozen, additional transformations by using optimize_for_inference tool can help optimize the graph for inference.\nWe also provide a wrapper python script for those tools from TensorFlow github, and the wrapper python script \"freeze_optimize_v2.py\" provides the same funcitionality as freeze_graph.py and optimize_for_inference.py from TensorFlow github.\nA freeze_optimize_v2.py wrapper python script for inference optimization is used in this article, and Intel is working on upstreaming this script to the TensorFlow GitHub. The input of this script is the directory of the original saved model, and the output of this script is the directory of the optimized model. By running the command below, users can get the optimized model pb file under the output_saved_model_dir.\n```\n$python freeze_optimize_v2.py --input_saved_model_dir=pre-trained-models/ssd_inception_v2_coco_2018_01_28/saved_model --output_saved_model_dir=pre-trained-models/ssd_inception_v2_coco_2018_01_28/optimized_model\n```\nDue to a limitation of convert_variables_to_constants_v2 function in TensorFlow, freeze_optimize_v2.py doesn't support graphs with embedding or control flow related ops.", "doc_id": 241}
{"doc": "Run the Benchmark with the Optimized Pre-trained Model\nThen users need to re-run the benchmark with the optimized model pb file using the following command, and users should be able to see the improved performance throughput with the optimized model.\n```\n$python tf_savemodel_benchmark.py --model_path pre-trained-models/ssd_inception_v2_coco_2018_01_28/optimized_model --num_iter 200 --num_warmup 10 --disable_optimize\n```\nThe diagram below is the output result from our previous run on Intel\u00ae Xeon SKX machines.\nThere is a 1.66X speedup from the optimized model in this run as shown in the diagram below.\nThe only difference is the model optimization, and users could achieve this 1.6X speedup in the same inference workload.\nUsers should be able to see the speedup number on their environment by running the published tutorial mentioned above.", "doc_id": 242}
{"doc": "Analyze the Original and Optimized Pre-trained Model PB Files\nTo further show readers the difference between the original and optimized model PB files, we use a PB file parser to analyze those pb files.\n- Original pre-trained PB file\nThe PB file parser will get the call count number of different TensorFlow operations in the PB file.\n\nHere is the table of original pre-trained pb file, The top 5 call counts hotspots are Const, Identity, Relu6, FusedBatchNorm and Conv2D.", "doc_id": 243}
{"doc": "A \"Double Play\" for MLPerf\u2122 Inference Performance Gains with 3rd Generation Intel\u00ae Xeon\u00ae Scalable Processors\nKey Takeaways\n1. Intel demonstrates, as the only data center CPU vendor to submit MLPerf inference results on a broad set of models, that it is practical to run deep learning inference anywhere on the massive existing base of Intel Xeon servers alongside other applications. Software optimizations are crucial to delivering the full value of Intel\u2019s hardware advances. In this blog, some of Intel\u2019s software engineers describe the methods they used in optimizing the MLPerf inference 1.1 submissions.\n2. Intel\u2019s recent MLPerf inference v1.1 datacenter submissions affirm Intel\u2019s continued momentum in CPU performance for machine learning. The latest 3rd Gen Intel\u00ae Xeon\u00ae Scalable processors (codenamed \u201cIce Lake\u201d) delivers up to a 2.3X Server performance improvement compared to the last round of MLPerf submissions on the BERT [f:MLPerf v0.7 Inference Datacenter Closed ResNet, entry 0.7-101. https://mlcommons.org/en/inference-datacenter-07/] [f:MLPerf v1.1 Inference Datacenter Closed ResNet, entry 1.1-023. https://mlcommons.org/en/inference-datacenter-11/]. Intel\u2019s MLPerf 1.1 results also demonstrate up to 1.5X increase in Offline and 1.65X increase in Server performance compared to 2nd Generation Intel\u00ae Xeon\u00ae Scalable Processors (codenamed \u201cCascade Lake\u201d) on ResNet50-v1.5 in MLPerf Inference v0.7 [f:MLPerf v0.7 Inference Datacenter Closed ResNet, entry 0.7-101. https://mlcommons.org/en/inference-datacenter-07/] [f:MLPerf v1.1 Inference Datacenter Closed ResNet, entry 1.1-023. https://mlcommons.org/en/inference-datacenter-11/].\n3. The 3rd Gen Intel\u00ae Xeon\u00ae Scalable processors (codenamed \u201cCooper Lake\u201d) delivers up to 3.0X Server performance improvement compared to the last round of MLPerf submissions on the RNN-T model[f:MLPerf v1.0 Inference Datacenter Closed BERT, RNN-T, entry 1.0-20, 1.0-52 https://mlcommons.org/en/inference-datacenter-10/] [f:MLPerf v1.1 Inference Datacenter Closed BERT, RNN-T, entry 1.1-024, 1.1-026 https://mlcommons.org/en/inference-datacenter-11/].", "doc_id": 244}
{"doc": "3D Digital Face Reconstruction Solution enabled by 3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors\nKey Takeaways\n1. Intel is helping Tencent to optimize and improve its PRN network-based 3D digital face reconstruction solution and promote its adoption in various game products.\n2. Technical practices Tencent and Intel carried out show that low-precision quantization is an effective way to accelerate inference and improve AI application performance.", "doc_id": 245}
{"doc": "the rapid development of deep learning has drawn more attention to face-focused applications in the computer vision and pattern recognition domain. As a result, 3D digital face reconstruction technologies that refine and bring 2D images to life are increasingly found in online games, education, and e-commerce areas.\nTencent Games is working with Intel to build a new 3D digital face reconstruction solution. Initial adopters are using this solution in game character generation and character model shifting, bringing gamers a smoother gaming experience. To deliver the performance and capabilities for this solution, Tencent Games is using the new 3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors (formerly codenamed Ice Lake) built-in AI acceleration through Intel\u00ae Deep Learning Boost (Intel\u00ae DL Boost) as well as, the Intel\u00ae Neural Compressor (formerly known as Intel\u00ae Low Precision Optimization Tool), part of Intel\u00ae oneAPI AI Analytics Toolkit. Together, these technologies significantly improve inference efficiency while ensuring accuracy and precision. By quantizing the Position Map Regression Network (PRN) from FP32-based inference down to INT8, Tencent Games is able to improve inference efficiency and provide a practical solution for 3D digital face reconstruction.\nThis blog introduces the Tencent Games teams exploration of PRN network-based 3D digital face reconstruction solution. We will also discuss various performance enhancements made possible with the new 3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors and Intel-optimized software.", "doc_id": 246}
{"doc": "PRN Network-based 3D Digital Face Reconstruction Solution\nAs Artificial Intelligence (AI) technologies continue to evolve and use cases expand, 3D digital face reconstruction is gaining more attention. Unlike 2D facial images, 3D digital face reconstruction requires not only color and texture information but also key deep information to create curved surfaces. Currently, AI scientists are leveraging a series of deep learning methods, such as the 3D Morphable Model (3DMM) and Volumetric Representation Network (VRN) for the 3D reconstruction of 2D face images.\nHowever, the requirement for predefined basic 3D digital face model or template and complicated regression network is causing high computing complexity and poor real-time processing. The central idea of the PRN network is to record all 3D coordinates of face Point Clouds through a UV Position Map - preserving semantic information of each UV polygon. The second step is to design an encoder-decoder convolutional neural network (CNN) for projection from the 2D face images to their corresponding positions in the UV polygon. The PRN network does not rely on the predefined basic 3D face model or template. Rather it canreconstruct the whole face with only the semantic information of the 2D image; As a light-weight extension of the CNN network, it performs better than other deep learning networks and has drawn significant industry attention.", "doc_id": 247}
{"doc": "Tencent Games is building a 3D face reconstruction solution based on the PRN network to incorporate AI capabilities into its product lines. When developing games or other products with 3D character modeling requirements, UI engineers can leverage the 3D face reconstruction technologies to convert 2D figures into draft 3D models by batches. They can adjust the details, saving modeling time and improving development efficiency. In some games, players can upload their face photo and leverage the \u201cintelligent face-making\u201d feature to create a more customized 3D profile, with their own facial characteristics. This capability, shown in Figure 2, significantly improves the user\u2019s gaming experience.", "doc_id": 248}
{"doc": "Tencent Games Boosts 3D Digital Face Reconstruction via Low-Precision Inference\nWhile the PRN network enables high-performance 3D digital face reconstruction, Tencent Games hopes to introduce more optimization solutions that will further improve implementation efficiency and the user experience.\nAs an extended model of the CNN network, a typical PRN network uses an encoder-decoder architecture, with face images as input and the projected position map as the output. The encoder structure consists of 10 cascading residual blocks and the decoder structure 17 deconvolution layers. The activation layer is powered by the ReLU function, and the output layer the Sigmoid function. Large-scale convolution and deconvolution computing consume massive computing power during training and inference.\nIn the past, deep learning training and inference were performed with the FP32 data set, which is highly demanding on processor performance, memory, and even power consumption. Recent studies have shown that using low-precision datasets such as INT8 , can improve the efficiency of processors and memories without significantly impacting deep learning scene results, such as facial image processing.2 Low-precision data sets can also help conserve computing power and improve implementation efficiency for frequently used deep learning computing tasks such as convolution and deconvolution computing.\nFigure 3 shows models trained with FP32 dataset can be converted to INT8 through quantization and continue with subsequent inference tasks. Such a quantization solution can maintain accuracy while training high-precision datasets and accelerate the task with INT8 dataset during the implementation of higher-frequency inference workloads These capabilities significantly improve the overall performance of the solution.", "doc_id": 249}
{"doc": "3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors and Quantization Tool for Overall Solution Optimization\n\nFor better working efficiency, Tencent Games has adopted the new 3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors as the infrastructure of the solution. The platform provides more cores, further optimized architecture, higher memory capacity, and enhanced security technologies to support the user with stronger computing power and Intel\u00ae DL Boost for AI acceleration.\n\nAs a unique AI accelerator developed by Intel, Intel\u00ae DL Boost is based on Vector Neural Network Instruction (VNNI). It provides a variety of deep learning-oriented features to help users run complex AI workloads on Intel\u00ae architecture-based platforms. For example, Intel\u00ae DL Boost can integrate multiple instructions into one to support optimization of multiplication computing under an 8- or 16-bit low-precision dataset. This capability is essential for solutions running massive matrix multiplication computing, such as the PRN network. Practices have shown that developers can gain the following advantages by adopting the VNNI instruction set provided by Intel\u00ae DL Boost:\n\n1. Improved computing speed: Some computing tasks can gain a theoretical peak 4x improvement in computing speed with the new instruction set under INT8 dataset compared with FP323;\n2. Reduced memory access workloads: The new instruction set helps reduce model weight size and bandwidth threshold for memory access during INT8 inference;\n3. Improved inference efficiency: By integrating with processor hardware features, Intel\u00ae DL Boost technology helps improve the efficiency of INT8 multiplications. At the same time, Intel\u00ae Optimizations for TensorFlow and Apache MXNet also support INT8 to boost inference efficiency.", "doc_id": 250}
{"doc": "Tencent Games implements the INT8 quantization of the solution with Intel\u00ae Neural Compressor and Intel\u00ae Optimization for TensorFlow, both provided as part of Intel\u00ae oneAPI AI Analytics Toolkit as a simple, easy to use interoperable package. The toolkit includes optimized python frameworks and libraries built using oneAPI to maximize performance of end-to-end data science and machine learning workflows on Intel architectures.\nIntel\u00ae Neural Compressor, an open-source Python library, can maintain a similar level of accuracy and precision after quantization while accelerating the deployment of low-precision inference solutions on popular deep learning architectures. It achieves these capabilities through a series of built-in strategies for auto-adjustments. Figure 4 shows the basic structure of Intel\u00ae Neural Compressor.", "doc_id": 251}
{"doc": "Intel\u00ae Neural Compressor facilitates adaptation to popular deep learning architectures via the framework adaptation layer, which reduces the tasks of Tencent Games for architecture adaptation and optimization. Intel\u00ae Neural Compressor facilitates auto-tuning of quantization strategies of the PRN network model given actual demands during the 3D digital face reconstruction process with its core Auto-tuner. Such tuning strategies may come from the built-in extensible tuning strategies or be set by the user regarding performance, model size, and encapsulation method. Intel\u00ae Neural Compressor can also perform architecture optimization via built-in APIs, such as KL divergence calibration for TensorFlow and MXNet4.", "doc_id": 252}
{"doc": "Validation Results\nTencent Games and Intel carried out multi-dimensional validation tests on the 3D digital face reconstruction solution to validate the performance improvement provided by 3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors and Intel\u00ae Neural Compressor based INT8 quantization. The first test compared the performance of the 2nd Gen Intel\u00ae Xeon\u00ae Scalable Processors (Cascade Lake) without INT8 quantization. As Figure 5 shows, the PRN network model deployed on 3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors (Test Config) saw a 53 percent improvement in inference speed5.", "doc_id": 253}
{"doc": "This result is validated more explicitly with game developers. According to Tencent Games internal data, 3D digital face reconstruction required tens of milliseconds before and only 10-20 milliseconds after migrating to the new platform7. These speedups resulted in significant savings in modeling time and while improving development efficiency for game development with massive character modeling tasks.\nThe benchmarking team also conducted validation tests to see whether the solution can maintain a high level of precision while ensuring efficiency improvement after INT8 quantization. The results show that INT8 quantization will not have a significant impact on accuracy and precision, whether on the 3rd Gen Intel\u00ae Xeon\u00ae Scalable processors or the last generation platform. Take accuracy, for example \u2013 the accuracy loss caused by quantization was only 0.00018.", "doc_id": 254}
{"doc": "3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors are based on Intel\u2019s 10nm+ process technology.  They offer more CPU cores, higher memory capacity, and frequency than previous generation.  Technologists from Alibaba Group and Intel worked together to explore what these capabilities mean for AI applications, particularly when used with Intel\u00ae Deep Learning Boost (Intel\u00ae DL Boost). We also explored the Intel\u00ae Neural Compressor (formerly known as Intel\u00ae Low Precision Optimization Tool), which helps customers rapidly develop and deploy their AI INT8 models on Intel\u00ae Xeon Scalable processor-based platforms.  We optimized the Alibaba Transformer model on 3rd Gen Intel Xeon Scalable processors and demonstrated 1.36x and 1.42x performance improvement in FP32 and INT8 inference over Intel\u2019s previous generation processors.", "doc_id": 255}
{"doc": "Technology Overview\nTransformer is a key model used in Alibaba\u2019s end-to-end Machine Learning Platform for AI (PAI). It is widely used in real-world natural language processing (NLP) tasks, serving millions of users through Alibaba\u2019s online service. Low latency and high throughput are keys to Transformer\u2019s success, and 8-bit low precision is a promising technique to meet such requirements.\n\nIntel\u00ae DL Boost offers powerful capabilities for 8-bit low precision inference on AI workloads. With the support of Intel\u00ae Neural Compressor, we can optimize 8-bit inference performance while significantly reducing accuracy loss. These capabilities demonstrate leadership in AI inference and shows the power of Intel\u00ae DL Boost and 3rd Gen Intel Xeon Scalable Processor.", "doc_id": 256}
{"doc": "Model Profiling\nAlibaba\u2019s Transformer model is a PyTorch model. We adopted profiling methodology to analyze the model performance. From the FP32 model profiling log below, we see that model is computation intensive. We see further that 70 percent of total time occupied by computation-intensive ops, such as conv and matmul. This information tells us that the AVX512_VNNI should bring a significant performance speed on the Transformer model. Meanwhile, increased memory bandwidth and frequency of the 3rd Gen Intel Xeon Scalable processor should also deliver benefits for memory-intensive operations.", "doc_id": 257}
{"doc": "The 3rd Gen Intel\u00ae Xeon\u00ae Scalable Processors family increases CPU core number, frequency and memory bandwidth compared to the 2nd Gen Intel\u00ae Xeon\u00ae Scalable Processors family. These changes brought a 1.42x performance improvement on the PyTorch Transformer INT8 model and a 1.36x performance improvement on the PyTorch Transformer FP32 model. Alibaba partner with Intel\u2019s latest CPU and INT8 quantization tool could bring up to 3.1x performance boost on Alibaba PAI blade inference toolkit. Alibaba Cloud expects the processors will help speed up Transformer tasks and provide more efficient services to Alibaba\u2019s million+ customers.", "doc_id": 258}
{"doc": "Investigations, conducted together with scientists at CERN, show promising results \u2013 with breakthrough performance \u2013 in their pursuit of faster Monte Carlo based simulations, which are an important part of many scientific, engineering, and financial applications. In order to help address the future needs of CERN\u2019s LHC (Large Hadron Collider), which is the world\u2019s largest particle accelerator), researchers at CERN, SURFsara, and Intel have been investigating approaches for supplying extraordinary new levels of simulation.\nEarly results from a promising approach that relied on trained Generative Adversarial Networks (GANs) were first revealed at the International Supercomputing Conference in 2018, where it was awarded best poster in the category \u201cprogramming models and systems software.\u201d\nNow, they have demonstrated success in accelerating inferencing nearly two-fold by using reduced precision without compromising accuracy at all. Their technique highlights a general approach to inferencing acceleration \u2013 that is supported by Intel Xeon SP processors today, and by GPUs coming to market. A paper outlining this work will be presented this week at the 10th International Conference on Pattern Recognition Applications and Methods.", "doc_id": 259}
{"doc": "Simulations, critical to data analytics, consume vast amounts of computing for Monte Carlo computations. Dr. Sofia Vallecorsa, a CERN physicist specializing in AI and Quantum research, has sought to ease the vast amount of compute currently consumed for Monte Carlo simulations. She observes, for instance, that more than half of the computing in the Worldwide LHC Computing Grid (WLCG \u2013 a global collaboration of more than 170 computing centers in 42 countries) is used for simulation.\nFuture plans to upgrade CERN\u2019s LHC will dramatically increase particle collision rates. This makes it is important to investigate new frameworks like this and assess their potential in helping to ensure computing requirements remain manageable.\nA team of researchers at CERN, SURFsara, and Intel, are investigating the use of deep learning engines for fast simulation. This work is being carried out as part of Intel\u2019s long-standing collaboration with CERN through CERN openlab. CERN openlab is a public-private partnership, founded in 2001, which works to help accelerate innovation in Information and Communications Technology (ICT). Today, Intel and CERN are working together on a broad range of investigations, from hardware evaluation to HPC and AI.", "doc_id": 260}
{"doc": "The project team has found a way to encapsulate expensive computations into a neural net via training, and extract \u2013through inferencing \u2013 a result much faster than the standard algorithmic approaches of a classical Monte Carlo simulation. The team successfully simulated a calorimeter for a potential future accelerator \u2013 using a conditional generative adversarial network (GAN) \u2013 using only a fraction of the compute resources previously needed.\nThe key test for their resulting work is that their machine-learning-generated distribution is indistinguishable from other high-fidelity methods in physics-based simulations. Having achieved this, the team at CERN has shown a way to achieve orders of magnitude faster performance.", "doc_id": 261}
{"doc": "INFERENCE WITH AS LITTLE PRECISION AS POSSIBLE, BUT NO LESS\nIn 1950, the New York Times reported that Albert Einstein observed that \u201cEverything should be made as simple as possible, but no simpler.\u201d\nSuch is the challenge when you use reduced precision data types within a neural network. Many papers have explored training networks using less and less precision. The motivation is performance, the challenge is loss of accuracy.\nOne can envision two approaches to using INT8 data type: one is to train a network using INT8, and another is to train using FP32 data type and then quantize to INT8. The latter has the advantage that the quantization can be selective in quantizing only parts of the network that do not adversely affect accuracy. The quantization is achieved by iterative trials using reducing precision, and measuring the resulting change in accuracy. This feedback loop is illustrated below and is guided by a tuning strategy we can control.\nThe researchers at CERN used the Intel Low Precision Optimization Tool, which is a new open-source Python library that supports automatic accuracy-driven tuning strategies. The tool helps to speed up deployment of low-precision inferencing solutions on popular DL frameworks including TensorFlow, PyTorch, MXNet, and so forth. In addition to the GitHub site, it is included in Intel AI Analytics Toolkit along with Intel optimized versions of TensorFlow, PyTorch, and pre-trained models to accelerate deep learning workflows.", "doc_id": 262}
{"doc": "In their work, they found that about half of the computations in the GAN could switch from FP32 to INT8 numerical precision, as supported by Intel DL Boost, without loss of accuracy. They saw nearly a doubling in performance as a result. That matches our intuition since we expect a complete conversion from FP32 to INT8 could yield up to a theoretical maximum 4X gain in performance because of additional computational performance and reduction in memory bandwidth. With half the network converted, it makes sense that a little under a 2X gain was achieved when 4X was the theoretical maximum for a complete conversion.", "doc_id": 263}
{"doc": "It is important to note that this significant gain comes without sacrificing accuracy. A complete conversion to INT8 would give better performance, but with a loss of accuracy. Quantization is an important technique made relatively easy thanks to tools supporting automatic accuracy-driven tuning. This allows us to achieve performance boosts while managing accuracy to whatever level we wish. The CERN team wished to maintain accuracy and, as illustrated below, they still saw 1.8X in gains from quantization alone for their complex GAN model inferencing. It shows better accuracy as well (lower is better: INT8 accuracy of 0.05324 versus FP32 accuracy of 0.061227).", "doc_id": 264}
{"doc": "Quantization led to a 1.8X speed up by utilizing Intel DL Boost (specifically INT8 computations) on an Intel Xeon Platinum 8280 processor, and it shows slightly improved accuracy as well. (See the CERN paper Reduced Precision Strategies for Deep Learning: A High Energy Physics Generative Adversarial Network Use Case, to be presented at the 10th International Conference on Pattern Recognition Applications and Methods in February.)", "doc_id": 265}
{"doc": "Both FP32 and INT8 inferencing were previously optimized for multicore. Valeriu Codreanu, head of High Performance Computing and Visualization at SURF, explains this performance optimization: \u201cSince inferencing is less computationally expensive than training (as only the generator part of the GAN is being used), the hardware efficiency when using multiple cores in this process is not optimal. To overcome this, we have used multistream quantized inference, achieving a speed-up of 2.2x compared to single-stream quantized inference, using the same Intel Xeon Platinum 8280 system.\u201d This is illustrated below:", "doc_id": 266}
{"doc": "QUANTIZATION, INTEL DL BOOST, AND ONEAPI\nQuantization is proving to be an effective way to accelerate inferencing, and Intel Xeon Scalable processors built-in support for AI acceleration (Intel DL Boost) with INT8 shows just how powerful this can be. Performance was nearly doubled compared with the prior 32-bit inferencing while maintaining accuracy. The maintaining of accuracy is possible thanks to the open-source quantization tool used to manage accuracy to the needs of the developer. There are many more details of the work in their paper referenced above.\nINT8 has broad support thanks to Intel Xeon SP processors, and it is also supported in Intel Xe GPUs. FPGAs can certainly support INT8 and other reduced precision formats. Quantization methods offer effective ways to use powerful hardware support in many forms.\nThe secret sauce underlying this work and making it even better: oneAPI makes Intel DL Boost and other acceleration easily available without locking in applications to a single vendor or device\nIt is worth mentioning how oneAPI adds value to this type of work. Key parts of the tools used, including the acceleration tucked inside TensorFlow and Python, utilize libraries with oneAPI support. That means they are openly ready for heterogeneous systems instead of being specific to only one vendor or one product (e.g. GPU).\noneAPI is a cross-industry, open, standards-based unified programming model that delivers a common developer experience across accelerator architectures. Intel helped create oneAPI, and supports it with a range of open source compilers, libraries, and other tools. By programming to use INT8 via oneAPI, the kind of work done at CERN described in this article could be carried out using Intel Xe GPUs, FPGAs, or any other device supporting INT8 or other numerical formats for which they may quantize.", "doc_id": 267}
{"doc": "SUMMARY\nBoosting performance for inferencing has wide applicability, and is within reach thanks to Intel Xeon Scalable processors with Intel DL BoostAdditionally, training GANs and using Intel DL Boost to accelerate via quantization without sacrificing accuracy opens up exciting new possibilities for all applications that use Monte Carlo simulations.\nThe work at CERN to accelerate data analytics for probabilistic inference by quantizing and then using Intel Xeon Scalable processors is remarkably effective. It highlights why quantization has been gaining interest and will definitely become the norm for performance tuning inferencing workloads. Intel Xeon Scalable processors with Intel DL Boost were designed by engineers who foresaw this trend and Intel already has strong support in the processors today for this important inferencing acceleration capability.", "doc_id": 268}
