From c0365a972d652d98ab0eb7c648cf9d45b0447cf3 Mon Sep 17 00:00:00 2001
From: "Yi, A Wang" <yi.a.wang@intel.com>
Date: Tue, 24 Oct 2023 05:47:20 +0000
Subject: [PATCH] fix lm head overriden issue, move it from checkpoint in-loop
 loading to out loop

Signed-off-by: Yi, A Wang <yi.a.wang@intel.com>
---
 deepspeed/module_inject/load_checkpoint.py |  8 --------
 deepspeed/module_inject/replace_module.py  | 19 +++++++++++--------
 2 files changed, 11 insertions(+), 16 deletions(-)

diff --git a/deepspeed/module_inject/load_checkpoint.py b/deepspeed/module_inject/load_checkpoint.py
index 713558f..11f2290 100644
--- a/deepspeed/module_inject/load_checkpoint.py
+++ b/deepspeed/module_inject/load_checkpoint.py
@@ -255,14 +255,6 @@ def load_model_with_checkpoint(r_module,
 
     load_module_recursive(r_module)
 
-    embedding_weight = None
-
-    for n, p in r_module.named_parameters():
-        if "word_embeddings." in n or "embed_tokens." in n or "wte." in n:
-            embedding_weight = p
-    tie_word_embeddings = getattr(r_module.config, 'tie_word_embeddings', True)
-    if embedding_weight is not None and r_module.lm_head.weight.is_meta and tie_word_embeddings:
-        r_module.lm_head.weight = embedding_weight
     for sd_ in sd:
         del sd_
     sd = None
diff --git a/deepspeed/module_inject/replace_module.py b/deepspeed/module_inject/replace_module.py
index 080e79c..560aabd 100644
--- a/deepspeed/module_inject/replace_module.py
+++ b/deepspeed/module_inject/replace_module.py
@@ -544,6 +544,15 @@ def replace_transformer_layer(orig_layer_impl, model, checkpoint_dict, config, m
 
         return new_module
 
+    def set_lm_head(module):
+        embedding_weight = None
+        for n, p in module.named_parameters():
+            if "word_embeddings." in n or "embed_tokens." in n or "wte." in n:
+                embedding_weight = p
+        if embedding_weight is not None and hasattr(module, "lm_head") and hasattr(
+                module.lm_head, "weight") and module.lm_head.weight.is_meta:
+            module.lm_head.weight = embedding_weight
+
     #TODO:[SW-148847] check new flow of loading checkpoint for AutoTP
     if False and checkpoint_dict != None and not config.replace_with_kernel_inject:
         # AutoTP shard loading
@@ -557,6 +566,7 @@ def replace_transformer_layer(orig_layer_impl, model, checkpoint_dict, config, m
                                              checkpoint=checkpoint[i])
             pbar.update(1)
             gc.collect()
+        set_lm_head(replaced_module)
     else:
         replaced_module = replace_module(model=model,
                                          orig_class=orig_layer_impl,
@@ -635,6 +645,7 @@ def replace_transformer_layer(orig_layer_impl, model, checkpoint_dict, config, m
                                                container=container_g)
                     sds = [None for _ in sds]
                     gc.collect()
+        set_lm_head(replaced_module)
         print(f"checkpoint loading time at rank {rank}: {time.time()-start_time} sec")
 
     if config.save_mp_checkpoint_path is not None:
@@ -803,14 +814,6 @@ def replace_module(model, orig_class, replace_fn, _replace_policy, checkpoint=No
         "You can find some samples here: https://github.com/microsoft/DeepSpeed/blob/master/deepspeed/module_inject/replace_policy.py"
 
     replaced_module, _ = _replace_module(model, policy, state_dict=sd)
-    if checkpoint != None:
-        embedding_weight = None
-        for n, p in replaced_module.named_parameters():
-            if "word_embeddings." in n or "embed_tokens." in n or "wte." in n:
-                embedding_weight = p
-        if embedding_weight is not None and hasattr(replaced_module, "lm_head") and hasattr(
-                replaced_module.lm_head, "weight") and replaced_module.lm_head.weight.is_meta:
-            replaced_module.lm_head.weight = embedding_weight
     return replaced_module
 
 
-- 
2.34.1

