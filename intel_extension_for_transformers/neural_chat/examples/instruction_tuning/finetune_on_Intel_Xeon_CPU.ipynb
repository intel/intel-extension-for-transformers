{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d62620f",
   "metadata": {
    "papermill": {
     "duration": 0.00822,
     "end_time": "2023-09-04T05:03:23.645333",
     "exception": false,
     "start_time": "2023-09-04T05:03:23.637113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Finetune LLaMA2 and MPT on Intel Xeon CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c327c003",
   "metadata": {
    "papermill": {
     "duration": 0.004233,
     "end_time": "2023-09-04T05:03:23.656624",
     "exception": false,
     "start_time": "2023-09-04T05:03:23.652391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Prerequisite​"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341265af",
   "metadata": {
    "papermill": {
     "duration": 0.003582,
     "end_time": "2023-09-04T05:03:23.664605",
     "exception": false,
     "start_time": "2023-09-04T05:03:23.661023",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.1 Setup Environment​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5bf612",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install intel-extension-for-transformers torch datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95001899",
   "metadata": {
    "papermill": {
     "duration": 0.002828,
     "end_time": "2023-09-04T05:03:23.670622",
     "exception": false,
     "start_time": "2023-09-04T05:03:23.667794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1.2 Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0a3909",
   "metadata": {
    "papermill": {
     "duration": 0.002961,
     "end_time": "2023-09-04T05:03:23.676592",
     "exception": false,
     "start_time": "2023-09-04T05:03:23.673631",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Download Alpaca dataset from [here](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d17f0af8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T05:03:23.684897Z",
     "iopub.status.busy": "2023-09-04T05:03:23.684260Z",
     "iopub.status.idle": "2023-09-04T05:03:23.704653Z",
     "shell.execute_reply": "2023-09-04T05:03:23.702044Z"
    },
    "papermill": {
     "duration": 0.030173,
     "end_time": "2023-09-04T05:03:23.709551",
     "exception": false,
     "start_time": "2023-09-04T05:03:23.679378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "alpaca_data_path = \"/path/to/alpaca_data.json\"\n",
    "llama2_model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "mpt_model_name_or_path = \"mosaicml/mpt-7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560d487",
   "metadata": {
    "papermill": {
     "duration": 0.006213,
     "end_time": "2023-09-04T05:03:23.722287",
     "exception": false,
     "start_time": "2023-09-04T05:03:23.716074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Finetune LLaMA2 on Intel Xeon CPU with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e19e8",
   "metadata": {
    "papermill": {
     "duration": 0.003017,
     "end_time": "2023-09-04T05:03:23.730215",
     "exception": false,
     "start_time": "2023-09-04T05:03:23.727198",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.1 Setup Finetuning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb256f78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T05:03:23.739832Z",
     "iopub.status.busy": "2023-09-04T05:03:23.739217Z",
     "iopub.status.idle": "2023-09-04T05:03:27.318562Z",
     "shell.execute_reply": "2023-09-04T05:03:27.315683Z"
    },
    "papermill": {
     "duration": 3.589273,
     "end_time": "2023-09-04T05:03:27.322844",
     "exception": false,
     "start_time": "2023-09-04T05:03:23.733571",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'habana_frameworks.torch.hpu' is not installed.\n",
      "Package 'intel_extension_for_pytorch' is not installed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=llama2_model_name_or_path,\n",
    "    use_fast_tokenizer=False,\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    train_file=alpaca_data_path,\n",
    "    dataset_concatenation=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_peft_finetuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "finetune_args = FinetuningArguments(\n",
    "    lora_all_linear=True,\n",
    "    do_lm_eval=True,\n",
    ")\n",
    "\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "        model_args=model_args,\n",
    "        data_args=data_args,\n",
    "        training_args=training_args,\n",
    "        finetune_args=finetune_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deb125c",
   "metadata": {
    "papermill": {
     "duration": 0.005453,
     "end_time": "2023-09-04T05:03:27.335842",
     "exception": false,
     "start_time": "2023-09-04T05:03:27.330389",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2.2 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a780d52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-04T05:03:27.348803Z",
     "iopub.status.busy": "2023-09-04T05:03:27.347877Z",
     "iopub.status.idle": "2023-09-05T01:53:29.528293Z",
     "shell.execute_reply": "2023-09-05T01:53:29.527348Z"
    },
    "papermill": {
     "duration": 75002.192354,
     "end_time": "2023-09-05T01:53:29.533552",
     "exception": false,
     "start_time": "2023-09-04T05:03:27.341198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:27 - WARNING - intel_extension_for_transformers.llm.finetuning.finetuning - Process rank: 0, device: cpu\n",
      "distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:27 - INFO - intel_extension_for_transformers.llm.finetuning.finetuning - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./llama_peft_finetuned_model/runs/Sep03_22-03-27_devcloud25,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=True,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./llama_peft_finetuned_model,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./llama_peft_finetuned_model,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-09-03 22:03:27,368 >> loading configuration file /home/devcloud/xyy/models/Llama-2-7b-hf/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:768] 2023-09-03 22:03:27,374 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/devcloud/xyy/models/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-09-03 22:03:27,376 >> loading configuration file /home/devcloud/xyy/models/Llama-2-7b-hf/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:768] 2023-09-03 22:03:27,380 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/devcloud/xyy/models/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-03 22:03:27,385 >> loading file tokenizer.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-03 22:03:27,387 >> loading file added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-03 22:03:27,390 >> loading file special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-03 22:03:27,392 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/load.py:2072: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=None' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:27 - INFO - datasets.builder - Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:27 - INFO - datasets.info - Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:27 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:27 - INFO - datasets.builder - Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:27 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.builder - Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.info - Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.builder - Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.builder - Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.info - Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.builder - Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:28 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:2600] 2023-09-03 22:03:28,763 >> loading weights file /home/devcloud/xyy/models/Llama-2-7b-hf/model.safetensors.index.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1172] 2023-09-03 22:03:28,765 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:599] 2023-09-03 22:03:28,768 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   0%|                                                                                                                                              | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  50%|███████████████████████████████████████████████████████████████████                                                                   | 1/2 [00:01<00:01,  1.67s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.02s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO|modeling_utils.py:3329] 2023-09-03 22:03:31,319 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3337] 2023-09-03 22:03:31,320 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /home/devcloud/xyy/models/Llama-2-7b-hf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:559] 2023-09-03 22:03:31,324 >> loading configuration file /home/devcloud/xyy/models/Llama-2-7b-hf/generation_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:599] 2023-09-03 22:03:31,326 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e4de1a0661c32ad1.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:32 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-e4de1a0661c32ad1.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:38 - INFO - intel_extension_for_transformers.llm.finetuning.finetuning - Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/03/2023 22:03:38 - INFO - intel_extension_for_transformers.llm.finetuning.finetuning - Using data collator of type DataCollatorForSeq2Seq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1686] 2023-09-03 22:04:25,240 >> ***** Running training *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1687] 2023-09-03 22:04:25,241 >>   Num examples = 12,390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1688] 2023-09-03 22:04:25,243 >>   Num Epochs = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1689] 2023-09-03 22:04:25,245 >>   Instantaneous batch size per device = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1692] 2023-09-03 22:04:25,246 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1693] 2023-09-03 22:04:25,247 >>   Gradient Accumulation steps = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1694] 2023-09-03 22:04:25,248 >>   Total optimization steps = 4,647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1695] 2023-09-03 22:04:25,252 >>   Number of trainable parameters = 19,988,480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2957573965106688\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4647' max='4647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4647/4647 20:29:57, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.121300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.113300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.078200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.068300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.073900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.045500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.039200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1934] 2023-09-04 18:35:01,509 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-09-04 18:35:01,693 >> loading configuration file /home/devcloud/xyy/models/Llama-2-7b-hf/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:768] 2023-09-04 18:35:01,695 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/home/devcloud/xyy/models/Llama-2-7b-hf\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-04 18:35:01,696 >> loading file tokenizer.model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-04 18:35:01,697 >> loading file added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-04 18:35:01,697 >> loading file special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-04 18:35:01,699 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:599] 2023-09-04 18:35:01,710 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/devcloud/.cache/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:35:02 - INFO - datasets.info - Loading Dataset Infos from /home/devcloud/.cache/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:35:02 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:35:02 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset truthful_qa (/home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:35:02 - INFO - datasets.builder - Found cached dataset truthful_qa (/home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:35:02 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5882/5882 [18:20<00:00,  5.34it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:29 - INFO - intel_extension_for_transformers.llm.finetuning.finetuning - {'results': {'truthfulqa_mc': {'mc1': 0.3157894736842105, 'mc1_stderr': 0.016272287957916916, 'mc2': 0.4612383556358881, 'mc2_stderr': 0.015009882867823779}}, 'versions': {'truthfulqa_mc': 1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Task     |Version|Metric|Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|---|-----:|\n",
      "|truthfulqa_mc|      1|mc1   |0.3158|±  |0.0163|\n",
      "|             |       |mc2   |0.4612|±  |0.0150|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8f1b4",
   "metadata": {
    "papermill": {
     "duration": 0.021833,
     "end_time": "2023-09-05T01:53:29.600781",
     "exception": false,
     "start_time": "2023-09-05T01:53:29.578948",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Finetune MPT on Intel Xeon CPU with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa63be",
   "metadata": {
    "papermill": {
     "duration": 0.020948,
     "end_time": "2023-09-05T01:53:29.643007",
     "exception": false,
     "start_time": "2023-09-05T01:53:29.622059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.1 Setup Finetuning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932e2b52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T01:53:29.688033Z",
     "iopub.status.busy": "2023-09-05T01:53:29.687189Z",
     "iopub.status.idle": "2023-09-05T01:53:29.708198Z",
     "shell.execute_reply": "2023-09-05T01:53:29.706980Z"
    },
    "papermill": {
     "duration": 0.045506,
     "end_time": "2023-09-05T01:53:29.709472",
     "exception": false,
     "start_time": "2023-09-05T01:53:29.663966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1299] 2023-09-04 18:53:29,696 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1713] 2023-09-04 18:53:29,698 >> PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1439] 2023-09-04 18:53:29,700 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|integrations.py:81] 2023-09-04 18:53:29,703 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=mpt_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    train_file=alpaca_data_path,\n",
    "    dataset_concatenation=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mpt_peft_finetuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "finetune_args = FinetuningArguments(\n",
    "    lora_all_linear=True,\n",
    "    do_lm_eval=True,\n",
    ")\n",
    "\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "        model_args=model_args,\n",
    "        data_args=data_args,\n",
    "        training_args=training_args,\n",
    "        finetune_args=finetune_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0814742",
   "metadata": {
    "papermill": {
     "duration": 0.021156,
     "end_time": "2023-09-05T01:53:29.760425",
     "exception": false,
     "start_time": "2023-09-05T01:53:29.739269",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3.2 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfd5ed90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-05T01:53:29.805625Z",
     "iopub.status.busy": "2023-09-05T01:53:29.804295Z",
     "iopub.status.idle": "2023-09-05T16:42:47.263053Z",
     "shell.execute_reply": "2023-09-05T16:42:47.261990Z"
    },
    "papermill": {
     "duration": 53357.484821,
     "end_time": "2023-09-05T16:42:47.266241",
     "exception": false,
     "start_time": "2023-09-05T01:53:29.781420",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1299] 2023-09-04 18:53:29,807 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1713] 2023-09-04 18:53:29,809 >> PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:29 - WARNING - intel_extension_for_transformers.llm.finetuning.finetuning - Process rank: 0, device: cpu\n",
      "distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:29 - INFO - intel_extension_for_transformers.llm.finetuning.finetuning - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=0,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./mpt_peft_finetuned_model/runs/Sep04_18-53-29_devcloud25,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=True,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./mpt_peft_finetuned_model,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./mpt_peft_finetuned_model,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-09-04 18:53:29,817 >> loading configuration file /home/devcloud/xyy/models/mpt-7b/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-09-04 18:53:29,824 >> loading configuration file /home/devcloud/xyy/models/mpt-7b/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:768] 2023-09-04 18:53:29,827 >> Model config MPTConfig {\n",
      "  \"_name_or_path\": \"/home/devcloud/xyy/models/mpt-7b\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": true,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-09-04 18:53:29,829 >> loading configuration file /home/devcloud/xyy/models/mpt-7b/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-09-04 18:53:29,832 >> loading configuration file /home/devcloud/xyy/models/mpt-7b/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:768] 2023-09-04 18:53:29,834 >> Model config MPTConfig {\n",
      "  \"_name_or_path\": \"/home/devcloud/xyy/models/mpt-7b\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": true,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-04 18:53:29,837 >> loading file vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-04 18:53:29,837 >> loading file merges.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-04 18:53:29,838 >> loading file tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-04 18:53:29,839 >> loading file added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-04 18:53:29,839 >> loading file special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-04 18:53:29,840 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.builder - Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.info - Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.builder - Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.builder - Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.info - Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.builder - Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:30 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:31 - INFO - datasets.builder - Using custom data configuration default-b171c08c62141a34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:31 - INFO - datasets.info - Loading Dataset Infos from /home/devcloud/miniconda3/envs/neuralchat/lib/python3.9/site-packages/datasets/packaged_modules/json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:31 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:31 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:31 - INFO - datasets.builder - Found cached dataset json (/home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:31 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:2600] 2023-09-04 18:53:31,129 >> loading weights file /home/devcloud/xyy/models/mpt-7b/pytorch_model.bin.index.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:1172] 2023-09-04 18:53:31,131 >> Instantiating MPTForCausalLM model under default dtype torch.bfloat16.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:599] 2023-09-04 18:53:31,132 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /home/devcloud/.cache/huggingface/modules/transformers_modules/mpt-7b/modeling_mpt.py\n",
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:   0%|                                                                                                                                              | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards:  50%|███████████████████████████████████████████████████████████████████                                                                   | 1/2 [00:03<00:03,  3.60s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.35s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.54s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO|modeling_utils.py:3329] 2023-09-04 18:53:36,286 >> All model checkpoint weights were used when initializing MPTForCausalLM.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:3337] 2023-09-04 18:53:36,287 >> All the weights of MPTForCausalLM were initialized from the model checkpoint at /home/devcloud/xyy/models/mpt-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MPTForCausalLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:559] 2023-09-04 18:53:36,289 >> loading configuration file /home/devcloud/xyy/models/mpt-7b/generation_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:599] 2023-09-04 18:53:36,289 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-717680c06aa312fc.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:37 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/devcloud/.cache/huggingface/datasets/json/default-b171c08c62141a34/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-717680c06aa312fc.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:43 - INFO - intel_extension_for_transformers.llm.finetuning.finetuning - Splitting train dataset in train and validation according to `eval_dataset_size`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/04/2023 18:53:43 - INFO - intel_extension_for_transformers.llm.finetuning.finetuning - Using data collator of type DataCollatorForSeq2Seq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1686] 2023-09-04 18:54:23,849 >> ***** Running training *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1687] 2023-09-04 18:54:23,850 >>   Num examples = 10,743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1688] 2023-09-04 18:54:23,850 >>   Num Epochs = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1689] 2023-09-04 18:54:23,851 >>   Instantaneous batch size per device = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1692] 2023-09-04 18:54:23,851 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1693] 2023-09-04 18:54:23,851 >>   Gradient Accumulation steps = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1694] 2023-09-04 18:54:23,852 >>   Total optimization steps = 4,029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1695] 2023-09-04 18:54:23,859 >>   Number of trainable parameters = 16,777,216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:280] 2023-09-04 18:54:23,873 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 6,666,063,872 || trainable%: 0.2516809967943853\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4029' max='4029' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4029/4029 14:24:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.274000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.238600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.233200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.217500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.192600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.209100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1934] 2023-09-05 09:19:23,140 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-09-05 09:19:23,180 >> loading configuration file /home/devcloud/xyy/models/mpt-7b/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:710] 2023-09-05 09:19:23,182 >> loading configuration file /home/devcloud/xyy/models/mpt-7b/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:768] 2023-09-05 09:19:23,184 >> Model config MPTConfig {\n",
      "  \"_name_or_path\": \"/home/devcloud/xyy/models/mpt-7b\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": true,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-05 09:19:23,185 >> loading file vocab.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-05 09:19:23,186 >> loading file merges.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-05 09:19:23,186 >> loading file tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-05 09:19:23,187 >> loading file added_tokens.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-05 09:19:23,188 >> loading file special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1837] 2023-09-05 09:19:23,189 >> loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:599] 2023-09-05 09:19:23,242 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiating an MPTForCausalLM model from /home/devcloud/.cache/huggingface/modules/transformers_modules/mpt-7b/modeling_mpt.py\n",
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /home/devcloud/.cache/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/05/2023 09:19:24 - INFO - datasets.info - Loading Dataset Infos from /home/devcloud/.cache/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/05/2023 09:19:24 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/05/2023 09:19:24 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset truthful_qa (/home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/05/2023 09:19:24 - INFO - datasets.builder - Found cached dataset truthful_qa (/home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/05/2023 09:19:24 - INFO - datasets.info - Loading Dataset info from /home/devcloud/.cache/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5882/5882 [23:19<00:00,  4.20it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/05/2023 09:42:47 - INFO - intel_extension_for_transformers.llm.finetuning.finetuning - {'results': {'truthfulqa_mc': {'mc1': 0.2607099143206854, 'mc1_stderr': 0.015368841620766372, 'mc2': 0.3698070432066931, 'mc2_stderr': 0.014355345799541779}}, 'versions': {'truthfulqa_mc': 1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Task     |Version|Metric|Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|---|-----:|\n",
      "|truthfulqa_mc|      1|mc1   |0.2607|±  |0.0154|\n",
      "|             |       |mc2   |0.3698|±  |0.0144|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "finetune_model(finetune_cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuralchat",
   "language": "python",
   "name": "neuralchat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 128369.690355,
   "end_time": "2023-09-05T16:42:51.931791",
   "environment_variables": {},
   "exception": null,
   "input_path": "finetune_on_Intel_Xeon_CPU.ipynb",
   "output_path": "tmp.ipynb",
   "parameters": {},
   "start_time": "2023-09-04T05:03:22.241436",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
