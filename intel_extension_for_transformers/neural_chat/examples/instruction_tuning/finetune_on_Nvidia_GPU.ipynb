{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune LLaMA2 and MPT on NVIDIA GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prerequisite​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setup Environment​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install intel-extension-for-transformers torch datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download Alpaca dataset from [here](https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca_data_path = \"/path/to/alpaca_data.json\"\n",
    "llama2_model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "mpt_model_name_or_path = \"mosaicml/mpt-7b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Finetune LLaMA2 on NVIDIA GPU with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Setup Finetuning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "\u001b[33m[2023-08-28 21:56:57,180] [ WARNING]\u001b[0m - Detected that datasets module was imported before paddlenlp. This may cause PaddleNLP datasets to be unavalible in intranet. Please import paddlenlp before datasets module to avoid download issues\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package 'habana_frameworks.torch.hpu' is not installed.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=llama2_model_name_or_path,\n",
    "    use_fast_tokenizer=False,\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    train_file=alpaca_data_path,\n",
    "    dataset_concatenation=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_peft_finetuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "finetune_args = FinetuningArguments(\n",
    "    lora_all_linear=True,\n",
    "    do_lm_eval=True,\n",
    ")\n",
    "\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "        model_args=model_args,\n",
    "        data_args=data_args,\n",
    "        training_args=training_args,\n",
    "        finetune_args=finetune_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-28 21:57:21,417] [ WARNING] finetuning.py:97 - Process rank: 0, device: cuda:0\n",
      "distributed training: True, 16-bits training: True\n",
      "[2023-08-28 21:57:21,419] [    INFO] finetuning.py:101 - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./llama_peft_finetuned_model/runs/Aug28_21-57-00_mlp-dgx-01,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./llama_peft_finetuned_model,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./llama_peft_finetuned_model,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "[INFO|configuration_utils.py:710] 2023-08-28 21:57:21,420 >> loading configuration file /models/Llama-2-7b-hf/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-28 21:57:21,422 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/models/Llama-2-7b-hf/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:710] 2023-08-28 21:57:21,422 >> loading configuration file /models/Llama-2-7b-hf/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-28 21:57:21,423 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/models/Llama-2-7b-hf/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 21:57:21,425 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 21:57:21,425 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 21:57:21,425 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 21:57:21,426 >> loading file tokenizer_config.json\n",
      "[2023-08-28 21:57:22,706] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-28 21:57:22,707] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-28 21:57:22,709] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-28 21:57:22,710] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-28 21:57:22,712] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-28 21:57:22,712] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "100%|██████████| 1/1 [00:00<00:00, 433.39it/s]\n",
      "[2023-08-28 21:57:23,941] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-28 21:57:23,942] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-28 21:57:23,943] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-28 21:57:23,944] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-28 21:57:23,946] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-28 21:57:23,947] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-28 21:57:25,375] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-28 21:57:25,376] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-28 21:57:25,378] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-28 21:57:25,379] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-28 21:57:25,380] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-28 21:57:25,381] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[INFO|modeling_utils.py:2600] 2023-08-28 21:57:25,401 >> loading weights file /models/Llama-2-7b-hf/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1172] 2023-08-28 21:57:25,402 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:599] 2023-08-28 21:57:25,403 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n",
      "[INFO|modeling_utils.py:3329] 2023-08-28 21:57:27,299 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3337] 2023-08-28 21:57:27,300 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /models/Llama-2-7b-hf/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:559] 2023-08-28 21:57:27,303 >> loading configuration file /models/Llama-2-7b-hf/generation_config.json\n",
      "[INFO|configuration_utils.py:599] 2023-08-28 21:57:27,304 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.9,\n",
      "  \"top_p\": 0.6,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "[2023-08-28 21:57:28,920] [ WARNING] arrow_dataset.py:2993 - Loading cached processed dataset at /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-5f4bae92bef4909f.arrow\n",
      "[2023-08-28 21:57:38,161] [    INFO] finetuning.py:437 - Splitting train dataset in train and validation according to `eval_dataset_size`\n",
      "[2023-08-28 21:57:38,167] [    INFO] finetuning.py:457 - Using data collator of type DataCollatorForSeq2Seq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19,988,480 || all params: 6,758,404,096 || trainable%: 0.2957573965106688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1686] 2023-08-28 21:58:53,236 >> ***** Running training *****\n",
      "[INFO|trainer.py:1687] 2023-08-28 21:58:53,237 >>   Num examples = 12,390\n",
      "[INFO|trainer.py:1688] 2023-08-28 21:58:53,238 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1689] 2023-08-28 21:58:53,238 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1692] 2023-08-28 21:58:53,239 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1693] 2023-08-28 21:58:53,239 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1694] 2023-08-28 21:58:53,239 >>   Total optimization steps = 4,647\n",
      "[INFO|trainer.py:1695] 2023-08-28 21:58:53,249 >>   Number of trainable parameters = 19,988,480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4647' max='4647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4647/4647 1:38:48, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.151500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.121200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.113200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.075800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.071600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.075200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.044800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.035300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1934] 2023-08-28 23:37:43,349 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|configuration_utils.py:710] 2023-08-28 23:37:43,535 >> loading configuration file /models/Llama-2-7b-hf/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-28 23:37:43,536 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/models/Llama-2-7b-hf/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 23:37:43,537 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 23:37:43,538 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 23:37:43,538 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 23:37:43,539 >> loading file tokenizer_config.json\n",
      "[INFO|configuration_utils.py:599] 2023-08-28 23:37:43,552 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "[2023-08-28 23:37:43,913] [ WARNING] load.py:975 - Using the latest cached version of the module from /models/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4 (last modified on Mon Aug 28 01:36:02 2023) since it couldn't be found locally at truthful_qa., or remotely on the Hugging Face Hub.\n",
      "[2023-08-28 23:37:43,925] [    INFO] info.py:431 - Loading Dataset Infos from /models/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "[2023-08-28 23:37:43,935] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-28 23:37:43,936] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "[2023-08-28 23:37:43,944] [ WARNING] builder.py:817 - Found cached dataset truthful_qa (/models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n",
      "[2023-08-28 23:37:43,945] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "100%|██████████| 1/1 [00:00<00:00, 410.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5882/5882 [06:23<00:00, 15.32it/s]\n",
      "[2023-08-28 23:44:16,990] [    INFO] finetuning.py:554 - {'results': {'truthfulqa_mc': {'mc1': 0.31701346389228885, 'mc1_stderr': 0.016289203374403382, 'mc2': 0.4622660487303061, 'mc2_stderr': 0.015048114647057709}}, 'versions': {'truthfulqa_mc': 1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Task     |Version|Metric|Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|---|-----:|\n",
      "|truthfulqa_mc|      1|mc1   |0.3170|±  |0.0163|\n",
      "|             |       |mc2   |0.4623|±  |0.0150|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finetune MPT on NVIDIA GPU with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Setup Finetuning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1299] 2023-08-28 23:46:58,375 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1713] 2023-08-28 23:46:58,377 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1439] 2023-08-28 23:46:58,377 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "[WARNING|integrations.py:81] 2023-08-28 23:46:58,379 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=mpt_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    train_file=alpaca_data_path,\n",
    "    dataset_concatenation=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mpt_peft_finetuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "finetune_args = FinetuningArguments(\n",
    "    lora_all_linear=True,\n",
    "    do_lm_eval=True,\n",
    ")\n",
    "\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "        model_args=model_args,\n",
    "        data_args=data_args,\n",
    "        training_args=training_args,\n",
    "        finetune_args=finetune_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-28 23:47:04,819] [ WARNING] finetuning.py:97 - Process rank: 0, device: cuda:0\n",
      "distributed training: True, 16-bits training: True\n",
      "[2023-08-28 23:47:04,823] [    INFO] finetuning.py:101 - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./mpt_peft_finetuned_model/runs/Aug28_23-46-58_mlp-dgx-01,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./mpt_peft_finetuned_model,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./mpt_peft_finetuned_model,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "[INFO|configuration_utils.py:710] 2023-08-28 23:47:04,825 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:710] 2023-08-28 23:47:04,831 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-28 23:47:04,832 >> Model config MPTConfig {\n",
      "  \"_name_or_path\": \"/models/mpt-7b\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": true,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:710] 2023-08-28 23:47:04,833 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:710] 2023-08-28 23:47:04,835 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-28 23:47:04,836 >> Model config MPTConfig {\n",
      "  \"_name_or_path\": \"/models/mpt-7b\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": true,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 23:47:04,838 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 23:47:04,838 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 23:47:04,839 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 23:47:04,839 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 23:47:04,839 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-28 23:47:04,839 >> loading file tokenizer_config.json\n",
      "[2023-08-28 23:47:06,195] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-28 23:47:06,196] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-28 23:47:06,201] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-28 23:47:06,202] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-28 23:47:06,203] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-28 23:47:06,204] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "100%|██████████| 1/1 [00:00<00:00, 97.34it/s]\n",
      "[2023-08-28 23:47:07,460] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-28 23:47:07,461] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-28 23:47:07,462] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-28 23:47:07,462] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-28 23:47:07,463] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-28 23:47:07,464] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-28 23:47:08,707] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-28 23:47:08,708] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-28 23:47:08,709] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-28 23:47:08,709] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-28 23:47:08,710] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-28 23:47:08,711] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[WARNING|modeling_utils.py:2211] 2023-08-28 23:47:08,736 >> The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "[INFO|modeling_utils.py:2600] 2023-08-28 23:47:08,737 >> loading weights file /models/mpt-7b/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1172] 2023-08-28 23:47:08,738 >> Instantiating MPTForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:599] 2023-08-28 23:47:08,740 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.50s/it]\n",
      "[INFO|modeling_utils.py:3329] 2023-08-28 23:47:15,869 >> All model checkpoint weights were used when initializing MPTForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3337] 2023-08-28 23:47:15,870 >> All the weights of MPTForCausalLM were initialized from the model checkpoint at /models/mpt-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MPTForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:559] 2023-08-28 23:47:15,873 >> loading configuration file /models/mpt-7b/generation_config.json\n",
      "[INFO|configuration_utils.py:599] 2023-08-28 23:47:15,874 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Map:   0%|          | 0/52002 [00:00<?, ? examples/s][2023-08-28 23:47:17,729] [    INFO] arrow_dataset.py:3325 - Caching processed dataset at /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-d1a3e321d0994dc1.arrow\n",
      "[2023-08-28 23:47:36,130] [    INFO] finetuning.py:437 - Splitting train dataset in train and validation according to `eval_dataset_size`\n",
      "[2023-08-28 23:47:36,137] [    INFO] finetuning.py:457 - Using data collator of type DataCollatorForSeq2Seq\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 6,666,063,872 || trainable%: 0.2516809967943853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1686] 2023-08-28 23:48:45,740 >> ***** Running training *****\n",
      "[INFO|trainer.py:1687] 2023-08-28 23:48:45,741 >>   Num examples = 10,743\n",
      "[INFO|trainer.py:1688] 2023-08-28 23:48:45,741 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1689] 2023-08-28 23:48:45,741 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1692] 2023-08-28 23:48:45,742 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1693] 2023-08-28 23:48:45,742 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1694] 2023-08-28 23:48:45,742 >>   Total optimization steps = 4,029\n",
      "[INFO|trainer.py:1695] 2023-08-28 23:48:45,745 >>   Number of trainable parameters = 16,777,216\n",
      "[WARNING|logging.py:280] 2023-08-28 23:48:45,759 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4029' max='4029' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4029/4029 1:06:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.322200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.288700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.284000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.246200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.235800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.202800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.211600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1934] 2023-08-29 00:54:47,851 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 00:54:47,936 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 00:54:47,939 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-29 00:54:47,940 >> Model config MPTConfig {\n",
      "  \"_name_or_path\": \"/models/mpt-7b\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": true,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 00:54:47,941 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 00:54:47,941 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 00:54:47,942 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 00:54:47,942 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 00:54:47,942 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 00:54:47,942 >> loading file tokenizer_config.json\n",
      "[INFO|configuration_utils.py:599] 2023-08-29 00:54:48,008 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[2023-08-29 00:54:50,703] [    INFO] file_utils.py:326 - HEAD request to https://huggingface.co/datasets/truthful_qa/resolve/main/truthful_qa.py timed out, retrying... [1.0]\n",
      "[2023-08-29 00:54:51,609] [ WARNING] load.py:975 - Using the latest cached version of the module from /models/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4 (last modified on Mon Aug 28 01:36:02 2023) since it couldn't be found locally at truthful_qa., or remotely on the Hugging Face Hub.\n",
      "[2023-08-29 00:54:51,611] [    INFO] info.py:431 - Loading Dataset Infos from /models/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "[2023-08-29 00:54:51,620] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-29 00:54:51,620] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "[2023-08-29 00:54:51,626] [ WARNING] builder.py:817 - Found cached dataset truthful_qa (/models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n",
      "[2023-08-29 00:54:51,626] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "100%|██████████| 1/1 [00:00<00:00, 508.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5882/5882 [03:02<00:00, 32.22it/s]\n",
      "[2023-08-29 00:57:58,494] [    INFO] finetuning.py:554 - {'results': {'truthfulqa_mc': {'mc1': 0.25091799265605874, 'mc1_stderr': 0.015176985027707689, 'mc2': 0.36732950033142087, 'mc2_stderr': 0.01428485016453199}}, 'versions': {'truthfulqa_mc': 1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Task     |Version|Metric|Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|---|-----:|\n",
      "|truthfulqa_mc|      1|mc1   |0.2509|±  |0.0152|\n",
      "|             |       |mc2   |0.3673|±  |0.0143|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finetune LLaMA2 on NVIDIA GPU with QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Setup Finetuning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1299] 2023-08-29 01:38:40,537 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1713] 2023-08-29 01:38:40,538 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1439] 2023-08-29 01:38:40,539 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "[WARNING|integrations.py:81] 2023-08-29 01:38:40,543 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=llama2_model_name_or_path,\n",
    "    use_fast_tokenizer=False,\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    train_file=alpaca_data_path,\n",
    "    dataset_concatenation=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_peft_finetuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "finetune_args = FinetuningArguments(\n",
    "    lora_all_linear=True,\n",
    "    do_lm_eval=True,\n",
    "    qlora=True,\n",
    ")\n",
    "\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "        model_args=model_args,\n",
    "        data_args=data_args,\n",
    "        training_args=training_args,\n",
    "        finetune_args=finetune_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-29 01:38:46,976] [ WARNING] finetuning.py:97 - Process rank: 0, device: cuda:0\n",
      "distributed training: True, 16-bits training: True\n",
      "[2023-08-29 01:38:46,979] [    INFO] finetuning.py:101 - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./llama_peft_finetuned_model/runs/Aug29_01-38-40_mlp-dgx-01,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./llama_peft_finetuned_model,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./llama_peft_finetuned_model,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 01:38:46,989 >> loading configuration file /models/Llama-2-7b-hf/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-29 01:38:46,990 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/models/Llama-2-7b-hf/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 01:38:46,991 >> loading configuration file /models/Llama-2-7b-hf/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-29 01:38:46,992 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/models/Llama-2-7b-hf/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 01:38:46,994 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 01:38:46,994 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 01:38:46,995 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 01:38:46,996 >> loading file tokenizer_config.json\n",
      "[2023-08-29 01:38:48,348] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-29 01:38:48,349] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-29 01:38:48,351] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-29 01:38:48,352] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-29 01:38:48,354] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-29 01:38:48,355] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "100%|██████████| 1/1 [00:00<00:00, 367.60it/s]\n",
      "[2023-08-29 01:38:49,654] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-29 01:38:49,655] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-29 01:38:49,657] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-29 01:38:49,658] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-29 01:38:49,660] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-29 01:38:49,660] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-29 01:38:50,938] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-29 01:38:50,939] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-29 01:38:50,941] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-29 01:38:50,942] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-29 01:38:50,944] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-29 01:38:50,945] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[INFO|modeling_utils.py:2600] 2023-08-29 01:38:50,954 >> loading weights file /models/Llama-2-7b-hf/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1172] 2023-08-29 01:38:50,956 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:599] 2023-08-29 01:38:50,957 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2715] 2023-08-29 01:38:51,089 >> Detected 8-bit loading: activating 8-bit loading for this model\n",
      "[2023-08-29 01:38:51,421] [ WARNING] modeling.py:706 - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.26s/it]\n",
      "[INFO|modeling_utils.py:3329] 2023-08-29 01:38:57,978 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3337] 2023-08-29 01:38:57,979 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /models/Llama-2-7b-hf/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:559] 2023-08-29 01:38:57,983 >> loading configuration file /models/Llama-2-7b-hf/generation_config.json\n",
      "[INFO|modeling_utils.py:2949] 2023-08-29 01:38:57,985 >> Generation config file not found, using a generation config created from the model config.\n",
      "Map:   0%|          | 0/52002 [00:00<?, ? examples/s][2023-08-29 01:39:00,490] [    INFO] arrow_dataset.py:3325 - Caching processed dataset at /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-14ae9ee100cc9dab.arrow\n",
      "[2023-08-29 01:39:56,169] [    INFO] finetuning.py:437 - Splitting train dataset in train and validation according to `eval_dataset_size`\n",
      "[2023-08-29 01:39:56,175] [    INFO] finetuning.py:457 - Using data collator of type DataCollatorForSeq2Seq\n",
      "[INFO|trainer.py:386] 2023-08-29 01:40:31,518 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "[INFO|trainer.py:394] 2023-08-29 01:40:31,518 >> The model is loaded in 8-bit precision. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check  the examples in https://github.com/huggingface/peft for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 19,988,480 || all params: 3,520,401,408 || trainable%: 0.5677897967708119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1686] 2023-08-29 01:40:31,905 >> ***** Running training *****\n",
      "[INFO|trainer.py:1687] 2023-08-29 01:40:31,906 >>   Num examples = 12,390\n",
      "[INFO|trainer.py:1688] 2023-08-29 01:40:31,906 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1689] 2023-08-29 01:40:31,906 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1692] 2023-08-29 01:40:31,906 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1693] 2023-08-29 01:40:31,907 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1694] 2023-08-29 01:40:31,907 >>   Total optimization steps = 4,647\n",
      "[INFO|trainer.py:1695] 2023-08-29 01:40:31,912 >>   Number of trainable parameters = 19,988,480\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4647' max='4647' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4647/4647 1:40:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.163600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.129700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.121100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.082200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.077500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.081300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.049600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.036400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1934] 2023-08-29 03:21:28,703 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 03:21:28,817 >> loading configuration file /models/Llama-2-7b-hf/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-29 03:21:28,818 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/models/Llama-2-7b-hf/\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 03:21:28,818 >> loading file tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 03:21:28,819 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 03:21:28,819 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 03:21:28,820 >> loading file tokenizer_config.json\n",
      "[INFO|configuration_utils.py:599] 2023-08-29 03:21:28,838 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      "\n",
      "[2023-08-29 03:21:30,355] [ WARNING] load.py:975 - Using the latest cached version of the module from /models/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4 (last modified on Mon Aug 28 01:36:02 2023) since it couldn't be found locally at truthful_qa., or remotely on the Hugging Face Hub.\n",
      "[2023-08-29 03:21:30,363] [    INFO] info.py:431 - Loading Dataset Infos from /models/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "[2023-08-29 03:21:30,371] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-29 03:21:30,372] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "[2023-08-29 03:21:30,377] [ WARNING] builder.py:817 - Found cached dataset truthful_qa (/models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n",
      "[2023-08-29 03:21:30,378] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "100%|██████████| 1/1 [00:00<00:00, 350.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5882/5882 [04:10<00:00, 23.52it/s]\n",
      "[2023-08-29 03:25:49,385] [    INFO] finetuning.py:554 - {'results': {'truthfulqa_mc': {'mc1': 0.31456548347613217, 'mc1_stderr': 0.016255241993179178, 'mc2': 0.4529007721356952, 'mc2_stderr': 0.01493900384942977}}, 'versions': {'truthfulqa_mc': 1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Task     |Version|Metric|Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|---|-----:|\n",
      "|truthfulqa_mc|      1|mc1   |0.3146|±  |0.0163|\n",
      "|             |       |mc2   |0.4529|±  |0.0149|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "finetune_model(finetune_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Finetune MPT on NVIDIA GPU with QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Setup Finetuning Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1299] 2023-08-29 03:29:46,914 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1713] 2023-08-29 03:29:46,916 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1439] 2023-08-29 03:29:46,918 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "[WARNING|integrations.py:81] 2023-08-29 03:29:46,920 >> Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from intel_extension_for_transformers.neural_chat.config import (\n",
    "    ModelArguments,\n",
    "    DataArguments,\n",
    "    FinetuningArguments,\n",
    "    TextGenerationFinetuningConfig,\n",
    ")\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=mpt_model_name_or_path,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "data_args = DataArguments(\n",
    "    train_file=alpaca_data_path,\n",
    "    dataset_concatenation=True,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mpt_peft_finetuned_model\",\n",
    "    overwrite_output_dir=True,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=3,\n",
    "    save_strategy=\"no\",\n",
    "    log_level=\"info\",\n",
    "    save_total_limit=2,\n",
    "    bf16=True,\n",
    ")\n",
    "\n",
    "finetune_args = FinetuningArguments(\n",
    "    lora_all_linear=True,\n",
    "    do_lm_eval=True,\n",
    "    qlora=True,\n",
    ")\n",
    "\n",
    "finetune_cfg = TextGenerationFinetuningConfig(\n",
    "        model_args=model_args,\n",
    "        data_args=data_args,\n",
    "        training_args=training_args,\n",
    "        finetune_args=finetune_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-08-29 03:29:52,601] [ WARNING] finetuning.py:97 - Process rank: 0, device: cuda:0\n",
      "distributed training: True, 16-bits training: True\n",
      "[2023-08-29 03:29:52,604] [    INFO] finetuning.py:101 - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./mpt_peft_finetuned_model/runs/Aug29_03-29-46_mlp-dgx-01,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=3,\n",
      "optim=adamw_hf,\n",
      "optim_args=None,\n",
      "output_dir=./mpt_peft_finetuned_model,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./mpt_peft_finetuned_model,\n",
      "save_on_each_node=False,\n",
      "save_safetensors=False,\n",
      "save_steps=500,\n",
      "save_strategy=no,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 03:29:52,614 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 03:29:52,616 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-29 03:29:52,617 >> Model config MPTConfig {\n",
      "  \"_name_or_path\": \"/models/mpt-7b\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": true,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 03:29:52,618 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 03:29:52,620 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-29 03:29:52,622 >> Model config MPTConfig {\n",
      "  \"_name_or_path\": \"/models/mpt-7b\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": true,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 03:29:52,623 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 03:29:52,624 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 03:29:52,624 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 03:29:52,625 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 03:29:52,625 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 03:29:52,626 >> loading file tokenizer_config.json\n",
      "[2023-08-29 03:29:54,007] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-29 03:29:54,008] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-29 03:29:54,010] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-29 03:29:54,010] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-29 03:29:54,012] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-29 03:29:54,012] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "100%|██████████| 1/1 [00:00<00:00, 431.20it/s]\n",
      "[2023-08-29 03:29:55,330] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-29 03:29:55,330] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-29 03:29:55,331] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-29 03:29:55,332] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-29 03:29:55,332] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-29 03:29:55,333] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-29 03:29:56,660] [    INFO] builder.py:511 - Using custom data configuration default-0642ff4ffa8135a8\n",
      "[2023-08-29 03:29:56,661] [    INFO] info.py:431 - Loading Dataset Infos from /home/xinyuye/miniconda3/envs/py39/lib/python3.9/site-packages/datasets/packaged_modules/json\n",
      "[2023-08-29 03:29:56,662] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-29 03:29:56,662] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[2023-08-29 03:29:56,664] [ WARNING] builder.py:817 - Found cached dataset json (/models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e)\n",
      "[2023-08-29 03:29:56,664] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e\n",
      "[WARNING|modeling_utils.py:2211] 2023-08-29 03:29:56,667 >> The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "[INFO|modeling_utils.py:2600] 2023-08-29 03:29:56,669 >> loading weights file /models/mpt-7b/pytorch_model.bin.index.json\n",
      "[INFO|modeling_utils.py:1172] 2023-08-29 03:29:56,670 >> Instantiating MPTForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:599] 2023-08-29 03:29:56,671 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2715] 2023-08-29 03:29:56,762 >> Detected 8-bit loading: activating 8-bit loading for this model\n",
      "[2023-08-29 03:29:56,914] [ WARNING] modeling.py:706 - The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.50s/it]\n",
      "[INFO|modeling_utils.py:3329] 2023-08-29 03:30:07,939 >> All model checkpoint weights were used when initializing MPTForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:3337] 2023-08-29 03:30:07,939 >> All the weights of MPTForCausalLM were initialized from the model checkpoint at /models/mpt-7b.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MPTForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:559] 2023-08-29 03:30:07,942 >> loading configuration file /models/mpt-7b/generation_config.json\n",
      "[INFO|configuration_utils.py:599] 2023-08-29 03:30:07,943 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "Map:   0%|          | 0/52002 [00:00<?, ? examples/s][2023-08-29 03:30:09,754] [    INFO] arrow_dataset.py:3325 - Caching processed dataset at /models/huggingface/datasets/json/default-0642ff4ffa8135a8/0.0.0/fe5dd6ea2639a6df622901539cb550cf8797e5a6b2dd7af1cf934bed8e233e6e/cache-437eef0fa2b15808.arrow\n",
      "[2023-08-29 03:30:27,725] [    INFO] finetuning.py:437 - Splitting train dataset in train and validation according to `eval_dataset_size`\n",
      "[2023-08-29 03:30:27,732] [    INFO] finetuning.py:457 - Using data collator of type DataCollatorForSeq2Seq\n",
      "[INFO|trainer.py:386] 2023-08-29 03:31:04,017 >> You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "[INFO|trainer.py:394] 2023-08-29 03:31:04,018 >> The model is loaded in 8-bit precision. To train this model you need to add additional modules inside the model such as adapters using `peft` library and freeze the model weights. Please check  the examples in https://github.com/huggingface/peft for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 16,777,216 || all params: 3,444,838,400 || trainable%: 0.4870247614517999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1686] 2023-08-29 03:31:04,599 >> ***** Running training *****\n",
      "[INFO|trainer.py:1687] 2023-08-29 03:31:04,600 >>   Num examples = 10,743\n",
      "[INFO|trainer.py:1688] 2023-08-29 03:31:04,601 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1689] 2023-08-29 03:31:04,601 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:1692] 2023-08-29 03:31:04,602 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1693] 2023-08-29 03:31:04,602 >>   Gradient Accumulation steps = 2\n",
      "[INFO|trainer.py:1694] 2023-08-29 03:31:04,603 >>   Total optimization steps = 4,029\n",
      "[INFO|trainer.py:1695] 2023-08-29 03:31:04,607 >>   Number of trainable parameters = 16,777,216\n",
      "[WARNING|logging.py:280] 2023-08-29 03:31:04,622 >> You're using a GPTNeoXTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4029' max='4029' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4029/4029 1:14:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.298700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.222500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.218100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1934] 2023-08-29 04:45:45,421 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 04:45:45,507 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:710] 2023-08-29 04:45:45,509 >> loading configuration file /models/mpt-7b/config.json\n",
      "[INFO|configuration_utils.py:768] 2023-08-29 04:45:45,510 >> Model config MPTConfig {\n",
      "  \"_name_or_path\": \"/models/mpt-7b\",\n",
      "  \"architectures\": [\n",
      "    \"MPTForCausalLM\"\n",
      "  ],\n",
      "  \"attn_config\": {\n",
      "    \"alibi\": true,\n",
      "    \"alibi_bias_max\": 8,\n",
      "    \"attn_impl\": \"torch\",\n",
      "    \"attn_pdrop\": 0,\n",
      "    \"attn_type\": \"multihead_attention\",\n",
      "    \"attn_uses_sequence_id\": false,\n",
      "    \"clip_qkv\": null,\n",
      "    \"prefix_lm\": false,\n",
      "    \"qk_ln\": false,\n",
      "    \"softmax_scale\": null\n",
      "  },\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"configuration_mpt.MPTConfig\",\n",
      "    \"AutoModelForCausalLM\": \"modeling_mpt.MPTForCausalLM\"\n",
      "  },\n",
      "  \"d_model\": 4096,\n",
      "  \"emb_pdrop\": 0,\n",
      "  \"embedding_fraction\": 1.0,\n",
      "  \"expansion_ratio\": 4,\n",
      "  \"init_config\": {\n",
      "    \"emb_init_std\": null,\n",
      "    \"emb_init_uniform_lim\": null,\n",
      "    \"fan_mode\": \"fan_in\",\n",
      "    \"init_div_is_residual\": true,\n",
      "    \"init_gain\": 0,\n",
      "    \"init_nonlinearity\": \"relu\",\n",
      "    \"init_std\": 0.02,\n",
      "    \"name\": \"kaiming_normal_\",\n",
      "    \"verbose\": 0\n",
      "  },\n",
      "  \"init_device\": \"cpu\",\n",
      "  \"learned_pos_emb\": true,\n",
      "  \"logit_scale\": null,\n",
      "  \"max_seq_len\": 2048,\n",
      "  \"model_type\": \"mpt\",\n",
      "  \"n_heads\": 32,\n",
      "  \"n_layers\": 32,\n",
      "  \"no_bias\": true,\n",
      "  \"norm_type\": \"low_precision_layernorm\",\n",
      "  \"resid_pdrop\": 0,\n",
      "  \"tokenizer_name\": \"EleutherAI/gpt-neox-20b\",\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false,\n",
      "  \"verbose\": 0,\n",
      "  \"vocab_size\": 50432\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 04:45:45,511 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 04:45:45,512 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 04:45:45,512 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 04:45:45,513 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 04:45:45,514 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1837] 2023-08-29 04:45:45,515 >> loading file tokenizer_config.json\n",
      "[INFO|configuration_utils.py:599] 2023-08-29 04:45:45,575 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"transformers_version\": \"4.31.0\",\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n",
      "[2023-08-29 04:45:45,813] [ WARNING] load.py:975 - Using the latest cached version of the module from /models/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4 (last modified on Mon Aug 28 01:36:02 2023) since it couldn't be found locally at truthful_qa., or remotely on the Hugging Face Hub.\n",
      "[2023-08-29 04:45:45,826] [    INFO] info.py:431 - Loading Dataset Infos from /models/huggingface/modules/datasets_modules/datasets/truthful_qa/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "[2023-08-29 04:45:45,838] [    INFO] builder.py:380 - Overwrite dataset info from restored data version if exists.\n",
      "[2023-08-29 04:45:45,846] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "[2023-08-29 04:45:45,854] [ WARNING] builder.py:817 - Found cached dataset truthful_qa (/models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4)\n",
      "[2023-08-29 04:45:45,859] [    INFO] info.py:351 - Loading Dataset info from /models/huggingface/datasets/truthful_qa/multiple_choice/1.1.0/63502f6bc6ee493830ce0843991b028d0ab568d221896b2ee3b8a5dfdaa9d7f4\n",
      "100%|██████████| 1/1 [00:00<00:00, 208.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running loglikelihood requests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5882/5882 [04:13<00:00, 23.18it/s]\n",
      "[2023-08-29 04:50:04,639] [    INFO] finetuning.py:554 - {'results': {'truthfulqa_mc': {'mc1': 0.24479804161566707, 'mc1_stderr': 0.01505186948671501, 'mc2': 0.36567230051304445, 'mc2_stderr': 0.014372749953340895}}, 'versions': {'truthfulqa_mc': 1}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|    Task     |Version|Metric|Value |   |Stderr|\n",
      "|-------------|------:|------|-----:|---|-----:|\n",
      "|truthfulqa_mc|      1|mc1   |0.2448|±  |0.0151|\n",
      "|             |       |mc2   |0.3657|±  |0.0144|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from intel_extension_for_transformers.neural_chat.chatbot import finetune_model\n",
    "finetune_model(finetune_cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
