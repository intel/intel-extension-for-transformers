import copy
import os
import torch
import unittest
import shutil
import torch.utils.data as data
from peft import (
    LoraConfig,
    TaskType,
    get_peft_model,
)
from transformers import (
    AutoTokenizer,
    TrainingArguments,
    Trainer
)
from intel_extension_for_transformers.transformers.modeling import AutoModelForCausalLM
from intel_extension_for_transformers.llm.quantization.nn.modules import QuantizedLinearQBits, QuantizedLoraLinearQBits
from intel_extension_for_transformers.llm.quantization.utils import convert_to_quantized_model, replace_linear
from intel_extension_for_transformers.transformers import WeightOnlyQuantConfig


class DummyDataset(data.Dataset):
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(llama_model_path)
        self.sequences = "Where is intel-extension-for-transformers based? NYC or SH? " + \
            "intel-extension-for-transformers is based in SH."
        self.encoded_dict = self.tokenizer(self.sequences)
        self.encoded_dict['labels'] = self.encoded_dict['input_ids']

    def __len__(self):
        return 1

    def __getitem__(self, index):
        """Returns one data pair (source and target)."""
        return self.encoded_dict

class M(torch.nn.Module):
    def __init__(self, with_bias=False):
        super().__init__()
        self.linear = torch.nn.Linear(32, 2, bias=with_bias)

    def forward(self, x):
        return self.linear(x)


llama_model_path = "fxmarty/tiny-llama-fast-tokenizer"

class TestWeightOnly(unittest.TestCase):
    
    @classmethod
    def setUpClass(cls):
        cls.workspace = "./woq_config_tmp"
        # if workspace not exist, crate it
        if not os.path.exists(cls.workspace):
            os.mkdir(cls.workspace)
    
    @classmethod
    def tearDownClass(cls) -> None:
        shutil.rmtree(cls.workspace, ignore_errors=True)
        shutil.rmtree('tmp', ignore_errors=True)

    def test_woq_config(self):
        config = WeightOnlyQuantConfig(weight_dtype="int4_fullrange", group_size=32)
        diff_res = config.to_diff_dict()
        ref_config = {'weight_dtype': 'int4_fullrange'}
        self.assertEqual(diff_res, ref_config)
        print(diff_res)
        print(config.to_dict())
        print(config.to_json_string())
        config.to_json_file(f"{self.workspace}/config.json")
        print(config)

    def test_woq_config_post_init_runtime(self):
        config = WeightOnlyQuantConfig(weight_dtype="fp4", compute_dtype="int8", scheme="asym",
                                       scale_dtype="fp8")
        config.post_init_runtime()
        config_dict = config.to_dict()
        self.assertEqual(config_dict["weight_dtype"], "fp4_e2m1")
        self.assertEqual(config_dict["compute_dtype"], "fp32")
        self.assertEqual(config_dict["scheme"], "sym")
        self.assertEqual(config_dict["scale_dtype"], "fp32")
        config.to_json_file(f"{self.workspace}/config_post_init_runtime.json")
        print(config)

    def test_int8(self):
        raw_wei = torch.rand(2, 32, dtype=torch.float)
        compress_wei = torch.ops.jblasop.woq_quantize(
            raw_wei, True, 32, "fp32", "int8", "fp32")
        revert_wei = torch.zeros(2, 32, dtype=torch.float)
        torch.ops.jblasop.woq_dequantize(
            compress_wei, revert_wei, True, "fp32", "int8", "fp32")
        for bias in [True, False]:
            model = M(with_bias=bias)
            with torch.no_grad():
                model.linear.weight = torch.nn.Parameter(revert_wei)
            activation = torch.rand(1, 32, dtype=torch.float)
            output = model(activation)

            config = WeightOnlyQuantConfig(weight_dtype="int8", group_size=32)
            convert_to_quantized_model(model, config)
            output_quant = model(activation)
            print(output)
            print(output_quant)
            assert torch.allclose(output, output_quant, rtol=0.01)

    def test_int4(self):
        raw_wei = torch.rand(2, 32, dtype=torch.float)
        compress_wei = torch.ops.jblasop.woq_quantize(
            raw_wei, True, 32, "fp32", "int4_fullrange", "fp32")
        revert_wei = torch.zeros(2, 32, dtype=torch.float)
        torch.ops.jblasop.woq_dequantize(
            compress_wei, revert_wei, True, "fp32", "int4_fullrange", "fp32")
        for bias in [True, False]:
            model = M(with_bias=bias)
            with torch.no_grad():
                model.linear.weight = torch.nn.Parameter(revert_wei)
            activation = torch.rand(1, 5, 32, dtype=torch.float)
            output = model(activation)
            with torch.no_grad():
                model.linear.weight = torch.nn.Parameter(raw_wei)

            config = WeightOnlyQuantConfig(weight_dtype="int4_fullrange", group_size=32)
            convert_to_quantized_model(model, config)
            output_quant = model(activation)
            print(output)
            print(output_quant)
            assert torch.allclose(output, output_quant, rtol=0.01)

    def test_auto_model(self):
        model = AutoModelForCausalLM.from_pretrained(llama_model_path, load_in_4bit=True, use_llm_runtime=False)
        module_list = []
        for name, module in model.named_modules():
            if isinstance(module, QuantizedLinearQBits):
                module_list.append(name)
        self.assertTrue(len(module_list) > 0)

    def test_auto_model_with_config(self):
        config = WeightOnlyQuantConfig()
        model = AutoModelForCausalLM.from_pretrained(
            llama_model_path, quantization_config=config, use_llm_runtime=False
        )
        module_list = []
        for name, module in model.named_modules():
            if isinstance(module, QuantizedLinearQBits):
                module_list.append(name)
        self.assertTrue(len(module_list) > 0)

    def test_nf4_training(self):
        model = AutoModelForCausalLM.from_pretrained(llama_model_path, load_in_4bit=True, use_llm_runtime=False)
        peft_config = LoraConfig(
            r=8,
            lora_alpha=16,
            lora_dropout=0.05,
            target_modules=None,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        lora_weights = {}
        for name, module in model.named_modules():
            if isinstance(module, QuantizedLoraLinearQBits) and "nf4" in module.weight_dtype:
                lora_weights[name] = [
                    getattr(module.lora_A, module.active_adapter[0]).weight.clone(),
                    getattr(module.lora_B, module.active_adapter[0]).weight.clone()
                ]
        self.assertTrue(len(lora_weights) > 0)

        trainer = Trainer(
            model=model,
            train_dataset=DummyDataset(),
            eval_dataset=DummyDataset(),
            args=TrainingArguments(output_dir='tmp', logging_steps=50, num_train_epochs=1000, learning_rate=1e-4)
        )
        trainer.train()
        for name, module in model.named_modules():
            if isinstance(module, QuantizedLoraLinearQBits) and "nf4" in module.weight_dtype:
                self.assertTrue((lora_weights[name][0] != getattr(module.lora_A, module.active_adapter[0]).weight).any())
                self.assertTrue((lora_weights[name][1] != getattr(module.lora_B, module.active_adapter[0]).weight).any())
                module.merge()
                module.unmerge()
        model.merge_and_unload()


    def test_int8_training(self):
        model = AutoModelForCausalLM.from_pretrained(llama_model_path, load_in_8bit=True, use_llm_runtime=False)
        peft_config = LoraConfig(
            r=8,
            lora_alpha=16,
            lora_dropout=0.05,
            target_modules=None,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        model = get_peft_model(model, peft_config)
        model.print_trainable_parameters()
        lora_weights = {}
        for name, module in model.named_modules():
            if isinstance(module, QuantizedLoraLinearQBits) and "int8" in module.weight_dtype:
                lora_weights[name] = [
                    getattr(module.lora_A, module.active_adapter[0]).weight.clone(),
                    getattr(module.lora_B, module.active_adapter[0]).weight.clone()
                ]
        self.assertTrue(len(lora_weights) > 0)

        trainer = Trainer(
            model=model,
            train_dataset=DummyDataset(),
            eval_dataset=DummyDataset(),
            args=TrainingArguments(output_dir='tmp', logging_steps=50, num_train_epochs=1000, learning_rate=1e-4)
        )
        trainer.train()
        for name, module in model.named_modules():
            if isinstance(module, QuantizedLoraLinearQBits) and "int8" in module.weight_dtype:
                self.assertTrue((lora_weights[name][0] != getattr(module.lora_A, module.active_adapter[0]).weight).any())
                self.assertTrue((lora_weights[name][1] != getattr(module.lora_B, module.active_adapter[0]).weight).any())

if __name__ == "__main__":
    unittest.main()
