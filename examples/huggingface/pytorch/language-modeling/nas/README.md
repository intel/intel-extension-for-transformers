Step-by-Stepâ€‹
============

Neural architecture search (NAS) training script [`run_mlm.py`](./run_mlm.py) is based on [`run_mlm.py`](https://github.com/IntelLabs/Model-Compression-Research-Package/blob/main/examples/transformers/language-modeling/run_mlm.py) of Model-Compression-Research-Package by IntelLabs.

# Prerequisiteâ€‹

## 1. Environmentâ€‹
Recommend python 3.9 or higher version.
```shell
pip install -r requirements.txt
pip install transformers==4.34.1
```
>**Note**: Please use transformers no higher than 4.34.1

## 2. Prepare Dataset
Datasets are downloaded and processed using `ðŸ¤— Datasets` package.

# NAS

## 1. Usage
The script `run_mlm.py` can be used for NAS of `ðŸ¤— Transformers` models.

### 1.1 MobileBERT
Search best model architectures based on `google/mobilebert-uncased` on English Wikipedia and BookCorpus datasets using the following command:

``` bash
python run_mlm.py \
    --model_name_or_path google/mobilebert-uncased \
    --datasets_name_config wikipedia:20200501.en bookcorpusopen \
    --do_train --do_eval --nas \
    --data_process_type segment_pair_nsp \
    --max_seq_length 128 \
    --dataset_cache_dir <DATA_CACHE_DIR> \
    --output_dir <OUTPUT_DIR>
```

### 1.2 BERTTiny
Search best model architectures based on `prajjwal1/bert-tiny` on English Wikipedia and BookCorpus datasets using the following command:

``` bash
python run_mlm.py \
    --model_name_or_path prajjwal1/bert-tiny \
    --datasets_name_config wikipedia:20200501.en bookcorpusopen \
    --do_train --do_eval --nas \
    --data_process_type segment_pair_nsp \
    --max_seq_length 128 \
    --dataset_cache_dir <DATA_CACHE_DIR> \
    --output_dir <OUTPUT_DIR>
```
