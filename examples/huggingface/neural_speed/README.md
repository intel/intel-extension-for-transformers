# Step-by-Step

To get better performance of popular large language models (LLM), we recommend using [Neural Speed](https://github.com/intel/neural-speed.git), an innovated library designed to provide the most efficient inference of LLMs. Here, we provide the scripts `run_example.py` for inference, and `runtime_acc.py` for accuracy evaluation. 


# Prerequisiteâ€‹

We recommend install [Neural Speed](https://github.com/intel/neural-speed.git) from source code to fully leverage the latest features.

> Note: To build neural-speed from source code, GCC higher than 10 is required. If you can't upgrade system GCC, here is a solution using conda install.
> ```bash
> compiler_version==13.1
> conda install --update-deps -c conda-forge gxx==${compiler_version} gcc==${compiler_version} gxx_linux-64==${compiler_version} libstdcxx-ng sysroot_linux-64 -y
> ```

For other third-party dependencies, Pytorch and Intel-extension-for-pytorch >= 2.1 are required. Please make sure Pytorch and Intel-extension-for-pytorch are matched with each other.


To running accuracy evaluation, python >=3.9, <= 3.11 is required due to [text evaluation library](https://github.com/EleutherAI/lm-evaluation-harness/tree/master) limitation.


Other third-party dependencies are listed in requirements, please follow the steps below:


```bash
# build neural-speed from source code
git clone https://github.com/intel/neural-speed.git
cd neural-speed
pip install -r requirements.txt
python setup.py install
# come back to current working directory
cd ..
pip install -r requirements.txt
```

# Run


> Note: Please prepare LLMs and save locally before running inference.


## 1. Performance

``` bash
# int4 with group-size=32
OMP_NUM_THREADS=<physical cores num> numactl -m <node N> -C <cpu list> python runtime_example.py \
    --model_path ./Llama2 \
    --prompt "Once upon a time, there existed a little girl," \
    --max_new_tokens 32 \
    --group_size 128
```

## 2. Accuracy

```bash
# int4 with group-size=32
python runtime_acc.py \
    --model_name ./Llama2 \
    --tasks "lambada_openai"
```


> Note: If you are trying models generated by [autoround](../pytorch/text-generation/quantization/), need to disable `model_format` and `use_gptq` these two arguements.
> ```bash
>   # parser.add_argument('--model_format', type=str, default="runtime")
>   # parser.add_argument('--use_gptq', action='store_true')
>   results = evaluate(
>        model="hf-causal",
>        model_args=f'pretrained="{args.model_name}", dtype=float32',
>        tasks=[f"{args.tasks}"]
>   )
> ```