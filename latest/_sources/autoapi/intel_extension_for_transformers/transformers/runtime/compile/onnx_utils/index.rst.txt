intel_extension_for_transformers.transformers.runtime.compile.onnx_utils
========================================================================

.. py:module:: intel_extension_for_transformers.transformers.runtime.compile.onnx_utils

.. autoapi-nested-parse::

   The neural engine onnx utils.



Functions
---------

.. autoapisummary::

   intel_extension_for_transformers.transformers.runtime.compile.onnx_utils.get_children
   intel_extension_for_transformers.transformers.runtime.compile.onnx_utils.get_node_children_names
   intel_extension_for_transformers.transformers.runtime.compile.onnx_utils.get_initializer_children_names
   intel_extension_for_transformers.transformers.runtime.compile.onnx_utils.graph_node_names_details
   intel_extension_for_transformers.transformers.runtime.compile.onnx_utils.is_supported_onnx_graph
   intel_extension_for_transformers.transformers.runtime.compile.onnx_utils.is_supported_onnx_node
   intel_extension_for_transformers.transformers.runtime.compile.onnx_utils.change_num_name
   intel_extension_for_transformers.transformers.runtime.compile.onnx_utils.bias_to_int32
   intel_extension_for_transformers.transformers.runtime.compile.onnx_utils.onnx_extract_operator


Module Contents
---------------

.. py:function:: get_children(node, input_name_to_nodes=None)

   Get the node's output nodes in the graph.


.. py:function:: get_node_children_names(model, node)

   Get the node's output nodes' name in the graph.

   :param model: ONNXModel
   :param node: NodeProto in onnx model

   :returns: names list
   :rtype: outputs


.. py:function:: get_initializer_children_names(model, initializer)

   Get the initializer's output nodes' name in the graph.

   :param model: ONNXModel
   :param initializer: initializer in onnx model

   :returns: names list
   :rtype: outputs


.. py:function:: graph_node_names_details(model)

   Parse the graph nodes ans get the graph_nodes_dict.

   Be used for Graph class with creating a new graph.
   The node_name is the key, node in value is for getting the Const
   tensor value and the input_tensor source op; output_names in value
   is the node output name list; outputs in value is for output_tensor dest op
   :param model: ONNXModel

   :returns: the graph node info dict
   :rtype: node_names_details


.. py:function:: is_supported_onnx_graph(graph)

   Check if the onnx graph supported.


.. py:function:: is_supported_onnx_node(node_name)

   Check if the node type is supported.


.. py:function:: change_num_name(tensor_name)

   For number string.


.. py:function:: bias_to_int32(bias_node, a_scale, b_scale)

   Convert the int8 bias to int32 bias.

   :param bias_node: bias_add in graph (from onnx framework)
   :param a_scale: matmul node input matrice a scale tensor
   :param b_scale: matmul node input matrice b scale tensor
   :param model: Graph class

   :returns: int32 bias numpy array

   fp32_bias = (int8_bias - int8_bias_zero_point) * int8_bias_scale
   int32_bias = fp32_bias / (a_scale * b_scale)


.. py:function:: onnx_extract_operator(node, framework_model, nodes_dict, engine_graph=None)

   Decorate the operator in onnx.

   :param node: NodeProto
   :param framework_model: ONNXModel
   :param nodes_dict: dict, return value from graph_node_names_details
   :param tf_dtypes: dict, for get the dtype string
   :param engine_graph: Engine Graph class

   :returns: node op type
             input_tensors: Tensor list, contains the node input tensors info
             output_tensors: Tensor list, contains the node output tensor info
   :rtype: op_type


