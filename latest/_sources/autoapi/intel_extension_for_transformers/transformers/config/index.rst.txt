intel_extension_for_transformers.transformers.config
====================================================

.. py:module:: intel_extension_for_transformers.transformers.config

.. autoapi-nested-parse::

   Config: provide config classes for optimization processes.



Classes
-------

.. autoapisummary::

   intel_extension_for_transformers.transformers.config.Provider
   intel_extension_for_transformers.transformers.config.DynamicLengthConfig
   intel_extension_for_transformers.transformers.config.BenchmarkConfig
   intel_extension_for_transformers.transformers.config.PrunerV2
   intel_extension_for_transformers.transformers.config.WeightPruningConfig


Functions
---------

.. autoapisummary::

   intel_extension_for_transformers.transformers.config.check_value


Module Contents
---------------

.. py:class:: Provider



   Optimization functionalities provider: INC or NNCF.


.. py:function:: check_value(name, src, supported_type, supported_value=[])

   Check if the given object is the given supported type and in the given supported value.

   Example::

       def datatype(self, datatype):
           if check_value('datatype', datatype, list, ['fp32', 'bf16', 'uint8', 'int8']):
               self._datatype = datatype


.. py:class:: DynamicLengthConfig(max_length: int = None, length_config: str = None, const_rate: float = None, num_sandwich: int = 2, length_drop_ratio_bound: float = 0.2, layer_dropout_prob: float = None, layer_dropout_bound: int = 0, dynamic_training: bool = False, load_store_file: str = None, evo_iter: int = 30, population_size: int = 20, mutation_size: int = 30, mutation_prob: float = 0.5, crossover_size: int = 30, num_cpus: int = 48, distributed_world_size: int = 5, latency_constraint: bool = True, evo_eval_metric='eval_f1')



   Configure the dynamic length config for Quantized Length Adaptive Transformer.

   :param max_length: Limit the maximum length of each layer
   :param length_config: The length number for each layer
   :param const_rate: Length drop ratio
   :param num_sandwich: Sandwich num used in training
   :param length_drop_ratio_bound: Length dropout ratio list
   :param layer_dropout_prob: The layer dropout with probability
   :param layer_dropout_bound: Length dropout ratio
   :param dynamic_training: Whether to use dynamic training
   :param load_store_file: The path for store file
   :param evo_iter: Iterations for evolution search
   :param population_size: Population limitation for evolution search
   :param mutation_size: Mutation limitation for evolution search
   :param mutation_prob: Mutation probability used in evolution search
   :param crossover_size: Crossover limitation for evolution search
   :param num_cpus: The cpu nums used in evolution search
   :param distributed_world_size: Distributed world size in evolution search training
   :param latency_constraint: Latency constraint used in evolution search
   :param evo_eval_metric: The metric name used in evolution search


.. py:class:: BenchmarkConfig(backend: str = 'torch', batch_size: int = 1, warmup: int = 5, iteration: int = 20, cores_per_instance: int = 4, num_of_instance: int = -1, torchscript: bool = False, generate: bool = False, **kwargs)

   Config Class for Benchmark.

   :param backend: the backend used for benchmark. Defaults to "torch".
   :type backend: str, optional
   :param warmup: skip iters when collecting latency. Defaults to 5.
   :type warmup: int, optional
   :param iteration: total iters when collecting latency. Defaults to 20.
   :type iteration: int, optional
   :param cores_per_instance: the core number for 1 instance. Defaults to 4.
   :type cores_per_instance: int, optional
   :param num_of_instance: the instance number. Defaults to -1.
   :type num_of_instance: int, optional
   :param torchscript: Enable it if you want to jit trace it                                       before benchmarking. Defaults to False.
   :type torchscript: bool, optional
   :param generate: Enable it if you want to use model.generate                                    when benchmarking. Defaults to False.
   :type generate: bool, optional


.. py:class:: PrunerV2(target_sparsity=None, pruning_type=None, pattern=None, op_names=None, excluded_op_names=None, start_step=None, end_step=None, pruning_scope=None, pruning_frequency=None, min_sparsity_ratio_per_op=None, max_sparsity_ratio_per_op=None, sparsity_decay_type=None, pruning_op_types=None, reg_type=None, criterion_reduce_type=None, parameters=None, resume_from_pruned_checkpoint=None)

   Similar to torch optimizer's interface.


.. py:class:: WeightPruningConfig(pruning_configs=[{}], target_sparsity=0.9, pruning_type='snip_momentum', pattern='4x1', op_names=[], excluded_op_names=[], start_step=0, end_step=0, pruning_scope='global', pruning_frequency=1, min_sparsity_ratio_per_op=0.0, max_sparsity_ratio_per_op=0.98, sparsity_decay_type='exp', pruning_op_types=['Conv', 'Linear'], **kwargs)

   Similar to torch optimizer's interface.


