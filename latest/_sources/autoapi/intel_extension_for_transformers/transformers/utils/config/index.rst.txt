intel_extension_for_transformers.transformers.utils.config
==========================================================

.. py:module:: intel_extension_for_transformers.transformers.utils.config

.. autoapi-nested-parse::

   Configs for intel extension for transformers.



Classes
-------

.. autoapisummary::

   intel_extension_for_transformers.transformers.utils.config.QuantizationMethod
   intel_extension_for_transformers.transformers.utils.config.ITREXQuantizationConfigMixin
   intel_extension_for_transformers.transformers.utils.config.QuantAwareTrainingConfig
   intel_extension_for_transformers.transformers.utils.config.DynamicQuantConfig
   intel_extension_for_transformers.transformers.utils.config.StaticQuantConfig
   intel_extension_for_transformers.transformers.utils.config.SmoothQuantConfig
   intel_extension_for_transformers.transformers.utils.config.RtnConfig
   intel_extension_for_transformers.transformers.utils.config.GPTQConfig
   intel_extension_for_transformers.transformers.utils.config.AwqConfig
   intel_extension_for_transformers.transformers.utils.config.TeqConfig
   intel_extension_for_transformers.transformers.utils.config.AutoRoundConfig


Module Contents
---------------

.. py:class:: QuantizationMethod



   str(object='') -> str
   str(bytes_or_buffer[, encoding[, errors]]) -> str

   Create a new string object from the given object. If encoding or
   errors is specified, then the object must expose a data buffer
   that will be decoded using the given encoding and error handler.
   Otherwise, returns the result of object.__str__() (if defined)
   or repr(object).
   encoding defaults to sys.getdefaultencoding().
   errors defaults to 'strict'.


.. py:class:: ITREXQuantizationConfigMixin



   Mixin class for quantization config.


   .. py:method:: update(**kwargs)

      Updates attributes of this class instance with attributes from `kwargs` if they match existing atributtes,
      returning all the unused kwargs.

      :param kwargs: Dictionary of attributes to tentatively update this class.
      :type kwargs: `Dict[str, Any]`

      :returns: Dictionary containing all the key-value pairs that were not used to update the instance.
      :rtype: `Dict[str, Any]`



   .. py:method:: post_init_cpu()

      Safety checker that arguments are correct.



   .. py:method:: post_init_xpu()

      Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.



   .. py:method:: post_init_runtime()

      Safety checker that arguments are correct - also replaces some NoneType arguments with their default values.



   .. py:method:: to_json_file(json_file_path: Union[str, os.PathLike], use_diff: bool = True)

      Save this instance to a JSON file.

      :param json_file_path: Path to the JSON file in which this configuration instance's parameters will be saved.
      :type json_file_path: `str` or `os.PathLike`



   .. py:method:: save_pretrained(save_directory: Union[str, os.PathLike], push_to_hub: bool = False, **kwargs)

      Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the
      [`~PretrainedConfig.from_pretrained`] class method.

      :param save_directory: Directory where the configuration JSON file will be saved (will be created if it does not exist).
      :type save_directory: `str` or `os.PathLike`
      :param push_to_hub: Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the
                          repository you want to push to with `repo_id` (will default to the name of `save_directory` in your
                          namespace).
      :type push_to_hub: `bool`, *optional*, defaults to `False`
      :param kwargs: Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.
      :type kwargs: `Dict[str, Any]`, *optional*



.. py:class:: QuantAwareTrainingConfig(backend='default', tokenizer=None, train_dataset='NeelNanda/pile-10k', train_dataloader=None, train_func=None, train_shuffle=True, train_iters=100, train_padding=True, train_batch_size=8, train_len=512, train_pad_val=1, op_name_dict=None, op_type_dict=None, excluded_precisions=[], **kwargs)



   Mixin class for quantization config.


.. py:class:: DynamicQuantConfig(excluded_precisions=[], op_name_dict=None, op_type_dict=None, **kwargs)



   Mixin class for quantization config.


.. py:class:: StaticQuantConfig(backend='default', tokenizer=None, calib_dataset='NeelNanda/pile-10k', calib_dataloader=None, calib_func=None, calib_shuffle=True, calib_iters=100, calib_padding=False, calib_len=512, calib_pad_val=1, op_name_dict=None, op_type_dict=None, excluded_precisions=[], example_inputs=None, **kwargs)



   Mixin class for quantization config.


.. py:class:: SmoothQuantConfig(tokenizer=None, dataset='NeelNanda/pile-10k', alpha=0.5, scale_sharing=False, init_alpha=0.5, alpha_min=0.0, alpha_max=1.0, alpha_step=0.1, shared_criterion='max', do_blockwise=False, auto_alpha_args=None, n_samples=100, seq_len=512, excluded_precisions=[], ipex_opt_llm=None, num_beams=1, shuffle=False, padding=False, **kwargs)



   Mixin class for quantization config.


.. py:class:: RtnConfig(bits: int = 4, group_size: int = 32, group_dim: int = 1, compute_dtype: Any = None, weight_dtype: Any = None, scale_dtype: Any = None, use_full_range: bool = False, mse_range: bool = False, use_double_quant: bool = False, double_quant_dtype: str = 'int', double_quant_bits: int = 8, double_quant_use_sym: bool = False, double_quant_group_size: int = 256, sym: bool = True, layer_wise: bool = False, use_ggml: bool = False, use_quant: bool = True, use_neural_speed: bool = False, **kwargs)



   Mixin class for quantization config.


   .. py:method:: to_diff_dict() -> Dict[str, Any]

      Removes all attributes from config which correspond to the default config attributes
      for better readability and serializes to a Python dictionary.

      :returns: Dictionary of all the attributes that make up this configuration instance,
      :rtype: `Dict[str, Any]`



.. py:class:: GPTQConfig(bits: int = 4, tokenizer: Any = None, dataset: str = 'NeelNanda/pile-10k', batch_size: int = 8, group_size: int = 32, compute_dtype: Any = None, weight_dtype: Any = None, scale_dtype: Any = None, use_double_quant=False, double_quant_scale_dtype=None, sym: bool = True, blocksize: int = 128, damp_percent: float = 0.1, desc_act: bool = False, n_samples: int = 128, seq_len: int = 2048, static_groups: bool = False, use_mse_search: bool = False, true_sequential: bool = False, layer_wise: bool = False, use_ggml: bool = False, use_quant: bool = True, use_neural_speed: bool = False, **kwargs)



   Mixin class for quantization config.


   .. py:method:: post_init_gptq()

      Safety checker that arguments are correct.



   .. py:method:: to_diff_dict() -> Dict[str, Any]

      Removes all attributes from config which correspond to the default config attributes
      for better readability and serializes to a Python dictionary.

      :returns: Dictionary of all the attributes that make up this configuration instance,
      :rtype: `Dict[str, Any]`



.. py:class:: AwqConfig(bits: int = 8, tokenizer: Any = None, dataset: str = 'NeelNanda/pile-10k', group_size: int = 32, compute_dtype: Any = None, weight_dtype: Any = None, scale_dtype: Any = None, layer_wise: bool = False, n_samples: int = 128, seq_len: int = 2048, auto_scale: bool = True, auto_clip: bool = True, use_double_quant=False, double_quant_scale_dtype=None, zero_point: bool = True, use_ggml: bool = False, use_quant: bool = True, use_neural_speed: bool = False, **kwargs)



   Mixin class for quantization config.


   .. py:method:: to_diff_dict() -> Dict[str, Any]

      Removes all attributes from config which correspond to the default config attributes
      for better readability and serializes to a Python dictionary.

      :returns: Dictionary of all the attributes that make up this configuration instance,
      :rtype: `Dict[str, Any]`



.. py:class:: TeqConfig(bits: int = 8, tokenizer: Any = None, dataset: str = 'NeelNanda/pile-10k', group_size: int = 32, compute_dtype: Any = None, weight_dtype: Any = None, scale_dtype: Any = None, layer_wise: bool = False, absorb_to_layer: dict = {}, n_samples: int = 128, seq_len: int = 2048, use_double_quant=False, double_quant_scale_dtype=None, sym: bool = True, use_ggml: bool = False, use_neural_speed: bool = False, **kwargs)



   Mixin class for quantization config.


   .. py:method:: to_diff_dict() -> Dict[str, Any]

      Removes all attributes from config which correspond to the default config attributes
      for better readability and serializes to a Python dictionary.

      :returns: Dictionary of all the attributes that make up this configuration instance,
      :rtype: `Dict[str, Any]`



.. py:class:: AutoRoundConfig(bits: int = 4, tokenizer: Any = None, dataset: str = 'NeelNanda/pile-10k', group_size: int = 128, compute_dtype: Any = None, weight_dtype: Any = None, scale_dtype: Any = None, use_double_quant=False, double_quant_scale_dtype=None, sym: bool = False, lr: float = None, minmax_lr: float = None, disable_quanted_input: bool = True, n_samples: int = 128, seq_len: int = 2048, iters: int = 200, quant_lm_head: bool = False, use_ggml: bool = False, use_neural_speed: bool = False, **kwargs)



   Mixin class for quantization config.


   .. py:method:: to_diff_dict() -> Dict[str, Any]

      Removes all attributes from config which correspond to the default config attributes
      for better readability and serializes to a Python dictionary.

      :returns: Dictionary of all the attributes that make up this configuration instance,
      :rtype: `Dict[str, Any]`



