intel_extension_for_transformers.transformers.trainer
=====================================================

.. py:module:: intel_extension_for_transformers.transformers.trainer

.. autoapi-nested-parse::

   The trainer class for pytorch framework, to easily train or finetune a model.



Classes
-------

.. autoapisummary::

   intel_extension_for_transformers.transformers.trainer.BaseTrainer
   intel_extension_for_transformers.transformers.trainer.NLPTrainer
   intel_extension_for_transformers.transformers.trainer.NLPSeq2SeqTrainer


Module Contents
---------------

.. py:class:: BaseTrainer(*args, **kwargs)

   The base class of trainer.


   .. py:method:: builtin_eval_func(model)

      Custom Evaluate function to inference the model for specified metric on validation dataset.

      :param model: The model to evaluate.

      :returns: evaluation result, the larger is better.
      :rtype: [float]



   .. py:method:: builtin_train_func(model)

      Custom training function to train the model on training dataset.

      :param model: The model to train.

      :returns: evaluation result, the larger is better.
      :rtype: [float]



   .. py:method:: quantize(quant_config: Union[neural_compressor.config.PostTrainingQuantConfig, neural_compressor.config.QuantizationAwareTrainingConfig] = None, provider: str = Provider.INC.value, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None, calib_dataloader=None)

      The main entry point of automatic quantization tuning.

      :param quant_config: QuantizationConfig class containing
                           accuracy goal, quantization objective and related dataloaders etc.
      :param provider: The provider used to quantize.
      :param eval_func: The function used to evaluate the model.
      :type eval_func: :obj:`Callable`, optional
      :param train_func: The function used to train the model.
      :type train_func: :obj:`Callable`, optional
      :param calib_dataloader: The dataloader for calibration dataset.

      :returns: An objective of neural_compressor Quantization class, which can automativally searches for
                optimal quantization recipes for low precision model inference and achieving best tuning
                objectives.



   .. py:method:: prune(pruning_config: Union[neural_compressor.config.WeightPruningConfig] = None, provider: str = Provider.INC.value, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None)

      The main entry point of automatic quantization tuning.

      :param pruning_config: The path to the YAML configuration file or PruningConf class containing
      :param accuracy goal:
      :param pruning objective and related dataloaders etc.:
      :param provider: The provider used to quantize.
      :type provider: str
      :param eval_func: The function used to evaluate the model.
      :type eval_func: :obj:`Callable`, optional
      :param train_func: The function used to train the model.
      :type train_func: :obj:`Callable`, optional

      :returns: An objective of neural_compressor Pruning class.



   .. py:method:: distill(distillation_config: Union[neural_compressor.config.DistillationConfig] = None, provider: str = Provider.INC.value, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None)

      The main entry point of automatic distillation tuning.

      :param quant_config: The path to the YAML configuration file or DistillationConfig class containing
      :param accuracy goal:
      :param distillation objective and related dataloaders etc.:
      :param provider: The provider used to quantize.
      :type provider: str
      :param eval_func (:obj:`Callable`: The function to evaluate the model.
      :param optional: The function to evaluate the model.
      :param train_func (:obj:`Callable`: The function to train the model.
      :param optional: The function to train the model.

      :returns: An objective of neural_compressor Distillation class.



   .. py:method:: orchestrate_optimizations(config_list, eval_func: Optional[Callable] = None, train_func: Optional[Callable] = None)

      Main entry point for orchestrate optimizations.

      :param config_list: The list of configs.
      :param teacher_model: The model(torch.nn.Module) transfers knowledge
                            to a smaller model.
      :type teacher_model: :obj:`Callable`, optional
      :param eval_func: Evaluation function to evaluate the tuning objective.
      :type eval_func: :obj:`Callable`, optional
      :param train_func: Training function which will be combined with pruning.
      :type train_func: :obj:`Callable`, optional



   .. py:method:: train(compression_manager=None, resume_from_checkpoint: Optional[Union[str, bool]] = None, trial: Union[optuna, Dict[str, Any]] = None, ignore_keys_for_eval: Optional[List[str]] = None, **kwargs)

      The main entry point tor train model.

      :param compression_manager: handling the training process.
      :type compression_manager: :obj:`CompressionManager`, `optional`
      :param resume_from_checkpoint: If a :obj:`str`, local path
                                     to a saved checkpoint as saved by a previous instance of :class:`~transformers.Trainer`.
                                     If a :obj:`bool` and equals `True`, load the last checkpoint in `args.output_dir` as saved
                                     by a previous instance of :class:`~transformers.Trainer`. If present, training will resume
                                     from the model/optimizer/scheduler states loaded here.
      :type resume_from_checkpoint: :obj:`str` or :obj:`bool`, `optional`
      :param trial: The trial run or the
                    hyperparameter dictionary for hyperparameter search.
      :type trial: :obj:`optuna.Trial` or :obj:`Dict[str, Any]`, `optional`
      :param ignore_keys_for_eval: A list of keys in the output of your model
                                   (if it is a dictionary) that should be ignored when gathering predictions for evaluation
                                   during the training.
      :type ignore_keys_for_eval: :obj:`List[str]`, `optional`
      :param kwargs: Additional keyword arguments used to hide deprecated arguments



   .. py:method:: training_step(model: torch, inputs: Dict[str, Union[torch, Any]]) -> torch

      Perform a training step on a batch of inputs.

      Subclass and override to inject custom behavior.

      :param model: The model to train.
      :type model: :obj:`nn.Module`
      :param inputs: The inputs and targets of the model.
                     The dictionary will be unpacked before being fed to the model. Most models expect
                     the targets under the argument :obj:`labels`. Check your model's documentation for
                     all accepted arguments.
      :type inputs: :obj:`Dict[str, Union[torch.Tensor, Any]]`

      :returns: The tensor with training loss on this batch.
      :rtype: :obj:`torch.Tensor`



   .. py:method:: training_step_length_adaptive(model: torch, inputs: Dict[str, Union[torch, Any]]) -> torch

      Perform a training step on a batch of inputs.

      Subclass and override to inject custom behavior.

      :param model: The model to train.
      :type model: :obj:`nn.Module`
      :param inputs: The inputs and targets of the model.
                     The dictionary will be unpacked before being fed to the model. Most models expect the targets under the
                     argument :obj:`labels`. Check your model's documentation for all accepted arguments.
      :type inputs: :obj:`Dict[str, Union[torch.Tensor, Any]]`

      :returns: The tensor with training loss on this batch.
      :rtype: :obj:`torch.Tensor`



   .. py:method:: compute_loss(model, inputs, return_outputs=False)

      How the loss is computed by Trainer.

      By default, all models return the loss in the first element.

      Subclass and override for custom behavior.

      :param model: The target model to compute the loss.
      :type model: :obj:`nn.Module`
      :param inputs: The inputs and targets of the model.
      :type inputs: :obj:`Dict[str, Union[torch.Tensor, Any]]`



   .. py:method:: export_to_onnx(*args, **kwargs)

      The function to transfer model into onnx model.

      :param args: defined parameters.
      :param kwargs: additional keyword arguments used to hide deprecated arguments.



   .. py:method:: export_to_fp32_onnx(save_path=None, opset_version=14, do_constant_folding=True, verbose=True)

      The function to transfer model into fp32 onnx model.

      :param save_path: the save path of the exported model.
      :param opset_version: the onnx op version of the exported model.
      :param do_constant_folding: select to do constant folding or not.
      :param verbose: save onnx model.



   .. py:method:: export_to_bf16_onnx(save_path=None, opset_version=14, do_constant_folding=True, verbose=True)

      The function to transfer model into bf16 onnx model.

      :param save_path: the save path of the exported model.
      :param opset_version: the onnx op version of the exported model.
      :param do_constant_folding: select to do constant folding or not.
      :param verbose: save onnx model.



   .. py:method:: export_to_int8_onnx(save_path=None, quant_format='QDQ', dtype='S8S8', opset_version=14, sample_size=100, calibrate_method='minmax', scale_mapping=False)

      The function to transfer model into int8 onnx model.

      :param save_path: the save path of the exported model.
      :param quant_format: quantization format.
      :param dtype: the quantized op type.
      :param opset_version: the onnx op version of the exported model.
      :param sample_size: the sampling size to calibrate the min-max range of ops.
      :param calibrate_method: the calibration method for onnx export.
      :param scale_mapping: make scale mapping of pytorch model and onnx model.



   .. py:method:: export_to_jit()

      The function to transfer model into jit model.



   .. py:method:: get_export_args(model)

      Get input name, output names and axes for export.



   .. py:method:: infer_task(model)

      Infer task.



   .. py:method:: benchmark(model_name_or_path=None, backend: str = 'torch', batch_size: int = 8, cores_per_instance: int = 4, num_of_instance: int = -1, torchscript: bool = False, generate: bool = False, **kwargs)

      Get performance of model.

      :param backend: Defaults to "torch".
      :type backend: str, optional
      :param cores_per_instance: Defaults to 4.
      :type cores_per_instance: int, optional
      :param num_of_instance: Defaults to -1.
      :type num_of_instance: int, optional
      :param torchscript: Defaults to False.
      :type torchscript: bool, optional
      :param generate: Defaults to False.
      :type generate: bool, optional



   .. py:method:: set_dynamic_config(dynamic_config: intel_extension_for_transformers.transformers.DynamicLengthConfig)

      The function to set dynamic config.

      :param dynamic_config: the settings of the dynamic config.



   .. py:method:: run_evolutionary_search()

      Do evolutionary search.



.. py:class:: NLPTrainer(*args, **kwargs)



   Trainer for nlp base on class BaseTrainer and Trainer form Transformers.


.. py:class:: NLPSeq2SeqTrainer(*args, **kwargs)



   Trainer for seq2seq model.


   .. py:method:: builtin_eval_func(model)

      Custom Evaluate function to inference the model for specified metric on validation dataset.

      :param model: The model to evaluate.

      :returns: evaluation result, the larger is better.



