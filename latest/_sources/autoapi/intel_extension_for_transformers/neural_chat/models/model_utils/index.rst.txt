:orphan:

:py:mod:`intel_extension_for_transformers.neural_chat.models.model_utils`
=========================================================================

.. py:module:: intel_extension_for_transformers.neural_chat.models.model_utils


Module Contents
---------------


Functions
~~~~~~~~~

.. autoapisummary::

   intel_extension_for_transformers.neural_chat.models.model_utils.get_repo_root
   intel_extension_for_transformers.neural_chat.models.model_utils.get_checkpoint_files
   intel_extension_for_transformers.neural_chat.models.model_utils.write_checkpoints_json
   intel_extension_for_transformers.neural_chat.models.model_utils.model_on_meta
   intel_extension_for_transformers.neural_chat.models.model_utils.model_is_optimized
   intel_extension_for_transformers.neural_chat.models.model_utils.load_model
   intel_extension_for_transformers.neural_chat.models.model_utils.get_context_length
   intel_extension_for_transformers.neural_chat.models.model_utils.predict_stream
   intel_extension_for_transformers.neural_chat.models.model_utils.predict



.. py:function:: get_repo_root(model_name_or_path, local_rank=-1, token=None)

   Downloads the specified model checkpoint and returns the repository where it was downloaded.


.. py:function:: get_checkpoint_files(model_name_or_path, local_rank, token=None)

   Gets the list of files for the specified model checkpoint.


.. py:function:: write_checkpoints_json(model_name_or_path, local_rank, checkpoints_json, token=None)

   Dumps metadata into a JSON file for DeepSpeed-inference.


.. py:function:: model_on_meta(config)

   Checks if load the model to meta.


.. py:function:: model_is_optimized(config)

   Checks if the given config belongs to a model in optimum/habana/transformers/models, which has a
   new input token_idx.


.. py:function:: load_model(model_name, tokenizer_name, device='cpu', use_hpu_graphs=False, cpu_jit=False, ipex_int8=False, use_cache=True, peft_path=None, use_deepspeed=False, optimization_config=None, hf_access_token=None, use_llm_runtime=False, assistant_model=None)

   Load the model and initialize the tokenizer.

   :param model_name: The name of the model.
   :type model_name: str
   :param device: The device for the model. Defaults to 'cpu'. The valid value is 'cpu', 'cuda' or 'hpu'.
   :type device: str, optional
   :param use_hpu_graphs: Whether to use HPU graphs. Defaults to False. Only set when device is hpu.
   :type use_hpu_graphs: bool, optional
   :param assistant_model: The assistant model name. Defaults to None.
   :type assistant_model: str, optional

   :returns: None

   :raises ValueError:


.. py:function:: get_context_length(config)

   Get the context length of a model from a huggingface model config.


.. py:function:: predict_stream(**params)

   Generates streaming text based on the given parameters and prompt.

   :param params: A dictionary containing the parameters for text generation.
   :type params: dict
   :param `device`: Specifies the device type for text generation. It can be either "cpu", "cuda" or "hpu".
   :type `device`: string
   :param `prompt`: Represents the initial input or context provided to the text generation model.
   :type `prompt`: string
   :param `temperature`: Controls the randomness of the generated text.
                         Higher values result in more diverse outputs.
   :type `temperature`: float
   :param `top_p`: Specifies the cumulative probability threshold for using in the top-p sampling strategy.
                   Smaller values make the output more focused.
   :type `top_p`: float
   :param `top_k`: Specifies the number of highest probability tokens to consider in the top-k sampling strategy.
   :type `top_k`: int
   :param `repetition_penalty`: Controls the penalty applied to repeated tokens in the generated text.
                                Higher values discourage repetition.
   :type `repetition_penalty`: float
   :param `max_new_tokens`: Limits the maximum number of tokens to be generated.
   :type `max_new_tokens`: int
   :param `do_sample`: Determines whether to use sampling-based text generation.
                       If set to True, the output will be sampled; otherwise,
                       it will be determined by the model's top-k or top-p strategy.
   :type `do_sample`: bool
   :param `num_beams`: Controls the number of beams used in beam search.
                       Higher values increase the diversity but also the computation time.
   :type `num_beams`: int
   :param `model_name`: Specifies the name of the pre-trained model to use for text generation.
                        If not provided, the default model is "Intel/neural-chat-7b-v3-1".
   :type `model_name`: string
   :param `num_return_sequences`: Specifies the number of alternative sequences to generate.
   :type `num_return_sequences`: int
   :param `bad_words_ids`: Contains a list of token IDs that should not appear in the generated text.
   :type `bad_words_ids`: list or None
   :param `force_words_ids`: Contains a list of token IDs that must be included in the generated text.
   :type `force_words_ids`: list or None
   :param `use_hpu_graphs`: Determines whether to utilize Habana Processing Units (HPUs) for accelerated generation.
   :type `use_hpu_graphs`: bool
   :param `use_cache`: Determines whether to utilize kv cache for accelerated generation.
   :type `use_cache`: bool
   :param `ipex_int8`: Whether to use IPEX int8 model to inference.
   :type `ipex_int8`: bool
   :param `format_version`: the format version of return stats.
   :type `format_version`: string

   :returns: A generator that yields the generated streaming text.
   :rtype: generator


.. py:function:: predict(**params)

   Generates streaming text based on the given parameters and prompt.

   :param params: A dictionary containing the parameters for text generation.
   :type params: dict
   :param `device`: Specifies the device type for text generation. It can be either "cpu", "cuda" or "hpu".
   :type `device`: string
   :param `prompt`: Represents the initial input or context provided to the text generation model.
   :type `prompt`: string
   :param `temperature`: Controls the randomness of the generated text.
                         Higher values result in more diverse outputs.
   :type `temperature`: float
   :param `top_p`: Specifies the cumulative probability threshold for using in the top-p sampling strategy.
                   Smaller values make the output more focused.
   :type `top_p`: float
   :param `top_k`: Specifies the number of highest probability tokens to consider in the top-k sampling strategy.
   :type `top_k`: int
   :param `repetition_penalty`: Controls the penalty applied to repeated tokens in the generated text.
                                Higher values discourage repetition.
   :type `repetition_penalty`: float
   :param `max_new_tokens`: Limits the maximum number of tokens to be generated.
   :type `max_new_tokens`: int
   :param `do_sample`: Determines whether to use sampling-based text generation.
                       If set to True, the output will be sampled; otherwise,
                       it will be determined by the model's top-k or top-p strategy.
   :type `do_sample`: bool
   :param `num_beams`: Controls the number of beams used in beam search.
                       Higher values increase the diversity but also the computation time.
   :type `num_beams`: int
   :param `model_name`: Specifies the name of the pre-trained model to use for text generation.
                        If not provided, the default model is "mosaicml/mpt-7b-chat".
   :type `model_name`: string
   :param `num_return_sequences`: Specifies the number of alternative sequences to generate.
   :type `num_return_sequences`: int
   :param `bad_words_ids`: Contains a list of token IDs that should not appear in the generated text.
   :type `bad_words_ids`: list or None
   :param `force_words_ids`: Contains a list of token IDs that must be included in the generated text.
   :type `force_words_ids`: list or None
   :param `use_hpu_graphs`: Determines whether to utilize Habana Processing Units (HPUs) for accelerated generation.
   :type `use_hpu_graphs`: bool
   :param `use_cache`: Determines whether to utilize kv cache for accelerated generation.
   :type `use_cache`: bool
   :param `ipex_int8`: Whether to use IPEX int8 model to inference.
   :type `ipex_int8`: bool

   :returns: A generator that yields the generated streaming text.
   :rtype: generator


