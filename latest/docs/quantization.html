<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Quantization &mdash; Intel® Extension for Transformers 0.1.dev1+gf8bf64b documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Distillation" href="distillation.html" />
    <link rel="prev" title="Features" href="../feature.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../user_guide.html">User Guide</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../feature.html">Features</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quantization-fundamentals">Quantization Fundamentals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#accuracy-aware-tuning">Accuracy Aware Tuning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#supported-feature-matrix">Supported Feature Matrix</a></li>
<li class="toctree-l4"><a class="reference internal" href="#get-started">Get Started</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="distillation.html">Distillation</a></li>
<li class="toctree-l3"><a class="reference internal" href="pruning.html">Pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="autodistillation.html">AutoDistillation Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="data_augmentation.html">Data Augmentation</a></li>
<li class="toctree-l3"><a class="reference internal" href="benchmark.html">Benchmark</a></li>
<li class="toctree-l3"><a class="reference internal" href="export.html">Export to ONNX</a></li>
<li class="toctree-l3"><a class="reference internal" href="metrics.html">Metrics</a></li>
<li class="toctree-l3"><a class="reference internal" href="objectives.html">Objective</a></li>
<li class="toctree-l3"><a class="reference internal" href="pipeline.html">Pipeline</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../neural_engine.html">Neural Engine</a></li>
<li class="toctree-l2"><a class="reference internal" href="../kernel.html">Kernels</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../user_guide.html">User Guide</a></li>
          <li class="breadcrumb-item"><a href="../feature.html">Features</a></li>
      <li class="breadcrumb-item active">Quantization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/docs/quantization.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="quantization">
<h1>Quantization<a class="headerlink" href="#quantization" title="Link to this heading"></a></h1>
<ol class="simple">
<li><p><a class="reference external" href="#introduction">Introduction</a></p></li>
<li><p><a class="reference external" href="#quantization-fundamentals">Quantization Fundamentals</a></p></li>
<li><p><a class="reference external" href="#accuracy-aware-tuning">Accuracy Aware Tuning</a></p></li>
<li><p><a class="reference external" href="#supported-feature-matrix">Supported Feature Matrix</a></p></li>
<li><p><a class="reference external" href="#get-started">Get Started</a></p></li>
</ol>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>Quantization is a widely-used model compression technique that can reduce model size while also improving inference latency. The full precision data converts to low-precision, there is little degradation in model accuracy, but the inference performance of quantized model can gain higher performance by saving the memory bandwidth and accelerating computations with low precision instructions. Intel provided several lower precision instructions (ex: 8-bit or 16-bit multipliers), inference can get benefits from them. Refer to the Intel article on <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/lower-numerical-precision-deep-learning-inference-and-training.html">Lower Numerical Precision Deep Learning Inference and Training</a>.</p>
</section>
<section id="quantization-fundamentals">
<h2>Quantization Fundamentals<a class="headerlink" href="#quantization-fundamentals" title="Link to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Affine</span> <span class="pre">quantization</span></code> and <code class="docutils literal notranslate"><span class="pre">Scale</span> <span class="pre">quantization</span></code> are two common range mapping techniques used in tensor conversion between different data types.</p>
<p>The math equation is like: $X_{int8} = round(Scale \times X_{fp32} + ZeroPoint)$.</p>
<p><strong>Affine Quantization</strong></p>
<p>This is so-called <code class="docutils literal notranslate"><span class="pre">asymmetric</span> <span class="pre">quantization</span></code>, in which we map the min/max range in the float tensor to the integer range. Here int8 range is [-128, 127], uint8 range is [0, 255].</p>
<p>here:</p>
<p>If INT8 is specified, $Scale = (|X_{f_{max}} - X_{f_{min}}|) / 127$ and $ZeroPoint = -128 - X_{f_{min}} / Scale$.</p>
<p>or</p>
<p>If UINT8 is specified, $Scale = (|X_{f_{max}} - X_{f_{min}}|) / 255$ and $ZeroPoint = - X_{f_{min}} / Scale$.</p>
<p><strong>Scale Quantization</strong></p>
<p>This is so-called <code class="docutils literal notranslate"><span class="pre">Symmetric</span> <span class="pre">quantization</span></code>, in which we use the maximum absolute value in the float tensor as float range and map to the corresponding integer range.</p>
<p>The math equation is like:</p>
<p>here:</p>
<p>If INT8 is specified, $Scale = max(abs(X_{f_{max}}), abs(X_{f_{min}})) / 127$ and $ZeroPoint = 0$.</p>
<p>or</p>
<p>If UINT8 is specified, $Scale = max(abs(X_{f_{max}}), abs(X_{f_{min}})) / 255$ and $ZeroPoint = 128$.</p>
<p><em>NOTE</em></p>
<p>Sometimes the reduce_range feature, that’s using 7 bit width (1 sign bit + 6 data bits) to represent int8 range, may be needed on some early Xeon platforms, it’s because those platforms may have overflow issues due to fp16 intermediate calculation result when executing int8 dot product operation. After AVX512_VNNI instruction is introduced, this issue gets solved by supporting fp32 intermediate data.</p>
<section id="quantization-approach">
<h3>Quantization Approach<a class="headerlink" href="#quantization-approach" title="Link to this heading"></a></h3>
<p>Quantization has three different approaches:</p>
<ol class="simple">
<li><p>post training dynamic quantization</p></li>
<li><p>post training static quantization</p></li>
<li><p>quantization aware training.</p></li>
</ol>
<p>The first two approaches belong to optimization on inference. The last belongs to optimization during training.</p>
<section id="post-training-dynamic-quantization">
<h4>Post Training Dynamic Quantization<a class="headerlink" href="#post-training-dynamic-quantization" title="Link to this heading"></a></h4>
<p>The weights of the neural network get quantized into int8 format from float32 format offline. The activations of the neural network is quantized as well with the min/max range collected during inference runtime.</p>
<p>This approach is widely used in dynamic length neural networks, like NLP model.</p>
<img src="imgs/dynamic_quantization.png" width=270 height=124 alt="Dynamic Quantization">
<br></section>
<section id="post-training-static-quantization">
<h4>Post Training Static Quantization<a class="headerlink" href="#post-training-static-quantization" title="Link to this heading"></a></h4>
<p>Compared with <code class="docutils literal notranslate"><span class="pre">post</span> <span class="pre">training</span> <span class="pre">dynamic</span> <span class="pre">quantization</span></code>, the min/max range in weights and activations are collected offline on a so-called <code class="docutils literal notranslate"><span class="pre">calibration</span></code> dataset. This dataset should be able to represent the data distribution of those unseen inference dataset. The <code class="docutils literal notranslate"><span class="pre">calibration</span></code> process runs on the original fp32 model and dumps out all the tensor distributions for <code class="docutils literal notranslate"><span class="pre">Scale</span></code> and <code class="docutils literal notranslate"><span class="pre">ZeroPoint</span></code> calculations. Usually preparing 100 samples are enough for calibration.</p>
<p>This approach is major quantization approach people should try because it could provide the better performance comparing with <code class="docutils literal notranslate"><span class="pre">post</span> <span class="pre">training</span> <span class="pre">dynamic</span> <span class="pre">quantization</span></code>.</p>
<img src="imgs/PTQ.png" width=256 height=129 alt="PTQ">
<br></section>
<section id="quantization-aware-training">
<h4>Quantization Aware Training<a class="headerlink" href="#quantization-aware-training" title="Link to this heading"></a></h4>
<p>Quantization aware training emulates inference-time quantization in the forward pass of the training process by inserting <code class="docutils literal notranslate"><span class="pre">fake</span> <span class="pre">quant</span></code> ops before those quantizable ops. With <code class="docutils literal notranslate"><span class="pre">quantization</span> <span class="pre">aware</span> <span class="pre">training</span></code>, all weights and activations are <code class="docutils literal notranslate"><span class="pre">fake</span> <span class="pre">quantized</span></code> during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while aware of the fact that the model will ultimately be quantized; after quantizing, therefore, this method will usually yield higher accuracy than either dynamic quantization or post-training static quantization.</p>
<img src="imgs/QAT.png" width=244 height=147 alt="QAT"></section>
</section>
</section>
<section id="accuracy-aware-tuning">
<h2>Accuracy Aware Tuning<a class="headerlink" href="#accuracy-aware-tuning" title="Link to this heading"></a></h2>
<p>This feature can be used to solve accuracy loss pain points brought by applying low precision quantization and other lossy optimization methods.</p>
<p>This tuning algorithm is provided by <a class="reference external" href="https://github.com/intel/neural-compressor/blob/master/docs/source/quantization.html">Intel(R) Neural Compressor</a>.</p>
</section>
<section id="supported-feature-matrix">
<h2>Supported Feature Matrix<a class="headerlink" href="#supported-feature-matrix" title="Link to this heading"></a></h2>
<p>Quantization methods include the following three types:</p>
<table class="center">
    <thead>
        <tr>
            <th>Types</th>
            <th>Quantization</th>
            <th>Dataset Requirements</th>
            <th>Framework</th>
            <th>Backend</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td rowspan="2" align="center">Post-Training Static Quantization</td>
            <td rowspan="2" align="center">weights and activations</td>
            <td rowspan="2" align="center">calibration</td>
            <td align="center">PyTorch</td>
            <td align="center"><a href="https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization">PyTorch FX</a>/<a href="https://github.com/intel/intel-extension-for-pytorch">IPEX</a></td>
        </tr>
        <tr>
            <td align="center">TensorFlow</td>
            <td align="center"><a href="https://github.com/tensorflow/tensorflow">TensorFlow</a>/<a href="https://github.com/Intel-tensorflow/tensorflow">Intel TensorFlow</a></td>
        </tr>
        <tr>
            <td rowspan="1" align="center">Post-Training Dynamic Quantization</td>
            <td rowspan="1" align="center">weights</td>
            <td rowspan="1" align="center">none</td>
            <td align="center">PyTorch</td>
            <td align="center"><a href="https://pytorch.org/docs/stable/quantization.html#eager-mode-quantization">PyTorch eager mode</a></td>
        </tr>
        <tr>
            <td rowspan="2" align="center">Quantization-aware Training (QAT)</td>
            <td rowspan="2" align="center">weights and activations</td>
            <td rowspan="2" align="center">fine-tuning</td>
            <td align="center">PyTorch</td>
            <td align="center"><a href="https://pytorch.org/docs/stable/quantization.html#prototype-fx-graph-mode-quantization">PyTorch fx mode</a></td>
        </tr>
        <tr>
            <td align="center">TensorFlow</td>
            <td align="center"><a href="https://github.com/tensorflow/tensorflow">TensorFlow</a>/<a href="https://github.com/Intel-tensorflow/tensorflow">Intel TensorFlow</a></td>
        </tr>
    </tbody>
</table>
<br>
<br></section>
<section id="get-started">
<h2>Get Started<a class="headerlink" href="#get-started" title="Link to this heading"></a></h2>
<section id="script">
<h3>Script:<a class="headerlink" href="#script" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">intel_extension_for_transformers.transformers</span> <span class="kn">import</span> <span class="n">metric</span><span class="p">,</span> <span class="n">objectives</span><span class="p">,</span> <span class="n">QuantizationConfig</span>
<span class="kn">from</span> <span class="nn">intel_extension_for_transformers.transformers.trainer</span> <span class="kn">import</span> <span class="n">NLPTrainer</span>
<span class="c1"># Replace transformers.Trainer with NLPTrainer</span>
<span class="c1"># trainer = transformers.Trainer(......)</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">NLPTrainer</span><span class="p">(</span><span class="o">......</span><span class="p">)</span>
<span class="n">metric</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;eval_f1&quot;</span><span class="p">,</span> <span class="n">is_relative</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="mf">0.01</span>
<span class="p">)</span>
<span class="n">objective</span> <span class="o">=</span> <span class="n">objectives</span><span class="o">.</span><span class="n">performance</span>
<span class="n">q_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span>
    <span class="n">approach</span><span class="o">=</span><span class="s2">&quot;PostTrainingStatic&quot;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">metric</span><span class="p">],</span>
    <span class="n">objectives</span><span class="o">=</span><span class="p">[</span><span class="n">objective</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">quant_config</span><span class="o">=</span><span class="n">q_config</span><span class="p">)</span>
</pre></div>
</div>
<p>Please refer to <a class="reference external" href="../examples/huggingface/pytorch/text-classification/quantization/run_glue.py">quantization example</a> for the details.</p>
</section>
<section id="create-an-instance-of-metric">
<h3>Create an Instance of Metric<a class="headerlink" href="#create-an-instance-of-metric" title="Link to this heading"></a></h3>
<p>The Metric defines which metric will be used to measure the performance of tuned models.</p>
<ul>
<li><p>example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metric</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;eval_f1&quot;</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_relative</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">criterion</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">weight_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Please refer to <a class="reference external" href="metrics.html">metrics document</a> for the details.</p>
</li>
</ul>
</section>
<section id="create-an-instance-of-objective-optional">
<h3>Create an Instance of Objective(Optional)<a class="headerlink" href="#create-an-instance-of-objective-optional" title="Link to this heading"></a></h3>
<p>In terms of evaluating the status of a specific model during tuning, we should have general objectives to measure the status of different models.</p>
<ul>
<li><p>example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">objective</span> <span class="o">=</span> <span class="n">objectives</span><span class="o">.</span><span class="n">Objective</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;performance&quot;</span><span class="p">,</span> <span class="n">greater_is_better</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_ratio</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Please refer to <a class="reference external" href="objectives.html">objective document</a> for the details.</p>
</li>
</ul>
</section>
<section id="create-an-instance-of-quantizationconfig">
<h3>Create an Instance of QuantizationConfig<a class="headerlink" href="#create-an-instance-of-quantizationconfig" title="Link to this heading"></a></h3>
<p>The QuantizationConfig contains all the information related to the model quantization behavior. If you have created Metric and Objective instance(default Objective is “performance”), then you can create an instance of QuantizationConfig.</p>
<ul class="simple">
<li><p>arguments:</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: left;">Argument</th>
<th style="text-align: left;">Type</th>
<th style="text-align: left;">Description</th>
<th style="text-align: left;">Default value</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">framework</td>
<td style="text-align: left;">string</td>
<td style="text-align: left;">Which framework you used</td>
<td style="text-align: left;">"pytorch"</td>
</tr>
<tr>
<td style="text-align: left;">approach</td>
<td style="text-align: left;">string</td>
<td style="text-align: left;">Which quantization approach you used</td>
<td style="text-align: left;">"PostTrainingStatic"</td>
</tr>
<tr>
<td style="text-align: left;">timeout</td>
<td style="text-align: left;">integer</td>
<td style="text-align: left;">Tuning timeout(seconds), 0 means early stop; combine with max_trials field to decide when to exit</td>
<td style="text-align: left;">0</td>
</tr>
<tr>
<td style="text-align: left;">max_trials</td>
<td style="text-align: left;">integer</td>
<td style="text-align: left;">Max tune times</td>
<td style="text-align: left;">100</td>
</tr>
<tr>
<td style="text-align: left;">metrics</td>
<td style="text-align: left;">list of Metric</td>
<td style="text-align: left;">Used to evaluate accuracy of tuning model, no need for NoTrainerOptimizer</td>
<td style="text-align: left;">None</td>
</tr>
<tr>
<td style="text-align: left;">objectives</td>
<td style="text-align: left;">list of Objective</td>
<td style="text-align: left;">Objective with accuracy constraint guaranteed</td>
<td style="text-align: left;">performance</td>
</tr>
</tbody>
</table><ul>
<li><p>example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">q_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="p">(</span>
    <span class="n">approach</span><span class="o">=</span><span class="s2">&quot;PostTrainingDynamic&quot;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">metric</span><span class="p">],</span>
    <span class="n">objectives</span><span class="o">=</span><span class="p">[</span><span class="n">objective</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="quantization-with-trainer">
<h3>Quantization with Trainer<a class="headerlink" href="#quantization-with-trainer" title="Link to this heading"></a></h3>
<ul>
<li><p>Quantization with Trainer
NLPTrainer inherits from transformers.Trainer, so you can create trainer like you do in transformers examples. Then you can quantize model with trainer.quantize function.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">quant_config</span><span class="o">=</span><span class="n">q_config</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../feature.html" class="btn btn-neutral float-left" title="Features" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="distillation.html" class="btn btn-neutral float-right" title="Distillation" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f74931079a0> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>