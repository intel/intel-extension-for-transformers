<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transposed MHA &mdash; Intel® Extension for Transformers 0.1.dev1+g80a779b documentation</title>
      <link rel="stylesheet" href="../../../../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/graphviz.css" type="text/css" />
      <link rel="stylesheet" href="../../../../../../../_static/custom.css" type="text/css" />
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Transposed MHA</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../_sources/docs/intel_extension_for_transformers/llm/library/kernels/docs/kernel_desc/kernel_transpose_mha.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="transposed-mha">
<h1>Transposed MHA<a class="headerlink" href="#transposed-mha" title="Link to this heading"></a></h1>
<section id="problem-description">
<h2>Problem description<a class="headerlink" href="#problem-description" title="Link to this heading"></a></h2>
<p>In our sparse GEMM, the weight, activation and result matrices are all transposed. See this <a class="reference external" href="https://github.com/intel/intel-extension-for-transformers/blob/main/intel_extension_for_transformers/backends/neural_engine/kernels/docs/kernel_desc/kernel_vnni.html">documentation</a> for details.
In transformer architecture, MHA’s computing logic is $softmax(Q\times K^T)\times V$, while $Q^T,K^T,V^T$ matrices are generated after calling sparse-matmul. The inference engine may need to transpose the $Q^T$ and $V^T$ matrices twice before performing the MHA operation. It is worth noting that since many models are stacked transformer structures, after calculating the MHA, the result also needs to be transposed for the next sparse-matmul calculation. It is clear that we have introduced multiple reorder operations, which will significantly affect performance. In fact, we can completely execute MHA with transposed-memory-layout. That is $dst^T=V^T\times softmax(K\times Q^T,axis =1)$. With the fully transposed mode, we greatly reduce the transpose times, and only need to transpose $K$ once. Note that we will execute softmax with col-major at this point.</p>
</section>
<section id="details">
<h2>Details<a class="headerlink" href="#details" title="Link to this heading"></a></h2>
<p>At present, we provide two implementations of transpose_MHA, one is based on AMX ISA introduced by SPR and the other based on VNNI ISA(VNNI_W and VNNI_B, the latter will bring better performance) introduced by 2nd Xeon CLX. Please notice that because the problem-size is limited in bert model, so the MHA’s implementation is not a general method. The whole kernel is composed of preprocessing and three computing steps among them.</p>
<blockquote>
<div><p>We only support HEAD_SIZE = 32 /64 now.</p>
</div></blockquote>
<section id="reordering">
<h3>Reordering<a class="headerlink" href="#reordering" title="Link to this heading"></a></h3>
<ol class="simple">
<li><p>Reorder matrix Q/K to VNNI format for AMX/VNNI instruction. Refer to the <a class="reference external" href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#ig_expand=159,653,1854,5965,5461,1742,2782,2782,2774,4964,4916,4925,4944,1745,1589,4114,4089,4120,4114,4111,4108,5390,5389,5388,1595,2782,4541,6243,6255,4541,1510,2244,6836,7263,2250,5360,5388,5389,5390,5352,5353,5324,5381,4626,7264,607,6172,649,6159,6897,7264,7267,2808,2795,2782,2913,1431,1385,5381,574,572,5381,5380,5379,5378,5377,5376,5375,5374,5373,5390,5389,5388,5360,5359,6897,6937,6905,6830,6897,6903,6857,7265,6501,6519,6582,488,4506,5240,5459,5478,5489,5488,5487,5480,5463,640,6206,640,2808,4469,4470,4471,4463,4472,4469,2801,2759,2782,2787,640,2949,2949,2890,2913,2912,2585,2585,2585,2585,7041,7043,572,2579,640,4369,6903,6903,2756,1827,6529,572,7264,2749,5645,2808,2795,2782,7260,4469,7267,7266,2139,1998,2756,2750,2753,7439,7259,7259,2769,7439,4376,6172,6255,6252,7498,7400,7343,6410,6951,5289,4087,4123,4111,4107,2250,6951,6947,6944,6955,4374,4107,707,7544,184,4503,119,7539,7540&amp;techs=AMX">AMX</a>/<a class="reference external" href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#ig_expand=159,653,1854,5965,5461,1742,2782,2782,2774,4964,4916,4925,4944,1745,1589,4114,4089,4120,4114,4111,4108,5390,5389,5388,1595,2782,4541,6243,6255,4541,1510,2244,6836,7263,2250,5360,5388,5389,5390,5352,5353,5324,5381,4626,7264,607,6172,649,6159,6897,7264,7267,2808,2795,2782,2913,1431,1385,5381,574,572,5381,5380,5379,5378,5377,5376,5375,5374,5373,5390,5389,5388,5360,5359,6897,6937,6905,6830,6897,6903,6857,7265,6501,6519,6582,488,4506,5240,5459,5478,5489,5488,5487,5480,5463,640,6206,640,2808,4469,4470,4471,4463,4472,4469,2801,2759,2782,2787,640,2949,2949,2890,2913,2912,2585,2585,2585,2585,7041,7043,572,2579,640,4369,6903,6903,2756,1827,6529,572,7264,2749,5645,2808,2795,2782,7260,4469,7267,7266,2139,1998,2756,2750,2753,7439,7259,7259,2769,7439,4376,6172,6255,6252,7498,7400,7343,6410,6951,5289,4087,4123,4111,4107,2250,6951,6947,6944,6955,4374,4107,707,7544,184,4503,119,7539,7540,2899&amp;avx512techs=AVX512_VNNI">VNNI</a> documentation for details of VNNI format. In general, we reorder 4x16 tiles in the form shown below. <img alt="trans_mha_reorder" src="../../../../../../../_images/kernel_trans_mha_reorder.png" />In order to reduce the number of kernel launchings, kernel will calculate the appropriate <code class="docutils literal notranslate"><span class="pre">batchK</span></code> value before execution. The kernel responsible for preprocessing will transpose and reorder <code class="docutils literal notranslate"><span class="pre">batchK</span></code> sub-matrix at one time, so as to avoid the related overhead. <img alt="batchk_trans" src="../../../../../../../_images/kernel_trans_mha_batchk_trans.png" /></p></li>
</ol>
</section>
<section id="calculation">
<h3>Calculation<a class="headerlink" href="#calculation" title="Link to this heading"></a></h3>
<p>In calculating step1, jit kernel takes a sub-matrix of MxN in each iteration. The values of M and N are from prior experience. In each iteraion, the kernels perform the following computational logic:</p>
<ol class="simple">
<li><p>Calculate the block and execute binary_add(add mask).</p></li>
<li><p>For each value <code class="docutils literal notranslate"><span class="pre">x</span></code> in the block, use a low-order polynomial approximation to calculate <code class="docutils literal notranslate"><span class="pre">exp(x)</span></code> and col-major to calculate <code class="docutils literal notranslate"><span class="pre">sum(exp(x))</span></code>.</p></li>
<li><p>Convert the value of <code class="docutils literal notranslate"><span class="pre">exp(x)</span></code> from <code class="docutils literal notranslate"><span class="pre">fp32</span></code> to <code class="docutils literal notranslate"><span class="pre">bf16</span></code> to write back to tmp-buffer to reduce the bandwidth overhead when writing back, and take the inverse of <code class="docutils literal notranslate"><span class="pre">sum(exp(x))</span></code> divide by <code class="docutils literal notranslate"><span class="pre">scale</span></code>(1/255) to write back to tmp-buffer. These are essentially the first half of computing softmax.</p></li>
</ol>
<p>In the calculation step2, the following part of softmax is completed, and <code class="docutils literal notranslate"><span class="pre">sum(exp(x))</span></code> is used as the normalization of <code class="docutils literal notranslate"><span class="pre">exp(x)</span></code>, and the result is quantized into <code class="docutils literal notranslate"><span class="pre">u8</span></code> with the incoming scale, that is, the final result of $softmax(K\times Q^T,axis=1)$ is obtained. Then, to facilitate the subsequent multiplication with $V^T$, we also reorder this result.</p>
<p>Calculation step3 is a simple matmul, that is $V^T\times softmax(K\times Q^T,axis =1)$</p>
<p>So far, we have completed the calculation of transpose_MHA.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7effb65e7070> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>