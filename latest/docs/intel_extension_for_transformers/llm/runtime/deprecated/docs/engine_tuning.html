<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Engine Tuning &mdash; Intel® Extension for Transformers 0.1.dev1+g5e607e6 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/graphviz.css?v=eafc0fe6" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" />
    <link rel="next" title="Graph Fusion" href="graph_fusion.html" />
    <link rel="prev" title="Profiling" href="engine_profiling.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../../../../../user_guide.html">User Guide</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../../../../../feature.html">Features</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../../../../../../neural_engine.html">Neural Engine</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="deploy_and_integration.html">Deploy and Integration</a></li>
<li class="toctree-l3"><a class="reference internal" href="onnx_compile.html">Compile an ONNX model to Engine IR</a></li>
<li class="toctree-l3"><a class="reference internal" href="onnx_quantize.html">Quantize a ONNX model to engine low precision/int8 IR</a></li>
<li class="toctree-l3"><a class="reference internal" href="engine_profiling.html">Profiling</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Engine Tuning</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pattern-tuning-for-dispatching-best-pattern">Pattern Tuning for Dispatching Best Pattern</a></li>
<li class="toctree-l4"><a class="reference internal" href="#graph-tuning-for-dispatching-best-graph">Graph Tuning for Dispatching Best Graph</a></li>
<li class="toctree-l4"><a class="reference internal" href="#op-tuning-for-dispatching-best-kernel-and-related-runtime-config">OP Tuning for Dispatching Best Kernel and Related Runtime Config</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="graph_fusion.html">Graph Fusion</a></li>
<li class="toctree-l3"><a class="reference internal" href="pattern_recognize.html">Pattern Recognize</a></li>
<li class="toctree-l3"><a class="reference internal" href="operator_register.html">Customized Operators Register</a></li>
<li class="toctree-l3"><a class="reference internal" href="add_customized_pattern.html">Add Customized Pattern</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../kernel.html">Kernels</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../../../../../user_guide.html">User Guide</a></li>
          <li class="breadcrumb-item"><a href="../../../../../../neural_engine.html">Neural Engine</a></li>
      <li class="breadcrumb-item active">Engine Tuning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/docs/intel_extension_for_transformers/llm/runtime/deprecated/docs/engine_tuning.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="engine-tuning">
<h1>Engine Tuning<a class="headerlink" href="#engine-tuning" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p><a class="reference external" href="#introduction">Introduction</a></p></li>
<li><p><a class="reference external" href="#pattern-tuning-for-dispatching-best-pattern">Pattern Tuning for Dispatching Best Pattern</a></p></li>
<li><p><a class="reference external" href="#graph-tuning-for-dispatching-best-graph">Graph Tuning for Dispatching Best Graph</a></p></li>
<li><p><a class="reference external" href="#op-tuning-for-dispatching-best-kernel-and-related-runtime-config">OP Tuning for Dispatching Best Kernel and Related Runtime Config</a></p>
<ul>
<li><p><a class="reference external" href="#how-to-turn-on-op-tuning-mechanism">How to Turn on Op Tuning Mechanism</a></p></li>
<li><p><a class="reference external" href="#more-tuning-options">More Tuning Options</a></p></li>
</ul>
</li>
</ul>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Neural</span> <span class="pre">Engine</span></code> supports tuning mechanisms which will try to find the best pattern, best graph and best kernel implementation and related runtime configurations. It includes graph tuning and op tuning. The whole workflow is as follows:
<img alt="../../../../../../_images/engine_dispatcher.png" src="../../../../../../_images/engine_dispatcher.png" />
Just like the picture shown. ONNX model will be compiled to Neural Engine IR first. And then you can use graph dispatcher to tune IR on graph level. For graph level tuning, it includes graph tuning and pattern tuning. And for further performance, op tuning will bring the best recipe of op config.</p>
</section>
<section id="pattern-tuning-for-dispatching-best-pattern">
<h2>Pattern Tuning for Dispatching Best Pattern<a class="headerlink" href="#pattern-tuning-for-dispatching-best-pattern" title="Link to this heading"></a></h2>
<p>For pattern tuning, it is mainly for two big patterns (Super Bert/ Merge QKV). The algorithm will automatically determine the pattern should be off or on according to the performance. You just set tune = True in subgraph_match():</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">intel_extension_for_transformers.llm.runtime.deprecated.compile.loaders.loader</span> <span class="kn">import</span> <span class="n">Loader</span>
<span class="kn">from</span> <span class="nn">intel_extension_for_transformers.llm.runtime.deprecated.compile.extractors.extractor</span> <span class="kn">import</span> <span class="n">Extractor</span>
<span class="kn">from</span> <span class="nn">intel_extension_for_transformers.llm.runtime.deprecated.compile.sub_graph.subgraph_matcher</span> <span class="kn">import</span> <span class="n">SubGraphMatcher</span>
<span class="n">load</span> <span class="o">=</span> <span class="n">Loader</span><span class="p">()</span>
<span class="n">extract</span> <span class="o">=</span> <span class="n">Extractor</span><span class="p">()</span>
<span class="n">subgraph_match</span> <span class="o">=</span> <span class="n">SubGraphMatcher</span><span class="p">()</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">model_path</span><span class="p">)</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">extract</span><span class="p">(</span><span class="n">graph</span><span class="p">)</span>
<span class="c1"># pattern tuning</span>
<span class="n">graph</span> <span class="o">=</span> <span class="n">subgraph_match</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">tune</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">graph</span><span class="o">.</span><span class="n">inference</span><span class="p">([</span><span class="n">data</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="graph-tuning-for-dispatching-best-graph">
<h2>Graph Tuning for Dispatching Best Graph<a class="headerlink" href="#graph-tuning-for-dispatching-best-graph" title="Link to this heading"></a></h2>
<p>For graph tuning, it is mainly for the sparse graphs or dense graphs. In some cases such as small shapes or devices with ISA, the performance of dense ops maybe perform better than sparse ops. And sparse op will bring other transpose ops. We have an easy-to-use API to tune sparse graphs, dense graphs or mix graphs automatically. You just need add graph_dispatch after <code class="docutils literal notranslate"><span class="pre">compile</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">intel_extension_for_transformers.llm.runtime.deprecated.compile</span> <span class="kn">import</span> <span class="nb">compile</span>
<span class="n">model</span> <span class="o">=</span> <span class="nb">compile</span><span class="p">(</span><span class="n">int8_model_path</span><span class="p">)</span>
<span class="c1"># set shape for graph tuning</span>
<span class="n">model</span><span class="o">.</span><span class="n">graph_dispatch</span><span class="p">(</span><span class="n">inputs_shape</span> <span class="o">=</span> <span class="p">[</span><span class="n">shape_0</span><span class="p">,</span> <span class="n">shape_1</span><span class="p">,</span> <span class="n">shape_2</span><span class="p">])</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">inference</span><span class="p">([</span><span class="n">input_0</span><span class="p">,</span> <span class="n">input_1</span><span class="p">,</span> <span class="n">input_2</span><span class="p">])</span>
</pre></div>
</div>
<blockquote>
<div><p><strong>Note</strong> Sparse and dense graph tuning only can be used on int8 models.</p>
</div></blockquote>
</section>
<section id="op-tuning-for-dispatching-best-kernel-and-related-runtime-config">
<h2>OP Tuning for Dispatching Best Kernel and Related Runtime Config<a class="headerlink" href="#op-tuning-for-dispatching-best-kernel-and-related-runtime-config" title="Link to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">Neural</span> <span class="pre">Engine</span></code> supports op tuning mechanism for trying to get the best kernel implementation and related runtime configurations. Below are the tuning ways for now (We will consider enlarging the tuning space in subsequent versions). So, before you turn on the tuning mechanism, you need to check if the model can be tuned.</p>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: center;">op type</th>
<th style="text-align: center;">dtype</th>
<th style="text-align: center;">default kernel</th>
<th style="text-align: center;">tuning kernel</th>
<th style="text-align: center;">kernel config</th>
<th style="text-align: center;">remark</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">InnerProduct</td>
<td style="text-align: center;">fp32</td>
<td style="text-align: center;">DNNL InnerProduct</td>
<td style="text-align: center;">DNNL Convolution</td>
<td style="text-align: center;">input shape</td>
<td style="text-align: center;">dense fp32 weight</td>
</tr>
<tr>
<td style="text-align: center;">InnerProduct</td>
<td style="text-align: center;">int8</td>
<td style="text-align: center;">SparseLib InnerProduct</td>
<td style="text-align: center;">SparseLib InnerProduct</td>
<td style="text-align: center;">input shape, micro oc, sub func level</td>
<td style="text-align: center;">sparse int8 structured weight</td>
</tr>
</tbody>
</table><blockquote>
<div><p><strong>Note</strong></p>
<ol class="simple">
<li><p>DNNL InnerProduct (MK x KN) - DNNL Convolution (NHWC, conv kernel size is 1x1), split M into NxHxW, K=C. Tune best N, H, W combinations.</p></li>
<li><p>SparseLib InnerProduct (NK x KM) - SparseLib InnerProduct (NK x MmKMb), split M into MmxMb. micro oc is a positive integer fulfilling micro oc &lt;= OC &amp;&amp; micro oc % 4 == 0 determined the size along the output dimension is processed in each OMP iteration. Tune best MmxMb, micro oc and sub func level (higher sub_func value means more operations are done in sub-function, i.e. less unrolling) combinations.</p></li>
<li><p>see op_tuning.hpp if you want to know more details.</p></li>
</ol>
</div></blockquote>
<section id="how-to-turn-on-op-tuning-mechanism">
<h3>How to Turn on Op Tuning Mechanism<a class="headerlink" href="#how-to-turn-on-op-tuning-mechanism" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># load model from disk</span>
<span class="kn">from</span> <span class="nn">intel_extension_for_transformers.llm.runtime.deprecated.compile.graph</span> <span class="kn">import</span> <span class="n">Graph</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Graph</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">graph_init</span><span class="p">(</span><span class="n">conf</span><span class="o">.</span><span class="n">yaml</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">bin</span><span class="p">)</span>
<span class="c1"># or get model from compile</span>
<span class="kn">from</span> <span class="nn">intel_extension_for_transformers.llm.runtime.deprecated.compile</span> <span class="kn">import</span> <span class="nb">compile</span>
<span class="n">model</span> <span class="o">=</span> <span class="nb">compile</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">onnx</span><span class="p">)</span>
<span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;enable_op_tuning&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
<span class="n">model</span><span class="o">.</span><span class="n">execution_options</span> <span class="o">=</span> <span class="n">options</span>
<span class="c1"># inference with fixed shape data</span>
<span class="n">model</span><span class="o">.</span><span class="n">inference</span><span class="p">([</span><span class="n">data</span><span class="p">])</span>
</pre></div>
</div>
<p>You can see a txt file called <code class="docutils literal notranslate"><span class="pre">engine_dispatch_table.txt</span></code> in your workspace folder which records tuning results after executing the above code.</p>
<p>This table file contains the lines number (first row) and the rest of the lines contain best tuning results which are like: <code class="docutils literal notranslate"><span class="pre">op_type</span> <span class="pre">(string)</span> <span class="pre">-</span> <span class="pre">hash</span> <span class="pre">key</span> <span class="pre">(size_t)</span> <span class="pre">-</span> <span class="pre">kernel_config(kernel_name</span> <span class="pre">(string)</span> <span class="pre">-</span> <span class="pre">input_shape-</span> <span class="pre">...</span> <span class="pre">&lt;if</span> <span class="pre">has</span> <span class="pre">specific</span> <span class="pre">dispatch</span> <span class="pre">kernel</span> <span class="pre">config</span> <span class="pre">except</span> <span class="pre">kernel</span> <span class="pre">name&gt;)</span></code>.</p>
<p>Examples:</p>
<p>DNNL InnerProduct -DNNL Convolution</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>1
InnerProduct 3025159985633461085 Convolution 4,1,40,1024
</pre></div>
</div>
<p>SparseLib InnerProduct - SparseLib  InnerProduct</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>1
InnerProduct 14124194128933833351 SparseLib 4,1024,384 0 2
</pre></div>
</div>
<blockquote>
<div><p><strong>Note</strong> Do not do other things when tuning your model! Otherwise, the final results may be affected.</p>
</div></blockquote>
</section>
<section id="more-tuning-options">
<h3>More Tuning Options<a class="headerlink" href="#more-tuning-options" title="Link to this heading"></a></h3>
<p>You can set the table file path and tuning warmup iterations if you want to simulate real deployment conditions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;enable_op_tuning&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
           <span class="c1"># set tuning warmup iterations</span>
           <span class="s1">&#39;warmup_iter&#39;</span><span class="p">:</span> <span class="n">num_iterations</span><span class="p">,</span>
           <span class="c1"># set table file path</span>
           <span class="s1">&#39;dispatch_table_file_root&#39;</span><span class="p">:</span> <span class="n">file_root</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">model</span><span class="o">.</span><span class="n">execution_options</span> <span class="o">=</span> <span class="n">options</span>
<span class="c1"># inference with multi-iterations</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="c1"># data can have different shapes</span>
    <span class="n">model</span><span class="o">.</span><span class="n">inference</span><span class="p">([</span><span class="n">data</span><span class="p">])</span>
</pre></div>
</div>
<p>If you want to get model performance after tuning in one python script, you can use the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># load model here</span>
<span class="o">...</span>

<span class="c1"># 1. before tuning</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">inference</span><span class="p">([</span><span class="n">data</span><span class="p">])</span>
<span class="c1"># get performance here</span>

<span class="c1"># 2. tuning</span>
<span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;enable_op_tuning&#39;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
           <span class="c1"># set tuning warmup iterations</span>
           <span class="s1">&#39;warmup_iter&#39;</span><span class="p">:</span> <span class="n">num_iterations</span><span class="p">,</span>
           <span class="c1"># set table file path</span>
           <span class="s1">&#39;dispatch_table_file_root&#39;</span><span class="p">:</span> <span class="n">file_root</span><span class="p">,</span>
<span class="p">}</span>
<span class="n">model</span><span class="o">.</span><span class="n">execution_options</span> <span class="o">=</span> <span class="n">options</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">inference</span><span class="p">([</span><span class="n">data</span><span class="p">])</span>
<span class="c1"># get performance here</span>

<span class="c1"># 3. after tuning</span>
<span class="n">options</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;enable_op_tuning&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">}</span>
<span class="n">model</span><span class="o">.</span><span class="n">execution_options</span> <span class="o">=</span> <span class="n">options</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iterations</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">inference</span><span class="p">([</span><span class="n">data</span><span class="p">])</span>
<span class="c1"># get performance here</span>
</pre></div>
</div>
<p>However, we recommend you benchmark model performance one by one to prevent imprecise results.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="engine_profiling.html" class="btn btn-neutral float-left" title="Profiling" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="graph_fusion.html" class="btn btn-neutral float-right" title="Graph Fusion" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7fbb128d7880> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>