<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Finetune Embedding Model on Task-Specific Datasets &mdash; Intel® Extension for Transformers 0.1.dev1+gfb6120a documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../SECURITY.html">OpenSSF Badge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../SECURITY.html#security-policy">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Finetune Embedding Model on Task-Specific Datasets</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../_sources/docs/intel_extension_for_transformers/neural_chat/tools/embedding_finetune/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="finetune-embedding-model-on-task-specific-datasets">
<h1>Finetune Embedding Model on Task-Specific Datasets<a class="headerlink" href="#finetune-embedding-model-on-task-specific-datasets" title="Link to this heading"></a></h1>
<section id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>In this example, we show how to construct the training data for finetuning the embedding model and finetuning the specific embedding model.</p>
</section>
<section id="requirements">
<h2>2. Requirements<a class="headerlink" href="#requirements" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>On CPU</strong></p></li>
</ul>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="o">-</span><span class="n">r</span><span class="w"> </span><span class="n">requirements_cpu</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>On CUDA</strong></p></li>
</ul>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span><span class="w"> </span><span class="n">install</span><span class="w"> </span><span class="o">-</span><span class="n">r</span><span class="w"> </span><span class="n">requirements_cuda</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</div>
</section>
<section id="training-data-construction">
<h2>3. Training Data Construction<a class="headerlink" href="#training-data-construction" title="Link to this heading"></a></h2>
<p>Train data should be a JSON file, where each line is a dict like this:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s">&quot;query&quot;</span><span class="o">:</span><span class="w"> </span><span class="n">str</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;pos&quot;</span><span class="o">:</span><span class="w"> </span><span class="n">List</span><span class="p">[</span><span class="n">str</span><span class="p">],</span><span class="w"> </span><span class="s">&quot;neg&quot;</span><span class="o">:</span><span class="w"> </span><span class="n">List</span><span class="p">[</span><span class="n">str</span><span class="p">]}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">query</span></code> is the query, and <code class="docutils literal notranslate"><span class="pre">pos</span></code> is a positive text, <code class="docutils literal notranslate"><span class="pre">neg</span></code> is a list of negative texts.
See <a class="reference external" href="https://github.com/intel/intel-extension-for-transformers/blob/master/intel_extension_for_transformers/neural_chat/tools/embedding_finetune/augmented_example.jsonl">augmented_example.jsonl</a> for a data file.</p>
<p>If you have no negative texts for a query, You can use <a class="reference external" href="https://github.com/intel/intel-extension-for-transformers/blob/master/intel_extension_for_transformers/neural_chat/tools/embedding_finetune/mine_hard_neg.py">this script</a> as follows to mine a given number of hard negatives.</p>
<section id="mine-hard-negatives">
<h3>Mine Hard Negatives<a class="headerlink" href="#mine-hard-negatives" title="Link to this heading"></a></h3>
<p>Hard negatives mining is a widely used method to improve the quality of sentence embedding.
You can mine hard negatives following this command:</p>
<ul class="simple">
<li><p><strong>On CPU</strong></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>mine_hard_neg.py<span class="w"> </span><span class="se">\</span>
--model_name_or_path<span class="w"> </span>BAAI/bge-base-en-v1.5<span class="w"> </span><span class="se">\</span>
--input_file<span class="w"> </span>example.jsonl<span class="w"> </span><span class="se">\</span>
--output_file<span class="w"> </span>augmented_example.jsonl<span class="w"> </span><span class="se">\</span>
--range_for_sampling<span class="w"> </span><span class="m">2</span>-10<span class="w"> </span><span class="se">\</span>
--negative_number<span class="w"> </span><span class="m">5</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>On CUDA</strong></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>mine_hard_neg.py<span class="w"> </span><span class="se">\</span>
--model_name_or_path<span class="w"> </span>BAAI/bge-base-en-v1.5<span class="w"> </span><span class="se">\</span>
--input_file<span class="w"> </span>example.jsonl<span class="w"> </span><span class="se">\</span>
--output_file<span class="w"> </span>augmented_example.jsonl<span class="w"> </span><span class="se">\</span>
--range_for_sampling<span class="w"> </span><span class="m">2</span>-10<span class="w"> </span><span class="se">\</span>
--negative_number<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
--use_gpu_for_searching<span class="w"> </span>
</pre></div>
</div>
<p><strong>Some Important Arguments</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_file</span></code>: JSON data for finetuning. This script will retrieve top-k documents for each query,
and random sample negatives from the top-k documents (not including the positive documents).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_file</span></code>: path to save JSON data with mined hard negatives for finetuning</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">negative_number</span></code>: the number of sampled negatives</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">range_for_sampling</span></code>: where to sample negative. For example, <code class="docutils literal notranslate"><span class="pre">2-100</span></code> means sampling <code class="docutils literal notranslate"><span class="pre">negative_number</span></code> negatives from top2-top200 documents. You can set a larger value to reduce the difficulty of negatives (e.g., set it <code class="docutils literal notranslate"><span class="pre">60-300</span></code> to sample negatives from top60-300 passages)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_gpu_for_searching</span></code>: whether to use faiss-gpu to retrieve negatives.</p></li>
</ul>
</section>
</section>
<section id="training-example">
<h2>4. Training Example<a class="headerlink" href="#training-example" title="Link to this heading"></a></h2>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">python</span><span class="w"> </span><span class="n">finetune</span><span class="p">.</span><span class="n">py</span><span class="w"> </span>\
<span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="n">BAAI</span><span class="o">/</span><span class="n">bge</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">en</span><span class="o">-</span><span class="n">v1</span><span class="mf">.5</span><span class="n">_finetuned</span><span class="w"> </span>\
<span class="o">--</span><span class="n">model_name_or_path</span><span class="w"> </span><span class="n">BAAI</span><span class="o">/</span><span class="n">bge</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">en</span><span class="o">-</span><span class="n">v1</span><span class="mf">.5</span><span class="w"> </span>\
<span class="o">--</span><span class="n">train_data</span><span class="w"> </span><span class="n">augmented_example</span><span class="p">.</span><span class="n">jsonl</span><span class="w"> </span>\
<span class="o">--</span><span class="n">learning_rate</span><span class="w"> </span><span class="mf">1e-5</span><span class="w"> </span>\
<span class="o">--</span><span class="n">num_train_epochs</span><span class="w"> </span><span class="mi">5</span><span class="w"> </span>\
<span class="o">--</span><span class="n">per_device_train_batch_size</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span>\
<span class="o">--</span><span class="n">dataloader_drop_last</span><span class="w"> </span><span class="n">True</span><span class="w"> </span>\
<span class="o">--</span><span class="n">normalized</span><span class="w"> </span><span class="n">True</span><span class="w"> </span>\
<span class="o">--</span><span class="n">temperature</span><span class="w"> </span><span class="mf">0.02</span><span class="w"> </span>\
<span class="o">--</span><span class="n">query_max_len</span><span class="w"> </span><span class="mi">64</span><span class="w"> </span>\
<span class="o">--</span><span class="n">passage_max_len</span><span class="w"> </span><span class="mi">256</span><span class="w"> </span>\
<span class="o">--</span><span class="n">train_group_size</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span>\
<span class="o">--</span><span class="n">logging_steps</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span>\
<span class="o">--</span><span class="n">query_instruction_for_retrieval</span><span class="w"> </span><span class="s">&quot;&quot;</span><span class="w"> </span>\
<span class="o">--</span><span class="n">bf16</span><span class="w"> </span><span class="n">True</span>
</pre></div>
</div>
<p><strong>Some Important Arguments</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">per_device_train_batch_size</span></code>: batch size in training. In most cases, a larger batch size will bring stronger performance.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">train_group_size</span></code>: the number of positives and negatives for a query in training.
There is always one positive, so this argument will control the number of negatives (#negatives=train_group_size-1).
Noted that the number of negatives should not be larger than the number of negatives in data <code class="docutils literal notranslate"><span class="pre">&quot;neg&quot;:</span> <span class="pre">List[str]</span></code>.
Besides the negatives in this group, the in-batch negatives also will be used in fine-tuning.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">negatives_cross_device</span></code>: share the negatives across all GPUs. This argument will extend the number of negatives.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>: select an appropriate for your model. Recommend 1e-5/2e-5/3e-5 for large/base/small-scale.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">temperature</span></code>: It will influence the distribution of similarity scores.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">query_max_len</span></code>: max length for query. Please set it according to the average length of queries in your data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">passage_max_len</span></code>: max length for passage. Please set it according to the average length of passages in your data.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">query_instruction_for_retrieval</span></code>: instruction for query, which will be added to each query. You also can set it <code class="docutils literal notranslate"><span class="pre">&quot;&quot;</span></code> to add nothing to the query.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">use_inbatch_neg</span></code>: use passages in the same batch as negatives. The default value is True.</p></li>
</ul>
<p>For more training arguments please refer to <a class="reference external" href="https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments">transformers.TrainingArguments</a></p>
</section>
<section id="evaluation">
<h2>5. Evaluation<a class="headerlink" href="#evaluation" title="Link to this heading"></a></h2>
<p>We provide <a class="reference external" href="https://github.com/intel/intel-extension-for-transformers/blob/master/intel_extension_for_transformers/neural_chat/tools/embedding_finetune/evaluate.py">a simple script</a> to evaluate the model’s performance. We use two metrics: MRR (Mean reciprocal rank) and Hit (Hit Ratio). MRR is an internationally accepted mechanism for evaluating search algorithms. MRR emphasizes the position of ground truth in the retrieval list, the higher it is, the better. Hit emphasizes the accuracy of retrieval, that is, whether the ground truth is included in the retrieval items.</p>
<ul class="simple">
<li><p><strong>Before Finetune</strong></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py<span class="w"> </span><span class="se">\</span>
--model_name<span class="w"> </span>BAAI/bge-base-en-v1.5<span class="w"> </span><span class="se">\</span>
--index_file_jsonl_path<span class="w"> </span>candidate_context.jsonl<span class="w"> </span><span class="se">\</span>
--query_file_jsonl_path<span class="w"> </span>example.jsonl
</pre></div>
</div>
<ul class="simple">
<li><p><strong>After Finetune</strong></p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>evaluate.py<span class="w"> </span><span class="se">\</span>
--model_name<span class="w"> </span>BAAI/bge-base-en-v1.5_finetuned<span class="w"> </span><span class="se">\</span>
--index_file_jsonl_path<span class="w"> </span>candidate_context.jsonl<span class="w"> </span><span class="se">\</span>
--query_file_jsonl_path<span class="w"> </span>example.jsonl
</pre></div>
</div>
<p><strong>Some Important Arguments:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">index_file_jsonl_path</span></code>: path of JSON data including candidate context where each line is a dict like this:<code class="docutils literal notranslate"><span class="pre">{&quot;context&quot;:</span> <span class="pre">List[str]}</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">query_file_jsonl_path</span></code>: path of JSON data including queries and positives where each line is a dict like this:<code class="docutils literal notranslate"><span class="pre">{&quot;query&quot;:</span> <span class="pre">str,</span> <span class="pre">&quot;pos&quot;:</span> <span class="pre">List[str]}</span></code>.</p></li>
</ul>
<p>We conducted a finetuning on an internal business dataset. The results were as follows:</p>
<ul class="simple">
<li><p><strong>Before Finetune</strong></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;MRR@1&#39;</span><span class="p">:</span> <span class="mf">0.7385</span><span class="p">,</span> <span class="s1">&#39;Hit@1&#39;</span><span class="p">:</span> <span class="mf">0.7336</span><span class="p">}</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>After Finetune</strong></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;MRR@1&#39;</span><span class="p">:</span> <span class="mf">0.8297</span><span class="p">,</span> <span class="s1">&#39;Hit@1&#39;</span><span class="p">:</span> <span class="mf">0.8275</span><span class="p">}</span>
</pre></div>
</div>
</section>
<section id="verified-models">
<h2>6. Verified Models<a class="headerlink" href="#verified-models" title="Link to this heading"></a></h2>
<p>|  Model Name   | Enable  |
|  :—-:  | :—-:  |
| <a class="reference external" href="https://huggingface.co/BAAI/bge-large-en-v1.5">bge-large-en-v1.5</a>  | ✔ |
| <a class="reference external" href="https://huggingface.co/BAAI/bge-base-en-v1.5">bge-base-en-v1.5</a>  | ✔ |
| <a class="reference external" href="https://huggingface.co/thenlper/gte-large">gte-large</a>  | ✔ |
| <a class="reference external" href="https://huggingface.co/thenlper/gte-base">gte-base</a>  | ✔ |
| <a class="reference external" href="https://huggingface.co/infgrad/stella-base-en-v2">stella-base-en-v2</a>  | ✔ |
| <a class="reference external" href="https://huggingface.co/intfloat/e5-large-v2">e5-large-v2</a>  | ✔ |
| <a class="reference external" href="https://huggingface.co/sentence-transformers/all-mpnet-base-v2">all-mpnet-base-v2</a>  | ✔ |</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f1c066c6620> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>