<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NeuralChat Fine-tuning &mdash; Intel® Extension for Transformers 0.1.dev1+gb00652d documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html">OpenSSF Badge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html#security-policy">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">NeuralChat Fine-tuning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/docs/intel_extension_for_transformers/neural_chat/examples/finetuning/instruction/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="neuralchat-fine-tuning">
<h1>NeuralChat Fine-tuning<a class="headerlink" href="#neuralchat-fine-tuning" title="Link to this heading"></a></h1>
<p>This example demonstrates how to finetune the pretrained large language model (LLM) with the instruction-following dataset for creating the NeuralChat, a chatbot that can conduct the textual conversation. Giving NeuralChat the textual instruction, it will respond with the textual response. This example have been validated on the 4th Gen Intel® Xeon® Processors, Sapphire Rapids.</p>
<section id="validated-model-list">
<h2>Validated Model List<a class="headerlink" href="#validated-model-list" title="Link to this heading"></a></h2>
<p>|Pretrained model| Text Generation (Instruction) | Text Generation (ChatBot) | Summarization | Code Generation |
|————————————|—|—|— | — |
|Mistral-7B | ✅| ✅|✅| ✅
|LLaMA series| ✅| ✅|✅| ✅
|LLaMA2 series| ✅| ✅|✅| ✅
|MPT series| ✅| ✅|✅| ✅
|FLAN-T5 series| ✅ | <strong>WIP</strong>| <strong>WIP</strong> | <strong>WIP</strong>|
|Mixtral-8x7B | ✅ | ✅| N/A  | ✅|
|Phi series | ✅ | ✅| N/A  | ✅|</p>
</section>
</section>
<section id="prerequisite">
<h1>Prerequisite​<a class="headerlink" href="#prerequisite" title="Link to this heading"></a></h1>
<section id="environment">
<h2>1. Environment​<a class="headerlink" href="#environment" title="Link to this heading"></a></h2>
<section id="bare-metal">
<h3>Bare Metal<a class="headerlink" href="#bare-metal" title="Link to this heading"></a></h3>
<p>Recommend python 3.9 or higher version.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.34.1
<span class="c1"># To use ccl as the distributed backend in distributed training on CPU requires to install below requirement.</span>
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span><span class="nv">oneccl_bind_pt</span><span class="o">==</span><span class="m">2</span>.3.0<span class="w"> </span>-f<span class="w"> </span>https://developer.intel.com/ipex-whl-stable-cpu
</pre></div>
</div>
<blockquote>
<div><p><strong>Note</strong>: Suggest using transformers no higher than 4.34.1</p>
</div></blockquote>
</section>
<section id="docker">
<h3>Docker<a class="headerlink" href="#docker" title="Link to this heading"></a></h3>
<p>Pick either one of below options to setup docker environment.</p>
<section id="option-1-build-docker-image-from-scratch">
<h4>Option 1 : Build Docker image from scratch<a class="headerlink" href="#option-1-build-docker-image-from-scratch" title="Link to this heading"></a></h4>
<p>Please refer to this section : <a class="reference external" href="../../../docker/finetuning/README.html#21-build-docker-image">How to build docker images for NeuralChat FineTuning</a> to build docker image from scratch.</p>
</section>
<section id="option-2-pull-existing-docker-image">
<h4>Option 2: Pull existing Docker image<a class="headerlink" href="#option-2-pull-existing-docker-image" title="Link to this heading"></a></h4>
<p>Please follow the session <a class="reference external" href="../../../docker/finetuning/README.html#22-docker-pull-from-docker-hub">itrex docker setup</a> and use the docker pull command to pull itrex docker image.</p>
<p>Once you have the docker image ready, please follow <a class="reference external" href="../../../docker/finetuning/README.html#3-create-docker-container">run docker image</a> session to launch a docker instance from the image.</p>
</section>
</section>
</section>
<section id="prepare-the-model">
<h2>2. Prepare the Model<a class="headerlink" href="#prepare-the-model" title="Link to this heading"></a></h2>
<section id="meta-llama-llama-2-7b-hf">
<h3>meta-llama/Llama-2-7b-hf<a class="headerlink" href="#meta-llama-llama-2-7b-hf" title="Link to this heading"></a></h3>
<p>To acquire the checkpoints and tokenizer, the user can get those files from <a class="reference external" href="https://huggingface.co/meta-llama/Llama-2-7b-hf">meta-llama/Llama-2-7b-hf</a>.
Users could follow below commands to get the checkpoints from github repository after the access request to the files is approved.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/meta-llama/Llama-2-7b-hf
</pre></div>
</div>
</section>
<section id="mpt">
<h3>MPT<a class="headerlink" href="#mpt" title="Link to this heading"></a></h3>
<p>To acquire the checkpoints and tokenizer, the user can get those files from <a class="reference external" href="https://huggingface.co/mosaicml/mpt-7b">mosaicml/mpt-7b</a>.
Users could follow below commands to get the checkpoints from github repository.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mosaicml/mpt-7b
</pre></div>
</div>
<p>For missing GPTNeoTokenizer issue, we advise the user to modify the local <code class="docutils literal notranslate"><span class="pre">tokenizer_config.json</span></code> file according to the following recommendation:</p>
<ol class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">tokenizer_class</span></code> in <code class="docutils literal notranslate"><span class="pre">tokenizer_config.json</span></code> should be changed from <code class="docutils literal notranslate"><span class="pre">GPTNeoXTokenizer</span></code> to <code class="docutils literal notranslate"><span class="pre">GPTNeoXTokenizerFast</span></code>;</p></li>
</ol>
</section>
<section id="falcon">
<h3>Falcon<a class="headerlink" href="#falcon" title="Link to this heading"></a></h3>
<p>To acquire the checkpoints and tokenizer, the user can get those files from <a class="reference external" href="https://huggingface.co/tiiuae/falcon-7b">tiiuae/falcon-7b</a>.
Users could follow below commands to get the checkpoints from github repository.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/tiiuae/falcon-7b
</pre></div>
</div>
</section>
<section id="mistral">
<h3>Mistral<a class="headerlink" href="#mistral" title="Link to this heading"></a></h3>
<p>To acquire the checkpoints and tokenizer, the user can get those files from <a class="reference external" href="https://huggingface.co/mistralai/Mistral-7B-v0.1">mistralai/Mistral-7B-v0.1</a>.
Users could follow below commands to get the checkpoints from github repository.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mistralai/Mistral-7B-v0.1
</pre></div>
</div>
</section>
<section id="codellama">
<h3>CodeLlama<a class="headerlink" href="#codellama" title="Link to this heading"></a></h3>
<p>To acquire the checkpoints and tokenizer, the user can get those files from <a class="reference external" href="https://huggingface.co/codellama/CodeLlama-7b-hf">codellama/CodeLlama-7b-hf</a>.
Users could follow below commands to get the checkpoints from github repository.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/codellama/CodeLlama-7b-hf
</pre></div>
</div>
</section>
<section id="starcoder">
<h3>StarCoder<a class="headerlink" href="#starcoder" title="Link to this heading"></a></h3>
<p>To acquire the checkpoints and tokenizer, the user can get those files from <a class="reference external" href="https://huggingface.co/bigcode/starcoder">bigcode/starcoder</a>.
Users could follow below commands to get the checkpoints from github repository.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/bigcode/starcoder
</pre></div>
</div>
</section>
<section id="flan-t5">
<h3>FLAN-T5<a class="headerlink" href="#flan-t5" title="Link to this heading"></a></h3>
<p>The user can obtain the <a class="reference external" href="https://huggingface.co/google/flan-t5-xl">release model</a> from Huggingface.</p>
</section>
</section>
<section id="prepare-dataset">
<h2>3. Prepare Dataset<a class="headerlink" href="#prepare-dataset" title="Link to this heading"></a></h2>
<p>We select 4 kind of datasets to conduct the finetuning process for different tasks.</p>
<ol class="simple">
<li><p>Text Generation (General domain instruction): We use the <a class="reference external" href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca dataset</a> from Stanford University as the general domain dataset to fine-tune the model. This dataset is provided in the form of a JSON file, <a class="reference external" href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/alpaca_data.json">alpaca_data.json</a>. In Alpaca, researchers have manually crafted 175 seed tasks to guide <code class="docutils literal notranslate"><span class="pre">text-davinci-003</span></code> in generating 52K instruction data for diverse tasks. For finetuning on this dataset, user can choose to add <code class="docutils literal notranslate"><span class="pre">--task</span> <span class="pre">completion</span></code> argument or not since it’s default to <code class="docutils literal notranslate"><span class="pre">completion</span></code>.</p></li>
<li><p>Text Generation (Domain-specific instruction): Inspired by Alpaca, we constructed a domain-specific dataset focusing on Business and Intel-related issues. We made minor modifications to the <a class="reference external" href="https://github.com/tatsu-lab/stanford_alpaca/blob/main/prompt.txt">prompt template</a> to proactively guide Alpaca in generating more Intel and Business related instruction data. The generated data could be find in <code class="docutils literal notranslate"><span class="pre">intel_domain.json</span></code>. For finetuning on this dataset, user can choose to add <code class="docutils literal notranslate"><span class="pre">--task</span> <span class="pre">completion</span></code> argument or not since it’s default to <code class="docutils literal notranslate"><span class="pre">completion</span></code>.</p></li>
<li><p>Text Generation (ChatBot): To finetune a chatbot, we use the chat-style dataset <a class="reference external" href="https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200k">HuggingFaceH4/ultrachat_200k</a>. For finetuning on this dataset, please add <code class="docutils literal notranslate"><span class="pre">--task</span> <span class="pre">chat</span></code> argument.</p></li>
<li><p>Summarization: An English-language dataset <a class="reference external" href="https://huggingface.co/datasets/cnn_dailymail">cnn_dailymail</a> containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail, is used for this task. For finetuning on this dataset, please add <code class="docutils literal notranslate"><span class="pre">--task</span> <span class="pre">summarization</span></code> argument.</p></li>
<li><p>Code Generation: To enhance code performance of LLMs (Large Language Models), we use the <a class="reference external" href="https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1">theblackcat102/evol-codealpaca-v1</a>. For finetuning on this dataset, please add <code class="docutils literal notranslate"><span class="pre">--task</span> <span class="pre">code-generation</span></code> argument.</p></li>
</ol>
<section id="dataset-related-arguments">
<h3>Dataset related arguments<a class="headerlink" href="#dataset-related-arguments" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>dataset_name</strong>: The name of the dataset to use (via the datasets library).</p></li>
<li><p><strong>dataset_config_name</strong>: The configuration name of the dataset to use (via the datasets library).</p></li>
<li><p><strong>train_file</strong>: The input training data file (a text file).</p></li>
<li><p><strong>validation_file</strong>: An optional input evaluation data file to evaluate the perplexity on (a text file).</p></li>
<li><p><strong>max_seq_length</strong>: The maximum total input sequence length after tokenization. Sequences longer than this will be truncated.</p></li>
<li><p><strong>validation_split_percentage</strong>: The percentage of the train set used as validation set in case there’s no validation split.</p></li>
<li><p><strong>dataset_concatenation</strong>: Whether to concatenate the sentence for more efficient training.</p></li>
</ul>
</section>
</section>
</section>
<section id="finetune">
<h1>Finetune<a class="headerlink" href="#finetune" title="Link to this heading"></a></h1>
<p>We employ the <a class="reference external" href="https://arxiv.org/pdf/2106.09685.pdf">LoRA approach</a> to finetune the LLM efficiently.</p>
<section id="single-node-fine-tuning-in-xeon-spr">
<h2>1. Single Node Fine-tuning in Xeon SPR<a class="headerlink" href="#single-node-fine-tuning-in-xeon-spr" title="Link to this heading"></a></h2>
<p><strong>For FLAN-T5</strong>, use the below command line for finetuning on the Alpaca dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>finetune_seq2seq.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;google/flan-t5-xl&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--train_file<span class="w"> </span><span class="s2">&quot;stanford_alpaca/alpaca_data.json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span><span class="m">1</span>.0e-5<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.03<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--weight_decay<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_steps<span class="w"> </span><span class="m">2000</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./flan-t5-xl_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora
</pre></div>
</div>
<section id="for-llama2">
<h3>For LLaMA2<a class="headerlink" href="#for-llama2" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>use the below command line for code tuning with <code class="docutils literal notranslate"><span class="pre">meta-llama/Llama-2-7b-hf</span></code> on <a class="reference external" href="https://huggingface.co/datasets/theblackcat102/evol-codealpaca-v1">theblackcat102/evol-codealpaca-v1</a>.</p></li>
</ul>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;meta-llama/Llama-2-7b-hf&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_name<span class="w"> </span><span class="s2">&quot;theblackcat102/evol-codealpaca-v1&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span>epoch<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./llama2_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_fast_tokenizer<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--no_cuda

<span class="c1"># the script also support other models, like mpt.</span>
</pre></div>
</div>
<p><strong>For <a class="reference external" href="https://huggingface.co/codellama/CodeLlama-7b-hf">CodeLlama</a></strong>, use the below command line for finetuning on the <a class="reference external" href="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k">sahil2801/CodeAlpaca-20k</a> code instruction dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;codellama/CodeLlama-7b-hf&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_name<span class="w"> </span><span class="s2">&quot;sahil2801/CodeAlpaca-20k&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span>epoch<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./codellama_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_fast_tokenizer<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--no_cuda<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--task<span class="w"> </span>code-generation
</pre></div>
</div>
<p><strong>For <a class="reference external" href="https://huggingface.co/mosaicml/mpt-7b">MPT</a></strong>, use the below command line for finetuning on the Alpaca dataset. Only LORA supports MPT in PEFT perspective.it uses gpt-neox-20b tokenizer, so you need to define it in command line explicitly.This model also requires that trust_remote_code=True be passed to the from_pretrained method. This is because we use a custom MPT model architecture that is not yet part of the Hugging Face transformers package.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;mosaicml/mpt-7b&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--train_file<span class="w"> </span><span class="s2">&quot;/path/to/alpaca_data.json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span>epoch<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./mpt_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--trust_remote_code<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_fast_tokenizer<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--tokenizer_name<span class="w"> </span><span class="s2">&quot;EleutherAI/gpt-neox-20b&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--no_cuda<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p><strong>For Falcon</strong>, use the below command line for finetuning on the Alpaca dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;tiiuae/falcon-7b&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--train_file<span class="w"> </span><span class="s2">&quot;/path/to/alpaca_data.json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span>epoch<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./falcon_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_fast_tokenizer<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--no_cuda<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p><strong>For Mistral</strong>, use the below command line for finetuning on the Alpaca dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;mistralai/Mistral-7B-v0.1&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--train_file<span class="w"> </span><span class="s2">&quot;/path/to/alpaca_data.json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span>epoch<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./mistral_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_fast_tokenizer<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--no_cuda<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p><strong>For StarCoder</strong>, use the below command line for finetuning on the <a class="reference external" href="https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k">sahil2801/CodeAlpaca-20k</a> code instruction dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;bigcode/starcoder&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_name<span class="w"> </span><span class="s2">&quot;sahil2801/CodeAlpaca-20k&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">8</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">100</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span>epoch<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./starcoder_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_fast_tokenizer<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--no_cuda<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--task<span class="w"> </span>code-generation
</pre></div>
</div>
<p>Where the <code class="docutils literal notranslate"><span class="pre">--dataset_concatenation</span></code> argument is a way to vastly accelerate the fine-tuning process through training samples concatenation. With several tokenized sentences concatenated into a longer and concentrated sentence as the training sample instead of having several training samples with different lengths, this way is more efficient due to the parallelism characteristic provided by the more concentrated training samples.</p>
<p>For finetuning on SPR, add <code class="docutils literal notranslate"><span class="pre">--bf16</span></code> argument will speedup the finetuning process without the loss of model’s performance.
You could also indicate <code class="docutils literal notranslate"><span class="pre">--peft</span></code> to switch peft method in P-tuning, Prefix tuning, Prompt tuning, LLama Adapter, LoRA,
see https://github.com/huggingface/peft. Note for FLAN-T5/MPT, only LoRA is supported.</p>
</section>
</section>
<section id="multi-node-fine-tuning-in-xeon-spr">
<h2>2. Multi-node Fine-tuning in Xeon SPR<a class="headerlink" href="#multi-node-fine-tuning-in-xeon-spr" title="Link to this heading"></a></h2>
<p>We also supported Distributed Data Parallel finetuning on single node and multi-node settings. To use Distributed Data Parallel to speedup training, the bash command needs a small adjustment.
<br>
For example, to finetune FLAN-T5 through Distributed Data Parallel training, bash command will look like the following, where
<br>
<em><code class="docutils literal notranslate"><span class="pre">&lt;MASTER_ADDRESS&gt;</span></code></em> is the address of the master node, it won’t be necessary for single node case,
<br>
<em><code class="docutils literal notranslate"><span class="pre">&lt;NUM_PROCESSES_PER_NODE&gt;</span></code></em> is the desired processes to use in current node, for node with GPU, usually set to number of GPUs in this node, for node without GPU and use CPU for training, it’s recommended set to 1,
<br>
<em><code class="docutils literal notranslate"><span class="pre">&lt;NUM_NODES&gt;</span></code></em> is the number of nodes to use,
<br>
<em><code class="docutils literal notranslate"><span class="pre">&lt;NODE_RANK&gt;</span></code></em> is the rank of the current node, rank starts from 0 to <em><code class="docutils literal notranslate"><span class="pre">&lt;NUM_NODES&gt;</span></code></em><code class="docutils literal notranslate"><span class="pre">-1</span></code>.
<br></p>
<blockquote>
<div><p>Also please note that to use CPU for training in each node with multi-node settings, argument <code class="docutils literal notranslate"><span class="pre">--no_cuda</span></code> is mandatory, and <code class="docutils literal notranslate"><span class="pre">--ddp_backend</span> <span class="pre">ccl</span></code> is required if to use ccl as the distributed backend. In multi-node setting, following command needs to be launched in each node, and all the commands should be the same except for <em><code class="docutils literal notranslate"><span class="pre">&lt;NODE_RANK&gt;</span></code></em>, which should be integer from 0 to <em><code class="docutils literal notranslate"><span class="pre">&lt;NUM_NODES&gt;</span></code></em><code class="docutils literal notranslate"><span class="pre">-1</span></code> assigned to each node.</p>
</div></blockquote>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>mpirun<span class="w"> </span>-f<span class="w"> </span>nodefile<span class="w"> </span>-n<span class="w"> </span><span class="m">16</span><span class="w"> </span>-ppn<span class="w"> </span><span class="m">4</span><span class="w"> </span>-genv<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">56</span><span class="w"> </span>python3<span class="w"> </span>finetune_seq2seq.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;google/flan-t5-xl&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--train_file<span class="w"> </span><span class="s2">&quot;stanford_alpaca/alpaca_data.json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span><span class="m">1</span>.0e-5<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--warmup_ratio<span class="w"> </span><span class="m">0</span>.03<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--weight_decay<span class="w"> </span><span class="m">0</span>.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">5</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">10</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">2000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_total_limit<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>./flan-t5-xl_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_cuda<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ddp_backend<span class="w"> </span>ccl<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>If you have enabled passwordless SSH in cpu clusters, you could also use mpirun in master node to start the DDP finetune. Take llama alpaca finetune for example. follow the <a class="reference external" href="https://huggingface.co/docs/transformers/perf_train_cpu_many">hugginface guide</a> to install Intel® oneCCL Bindings for PyTorch, IPEX</p>
<p>oneccl_bindings_for_pytorch is installed along with the MPI tool set. Need to source the environment before using it.</p>
<p>for Intel® oneCCL &gt;= 1.12.0</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">oneccl_bindings_for_pytorch_path</span><span class="o">=</span><span class="k">$(</span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;from oneccl_bindings_for_pytorch import cwd; print(cwd)&quot;</span><span class="k">)</span>
<span class="nb">source</span><span class="w"> </span><span class="nv">$oneccl_bindings_for_pytorch_path</span>/env/setvars.sh
</pre></div>
</div>
<p>for Intel® oneCCL whose version &lt; 1.12.0</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">torch_ccl_path</span><span class="o">=</span><span class="k">$(</span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import torch; import torch_ccl; import os;  print(os.path.abspath(os.path.dirname(torch_ccl.__file__)))&quot;</span><span class="k">)</span>
<span class="nb">source</span><span class="w"> </span><span class="nv">$torch_ccl_path</span>/env/setvars.sh
</pre></div>
</div>
<p>The following command enables training with a total of 16 processes on 4 Xeons (node0/1/2/3, 2 sockets each node. taking node0 as the master node), ppn (processes per node) is set to 4, with two processes running per one socket. The variables OMP_NUM_THREADS/CCL_WORKER_COUNT can be tuned for optimal performance.</p>
<p>In node0, you need to create a configuration file which contains the IP addresses of each node (for example nodefile) and pass that configuration file path as an argument.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w"> </span>cat<span class="w"> </span>nodefile
<span class="w"> </span>xxx.xxx.xxx.xxx<span class="w"> </span><span class="c1">#node0 ip</span>
<span class="w"> </span>xxx.xxx.xxx.xxx<span class="w"> </span><span class="c1">#node1 ip</span>
<span class="w"> </span>xxx.xxx.xxx.xxx<span class="w"> </span><span class="c1">#node2 ip</span>
<span class="w"> </span>xxx.xxx.xxx.xxx<span class="w"> </span><span class="c1">#node3 ip</span>
</pre></div>
</div>
<p>Now, run the following command in node0 and <strong>4DDP</strong> will be enabled in node0 and node1 with BF16 auto mixed precision:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">CCL_WORKER_COUNT</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">MASTER_ADDR</span><span class="o">=</span>xxx.xxx.xxx.xxx<span class="w"> </span><span class="c1">#node0 ip</span>
<span class="c1">## for DDP ptun for LLama2</span>
mpirun<span class="w"> </span>-f<span class="w"> </span>nodefile<span class="w"> </span>-n<span class="w"> </span><span class="m">16</span><span class="w"> </span>-ppn<span class="w"> </span><span class="m">4</span><span class="w"> </span>-genv<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">56</span><span class="w"> </span>python3<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>meta-llama/Llama-2-7b-chat-hf<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--train_file<span class="w"> </span>./alpaca_data.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>./llama2_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--evaluation_strategy<span class="w"> </span><span class="s2">&quot;no&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_strategy<span class="w"> </span><span class="s2">&quot;steps&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">2000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_total_limit<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>1e-4<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--peft<span class="w"> </span>ptun<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group_by_length<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_fast_tokenizer<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_cuda<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ddp_backend<span class="w"> </span>ccl<span class="w"> </span><span class="se">\</span>

<span class="c1">## for DDP LORA for MPT</span>
mpirun<span class="w"> </span>-f<span class="w"> </span>nodefile<span class="w"> </span>-n<span class="w"> </span><span class="m">16</span><span class="w"> </span>-ppn<span class="w"> </span><span class="m">4</span><span class="w"> </span>-genv<span class="w"> </span><span class="nv">OMP_NUM_THREADS</span><span class="o">=</span><span class="m">56</span><span class="w"> </span>python3<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--model_name_or_path<span class="w"> </span>mosaicml/mpt-7b<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--train_file<span class="w"> </span>./alpaca_data.json<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output_dir<span class="w"> </span>./mpt_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_train_batch_size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--evaluation_strategy<span class="w"> </span><span class="s2">&quot;no&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_strategy<span class="w"> </span><span class="s2">&quot;steps&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_steps<span class="w"> </span><span class="m">2000</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--save_total_limit<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--learning_rate<span class="w"> </span>1e-4<span class="w">  </span><span class="se">\</span>
<span class="w">    </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--group_by_length<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--trust_remote_code<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--use_fast_tokenizer<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--tokenizer_name<span class="w"> </span><span class="s2">&quot;EleutherAI/gpt-neox-20b&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--no_cuda<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--ddp_backend<span class="w"> </span>ccl<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>you could also indicate <code class="docutils literal notranslate"><span class="pre">--peft</span></code> to switch peft method in P-tuning, Prefix tuning, Prompt tuning, LLama Adapter, LORA,
see https://github.com/huggingface/peft</p>
</section>
<section id="multi-node-fine-tuning-in-aws-m7i-spr-instances">
<h2>3. Multi-node Fine-tuning in AWS m7i SPR instances<a class="headerlink" href="#multi-node-fine-tuning-in-aws-m7i-spr-instances" title="Link to this heading"></a></h2>
<section id="build-docker-image-with-customized-ssh-server-port-from-scratch">
<h3>Build Docker image with customized SSH server port from scratch<a class="headerlink" href="#build-docker-image-with-customized-ssh-server-port-from-scratch" title="Link to this heading"></a></h3>
<p>AWS instances have a SSH server on by default, so we need to start SSH Server with different port inside the docker instance.<br />Users could pick their CUSTOM_PORT but we should not use 22 as the SSH Server port inside the docker instance.<br />Please refer to this section : <a class="reference external" href="../../../docker/finetuning/README.html#21-build-docker-image">How to build docker images for NeuralChat FineTuning</a> and add <code class="docutils literal notranslate"><span class="pre">--build-arg</span> <span class="pre">SSHD_PORT=&lt;CUSTOM_PORT&gt;</span></code> to build docker image from scratch.</p>
<p>ex : using 2345 as the CUSTOM_PORT</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>docker<span class="w"> </span>build<span class="w"> </span>--build-arg<span class="w"> </span><span class="nv">UBUNTU_VER</span><span class="o">=</span><span class="m">22</span>.04<span class="w"> </span>--build-arg<span class="w"> </span><span class="nv">SSHD_PORT</span><span class="o">=</span><span class="m">2345</span><span class="w"> </span>-f<span class="w"> </span>intel-extension-for-transformers/intel_extension_for_transformers/neural_chat/docker/Dockerfile<span class="w"> </span>-t<span class="w"> </span><span class="si">${</span><span class="nv">IMAGE_NAME</span><span class="si">}</span>:<span class="si">${</span><span class="nv">IMAGE_TAG</span><span class="si">}</span><span class="w"> </span>.<span class="w"> </span>--target<span class="w"> </span>cpu
</pre></div>
</div>
</section>
<section id="add-one-aws-inbound-rule-for-distributed-training">
<h3>Add one AWS inbound rule for distributed training<a class="headerlink" href="#add-one-aws-inbound-rule-for-distributed-training" title="Link to this heading"></a></h3>
<p>Allow all network traffic inside the cluster, so that distributed training runs unencumbered.<br />AWS provides a safe and convenient way to do this with security groups. We just need to create a security group that allows all traffic from instances configured with that same security group and make sure to attach it to all instances in the cluster.<br />Here’s how my setup looks.<br /><img src="../../../assets/pictures/AWS_inbound_rule.png" alt="AWS_inbound" ><br />Users could also refer to <a class="reference external" href="https://huggingface.co/blog/intel-sapphire-rapids">a huggingface blog</a> for more details.</p>
</section>
<section id="same-instructions-as-multi-node-fine-tuning-in-xeon-spr-session">
<h3>Same Instructions as Multi-node Fine-tuning in Xeon SPR session<a class="headerlink" href="#same-instructions-as-multi-node-fine-tuning-in-xeon-spr-session" title="Link to this heading"></a></h3>
<p>Please follow previous Multi-node Fine-tuning in Xeon SPR session with the docker image and AWS inbound rule changes.<br />For the IPs in nodefile, please <strong>use private IP instead of public IP</strong>.<br /><img src="../../../assets/pictures/AWS_private_ip.png" alt="AWS_private"></p>
</section>
</section>
<section id="single-card-fine-tuning-in-habana-dl1">
<h2>1. Single Card Fine-tuning in Habana DL1<a class="headerlink" href="#single-card-fine-tuning-in-habana-dl1" title="Link to this heading"></a></h2>
<p>Follow install guidance in <a class="reference external" href="https://github.com/huggingface/optimum-habana">optimum-habana</a></p>
<p>For LLaMA2, use the below command line for finetuning on the Alpaca dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--train_file<span class="w"> </span><span class="s2">&quot;/path/to/alpaca_data.json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--evaluation_strategy<span class="w"> </span><span class="s2">&quot;no&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span><span class="s2">&quot;steps&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_steps<span class="w"> </span><span class="m">2000</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w">  </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./llama2_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_fast_tokenizer<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--device<span class="w"> </span><span class="s2">&quot;hpu&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_habana<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_lazy_mode<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>For <a class="reference external" href="https://huggingface.co/mosaicml/mpt-7b">MPT</a>, use the below command line for finetuning on the Alpaca dataset. Only LORA supports MPT in PEFT perspective.it uses gpt-neox-20b tokenizer, so you need to define it in command line explicitly.This model also requires that trust_remote_code=True be passed to the from_pretrained method. This is because we use a custom MPT model architecture that is not yet part of the Hugging Face transformers package.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;mosaicml/mpt-7b&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--train_file<span class="w"> </span><span class="s2">&quot;/path/to/alpaca_data.json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--evaluation_strategy<span class="w"> </span><span class="s2">&quot;no&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span><span class="s2">&quot;steps&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_steps<span class="w"> </span><span class="m">2000</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w">  </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./mpt_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--trust_remote_code<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--tokenizer_name<span class="w"> </span><span class="s2">&quot;EleutherAI/gpt-neox-20b&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--device<span class="w"> </span><span class="s2">&quot;hpu&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_habana<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_lazy_mode<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>Where the <code class="docutils literal notranslate"><span class="pre">--dataset_concatenation</span></code> argument is a way to vastly accelerate the fine-tuning process through training samples concatenation. With several tokenized sentences concatenated into a longer and concentrated sentence as the training sample instead of having several training samples with different lengths, this way is more efficient due to the parallelism characteristic provided by the more concentrated training samples.</p>
<p>For finetuning on SPR, add <code class="docutils literal notranslate"><span class="pre">--bf16</span></code> argument will speedup the finetuning process without the loss of model’s performance.
You could also indicate <code class="docutils literal notranslate"><span class="pre">--peft</span></code> to switch peft method in P-tuning, Prefix tuning, Prompt tuning, LLama Adapter, LoRA,
see https://github.com/huggingface/peft. Note for MPT, only LoRA is supported.</p>
</section>
<section id="multi-card-fine-tuning-in-habana-dl1">
<h2>2. Multi Card Fine-tuning in Habana DL1<a class="headerlink" href="#multi-card-fine-tuning-in-habana-dl1" title="Link to this heading"></a></h2>
<p>Follow install guidance in <a class="reference external" href="https://github.com/huggingface/optimum-habana">optimum-habana</a></p>
<p>For LLaMA2, use the below command line for finetuning on the Alpaca dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>gaudi_spawn.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--world_size<span class="w"> </span><span class="m">8</span><span class="w"> </span>--use_mpi<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;meta-llama/Llama-2-7b-chat-hf&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--train_file<span class="w"> </span><span class="s2">&quot;/path/to/alpaca_data.json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--evaluation_strategy<span class="w"> </span><span class="s2">&quot;no&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span><span class="s2">&quot;steps&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_steps<span class="w"> </span><span class="m">2000</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w">  </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./llama2_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_fast_tokenizer<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--device<span class="w"> </span><span class="s2">&quot;hpu&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_habana<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_lazy_mode<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>For <a class="reference external" href="https://huggingface.co/mosaicml/mpt-7b">MPT</a>, use the below command line for finetuning on the Alpaca dataset. Only LORA supports MPT in PEFT perspective.it uses gpt-neox-20b tokenizer, so you need to define it in command line explicitly.This model also requires that trust_remote_code=True be passed to the from_pretrained method. This is because we use a custom MPT model architecture that is not yet part of the Hugging Face transformers package.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>gaudi_spawn.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--world_size<span class="w"> </span><span class="m">8</span><span class="w"> </span>--use_mpi<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;mosaicml/mpt-7b&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--train_file<span class="w"> </span><span class="s2">&quot;/path/to/alpaca_data.json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--evaluation_strategy<span class="w"> </span><span class="s2">&quot;no&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span><span class="s2">&quot;steps&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_steps<span class="w"> </span><span class="m">2000</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w">  </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./mpt_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--trust_remote_code<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--tokenizer_name<span class="w"> </span><span class="s2">&quot;EleutherAI/gpt-neox-20b&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--device<span class="w"> </span><span class="s2">&quot;hpu&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_habana<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_lazy_mode<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>Multi-card finetuning of Llama2-70B with DeepSpeed ZeRO-3 optimization and LoRA in 8 Gaudi2 card
The following command requires Habana DeepSpeed 1.13.0 or later.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">PT_HPU_MAX_COMPOUND_OP_SIZE</span><span class="o">=</span><span class="m">10</span><span class="w"> </span><span class="nv">DEEPSPEED_HPU_ZERO3_SYNC_MARK_STEP_REQUIRED</span><span class="o">=</span><span class="m">1</span><span class="w"> </span>python<span class="w"> </span>gaudi_spawn.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--world_size<span class="w"> </span><span class="m">8</span><span class="w"> </span>--use_deepspeed<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;meta-llama/Llama-2-70b-chat-hf&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--bf16<span class="w"> </span>True<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_name<span class="w"> </span>tatsu-lab/alpaca<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--evaluation_strategy<span class="w"> </span><span class="s2">&quot;no&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span><span class="s2">&quot;steps&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_steps<span class="w"> </span><span class="m">2000</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w">  </span><span class="se">\</span>
<span class="w">        </span>--logging_steps<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./llama2_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_fast_tokenizer<span class="w"> </span><span class="nb">false</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--device<span class="w"> </span><span class="s2">&quot;hpu&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_habana<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--use_lazy_mode<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--deepspeed<span class="w"> </span>llama2_ds_zero3_config.json<span class="w"> </span><span class="se">\</span>
</pre></div>
</div>
<p>Multi-card finetuning of <a class="reference external" href="https://huggingface.co/mistralai/Mixtral-8x7B-v0.1">mistralai/Mixtral-8x7B-v0.1</a> with DeepSpeed ZeRO-3 optimization and LoRA in 8 Gaudi2 card
The following command requires Habana DeepSpeed 1.13.0 or later.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">PT_HPU_MAX_COMPOUND_OP_SIZE</span><span class="o">=</span><span class="mi">10</span><span class="w"> </span><span class="n">DEEPSPEED_HPU_ZERO3_SYNC_MARK_STEP_REQUIRED</span><span class="o">=</span><span class="mi">1</span><span class="w"> </span>\
<span class="w">    </span><span class="n">python3</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">gaudi_spawn</span><span class="p">.</span><span class="n">py</span><span class="w"> </span><span class="o">--</span><span class="n">use_deepspeed</span><span class="w"> </span><span class="o">--</span><span class="n">world_size</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span>\
<span class="w">    </span><span class="n">finetune_clm</span><span class="p">.</span><span class="n">py</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">model_name_or_path</span><span class="w"> </span><span class="s">&quot;mistralai/Mixtral-8x7B-v0.1&quot;</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">bf16</span><span class="w"> </span><span class="n">True</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">dataset_name</span><span class="w"> </span><span class="n">tatsu</span><span class="o">-</span><span class="n">lab</span><span class="o">/</span><span class="n">alpaca</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">dataset_concatenation</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">per_device_train_batch_size</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">per_device_eval_batch_size</span><span class="w"> </span><span class="mi">8</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">gradient_accumulation_steps</span><span class="w"> </span><span class="mi">4</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">do_train</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">learning_rate</span><span class="w"> </span><span class="mf">1e-4</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">num_train_epochs</span><span class="w"> </span><span class="mi">3</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">logging_steps</span><span class="w"> </span><span class="mi">10</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">save_total_limit</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">overwrite_output_dir</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">log_level</span><span class="w"> </span><span class="n">info</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">save_strategy</span><span class="w"> </span><span class="n">epoch</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">output_dir</span><span class="w"> </span><span class="p">.</span><span class="o">/</span><span class="n">mixtral_peft_finetuned_model</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">peft</span><span class="w"> </span><span class="n">lora</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">lora_target_modules</span><span class="w"> </span><span class="n">q_proj</span><span class="w"> </span><span class="n">k_proj</span><span class="w"> </span><span class="n">v_proj</span><span class="w"> </span><span class="n">o_proj</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">lora_rank</span><span class="w"> </span><span class="mi">64</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">lora_alpha</span><span class="w"> </span><span class="mi">16</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">use_fast_tokenizer</span><span class="w"> </span><span class="n">True</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">use_habana</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">use_lazy_mode</span><span class="w"> </span>\
<span class="w">    </span><span class="o">--</span><span class="n">deepspeed</span><span class="w"> </span><span class="n">llama2_ds_zero3_config</span><span class="p">.</span><span class="n">json</span>
</pre></div>
</div>
<p>Where the <code class="docutils literal notranslate"><span class="pre">--dataset_concatenation</span></code> argument is a way to vastly accelerate the fine-tuning process through training samples concatenation. With several tokenized sentences concatenated into a longer and concentrated sentence as the training sample instead of having several training samples with different lengths, this way is more efficient due to the parallelism characteristic provided by the more concentrated training samples.</p>
<p>For finetuning on SPR, add <code class="docutils literal notranslate"><span class="pre">--bf16</span></code> argument will speedup the finetuning process without the loss of model’s performance.
You could also indicate <code class="docutils literal notranslate"><span class="pre">--peft</span></code> to switch peft method in P-tuning, Prefix tuning, Prompt tuning, LLama Adapter, LoRA,
see https://github.com/huggingface/peft. Note for MPT, only LoRA is supported.</p>
</section>
<section id="fine-tuning-on-intel-arc-gpus">
<h2>Fine-tuning on Intel Arc GPUs<a class="headerlink" href="#fine-tuning-on-intel-arc-gpus" title="Link to this heading"></a></h2>
<section id="single-card-fine-tuning">
<h3>1. Single Card Fine-tuning<a class="headerlink" href="#single-card-fine-tuning" title="Link to this heading"></a></h3>
<p>Follow the installation guidance in <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">intel-extension-for-pytorch</a> to install intel-extension-for-pytorch for GPU.</p>
<p>For <code class="docutils literal notranslate"><span class="pre">google/gemma-2b</span></code>, use the below command line for finetuning on the Alpaca dataset.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>finetune_clm.py<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--model_name_or_path<span class="w"> </span><span class="s2">&quot;google/gemma-2b&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--train_file<span class="w"> </span><span class="s2">&quot;/path/to/alpaca_data.json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--dataset_concatenation<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_train_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--per_device_eval_batch_size<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_accumulation_steps<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--evaluation_strategy<span class="w"> </span><span class="s2">&quot;no&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_strategy<span class="w"> </span><span class="s2">&quot;steps&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_steps<span class="w"> </span><span class="m">2000</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--save_total_limit<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--learning_rate<span class="w"> </span>1e-4<span class="w">  </span><span class="se">\</span>
<span class="w">        </span>--do_train<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--num_train_epochs<span class="w"> </span><span class="m">3</span><span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--overwrite_output_dir<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--log_level<span class="w"> </span>info<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--output_dir<span class="w"> </span>./gemma-2b_peft_finetuned_model<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--peft<span class="w"> </span>lora<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--gradient_checkpointing<span class="w"> </span>True
</pre></div>
</div>
</section>
</section>
</section>
<section id="evaluation-metrics">
<h1>Evaluation Metrics<a class="headerlink" href="#evaluation-metrics" title="Link to this heading"></a></h1>
<ul class="simple">
<li><p><strong>train loss:</strong> <code class="docutils literal notranslate"><span class="pre">--do_train</span></code> is set for training, <code class="docutils literal notranslate"><span class="pre">train</span> <span class="pre">loss</span></code> will be logged during training.</p></li>
<li><p><strong>eval loss:</strong> set <code class="docutils literal notranslate"><span class="pre">--do_eval</span></code>. If dataset path doesn’t have the <code class="docutils literal notranslate"><span class="pre">validation</span></code> split, the validation dataset will be split from train dataset with the <code class="docutils literal notranslate"><span class="pre">validation_split_percentage</span></code> argument (default is 0). For example, you can set <code class="docutils literal notranslate"><span class="pre">--validation_split_percentage</span> <span class="pre">5</span></code> to split %5 of train dataset.</p></li>
<li><p><strong>lm-eval (for finetuning <code class="docutils literal notranslate"><span class="pre">--task</span> <span class="pre">chat</span></code> or <code class="docutils literal notranslate"><span class="pre">--task</span> <span class="pre">completion</span></code>):</strong> set <code class="docutils literal notranslate"><span class="pre">--do_lm_eval</span> <span class="pre">true</span></code> and <code class="docutils literal notranslate"><span class="pre">--lm_eval_tasks</span> <span class="pre">truthfulqa_mc</span></code></p></li>
<li><p><strong>rouge related metrics:</strong> the metrics will be calculated when the finetuning task is summarization <code class="docutils literal notranslate"><span class="pre">--task</span> <span class="pre">summarization</span></code></p></li>
<li><p><strong>human eval (code generation metric):</strong> the metric will be calculated when the finetuning task is code-generation <code class="docutils literal notranslate"><span class="pre">--task</span> <span class="pre">code-generation</span></code></p></li>
</ul>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f6df67e7310> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>