<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction &mdash; Intel® Extension for Transformers 0.1.dev1+g76aa9b1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html">OpenSSF Badge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html#security-policy">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Introduction</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/docs/intel_extension_for_transformers/neural_chat/examples/serving/TGI/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>This README is intended to guide you through setting up a NeuralChat chatbot with TGI serving framework. You can deploy it on various platforms, including Intel XEON Scalable Processors, Habana’s Gaudi processors (HPU), Intel Data Center GPU and Client GPU, Nvidia Data Center GPU and Client GPU.</p>
<section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h1>
<p>Text Generation Inference (TGI) is a toolkit for deploying and serving Large Language Models (LLMs). TGI enables high-performance text generation for the most popular open-source LLMs, including Llama, Falcon, StarCoder, BLOOM, GPT-NeoX, and T5.</p>
<p>NeuralChat is now able to integrate TGI serving and offer customers TGI-format Restful APIs like /v1/generate and /v1/generate_stream. Going through this example, you could start a NeuralChat service with the plugins and frameworks you configure in tgi.yaml.</p>
</section>
<section id="setup-environment">
<h1>Setup Environment<a class="headerlink" href="#setup-environment" title="Link to this heading"></a></h1>
<section id="setup-conda">
<h2>Setup Conda<a class="headerlink" href="#setup-conda" title="Link to this heading"></a></h2>
<p>First, you need to install and configure the Conda environment:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and install Miniconda</span>
wget<span class="w"> </span>https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash<span class="w"> </span>Miniconda*.sh
<span class="nb">source</span><span class="w"> </span>~/.bashrc
</pre></div>
</div>
</section>
<section id="install-numactl">
<h2>Install numactl<a class="headerlink" href="#install-numactl" title="Link to this heading"></a></h2>
<p>Next, install the numactl library:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>numactl
</pre></div>
</div>
</section>
<section id="install-python-dependencies">
<h2>Install Python dependencies<a class="headerlink" href="#install-python-dependencies" title="Link to this heading"></a></h2>
<p>Install the following Python dependencies using Conda:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>astunparse<span class="w"> </span>ninja<span class="w"> </span>pyyaml<span class="w"> </span>mkl<span class="w"> </span>mkl-include<span class="w"> </span>setuptools<span class="w"> </span>cmake<span class="w"> </span>cffi<span class="w"> </span>typing_extensions<span class="w"> </span>future<span class="w"> </span>six<span class="w"> </span>requests<span class="w"> </span>dataclasses<span class="w"> </span>-y
conda<span class="w"> </span>install<span class="w"> </span>jemalloc<span class="w"> </span>gperftools<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>-y
conda<span class="w"> </span>install<span class="w"> </span>git-lfs<span class="w"> </span>-y
</pre></div>
</div>
<p>Install other dependencies using pip:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>../../../requirements.txt
</pre></div>
</div>
</section>
<section id="download-models">
<h2>Download Models<a class="headerlink" href="#download-models" title="Link to this heading"></a></h2>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>git-lfs<span class="w"> </span>install
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/Intel/neural-chat-7b-v3-1
</pre></div>
</div>
</section>
</section>
<section id="configure-yaml">
<h1>Configure YAML<a class="headerlink" href="#configure-yaml" title="Link to this heading"></a></h1>
<p>You can customize the configuration file ‘tgi.yaml’ to match your environment setup. Here’s a table to help you understand the configurable options:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Item</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>host</td>
<td>0.0.0.0</td>
</tr>
<tr>
<td>port</td>
<td>8000</td>
</tr>
<tr>
<td>model_name_or_path</td>
<td>"./neural-chat-7b-v3-1"</td>
</tr>
<tr>
<td>device</td>
<td>"cpu"/"gpu"/"hpu"</td>
</tr>
<tr>
<td>serving.framework</td>
<td>"tgi"</td>
</tr>
<tr>
<td>serving.framework.tgi_engine_params.endpoint</td>
<td>Your existed tgi service endpoint. when endpoint is set, neuralchat will not start a tgi service, and other params will not work any more.</td>
</tr>
<tr>
<td>serving.framework.tgi_engine_params.port</td>
<td>9876, the port that neuralchat will help to start tgi service.</td>
</tr>
<tr>
<td>serving.framework.tgi_engine_params.sharded</td>
<td>true (false only on cpu)</td>
</tr>
<tr>
<td>serving.framework.tgi_engine_params.num_shard</td>
<td>4 (not effective when sharded is false)</td>
</tr>
<tr>
<td>serving.framework.tgi_engine_params.habana_visible_devices</td>
<td>"0,1" (only on hpu)</td>
</tr>
</tbody>
</table></section>
<section id="run-the-neuralchat-server-with-tgi-framework">
<h1>Run the NeuralChat server with TGI framework<a class="headerlink" href="#run-the-neuralchat-server-with-tgi-framework" title="Link to this heading"></a></h1>
<p>To start the NeuralChat server with TGI framework, run the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>nohup<span class="w"> </span>bash<span class="w"> </span>run.sh<span class="w"> </span><span class="p">&amp;</span>
</pre></div>
</div>
</section>
<section id="consume-the-services">
<h1>Consume the Services<a class="headerlink" href="#consume-the-services" title="Link to this heading"></a></h1>
<p>After the services are successfully launched, you can consume the HTTP services offered by NeuralChat.</p>
<p>Here is an example of consuming TGI service, remember to substitute your real ip and port.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span><span class="si">${</span><span class="nv">your_ip</span><span class="si">}</span>:<span class="si">${</span><span class="nv">your_port</span><span class="si">}</span>/v1/tgi/generate<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-X<span class="w"> </span>POST<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-d<span class="w"> </span><span class="s1">&#39;{&quot;inputs&quot;:&quot;What is Deep Learning?&quot;,&quot;parameters&quot;:{&quot;max_new_tokens&quot;:17, &quot;do_sample&quot;: true}}&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>-H<span class="w"> </span><span class="s1">&#39;Content-Type: application/json&#39;</span>
</pre></div>
</div>
<p>Of course, you can also consume the service via <code class="docutils literal notranslate"><span class="pre">postman</span></code>, <code class="docutils literal notranslate"><span class="pre">http</span> <span class="pre">request</span></code>, or other ways.</p>
<p>If neuralchat is unable to call your local tgi service, try the command below then try again.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">unset</span><span class="w"> </span>http_proxy
</pre></div>
</div>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f3133afbf40> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>