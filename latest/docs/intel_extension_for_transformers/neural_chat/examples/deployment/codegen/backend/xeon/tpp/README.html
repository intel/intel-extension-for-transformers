<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Setup Conda &mdash; Intel® Extension for Transformers 0.1.dev1+g2434c64 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../../../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Setup Conda</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../../_sources/docs/intel_extension_for_transformers/neural_chat/examples/deployment/codegen/backend/xeon/tpp/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>This README serves as a guide to set up the backend for a code generation chatbot utilizing the NeuralChat framework. You can deploy this code generation chatbot across various platforms, including Intel XEON Scalable Processors, Habana’s Gaudi processors (HPU), Intel Data Center GPU and Client GPU, Nvidia Data Center GPU, and Client GPU.</p>
<p>This code generation chatbot demonstrates how to deploy it specifically on Intel XEON processors using Intel(R) Tensor Processing Primitives extension for PyTorch. To run the 34b or 70b level LLM model, we require implementing model parallelism using multi-node strategy.</p>
<section id="setup-conda">
<h1>Setup Conda<a class="headerlink" href="#setup-conda" title="Link to this heading"></a></h1>
<p>First, you need to install and configure the Conda environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and install Miniconda</span>
wget<span class="w"> </span>https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash<span class="w"> </span>Miniconda*.sh
<span class="nb">source</span><span class="w"> </span>~/.bashrc
conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>demo<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.9
conda<span class="w"> </span>activate<span class="w"> </span>demo
</pre></div>
</div>
</section>
<section id="install-numactl">
<h1>Install numactl<a class="headerlink" href="#install-numactl" title="Link to this heading"></a></h1>
<p>Next, install the numactl library:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>numactl
</pre></div>
</div>
</section>
<section id="install-itrex">
<h1>Install ITREX<a class="headerlink" href="#install-itrex" title="Link to this heading"></a></h1>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/intel/intel-extension-for-transformers.git
<span class="nb">cd</span><span class="w"> </span>./intel-extension-for-transformers/
python<span class="w"> </span>setup.py<span class="w"> </span>install
</pre></div>
</div>
</section>
<section id="install-neuralchat-python-dependencies">
<h1>Install NeuralChat Python Dependencies<a class="headerlink" href="#install-neuralchat-python-dependencies" title="Link to this heading"></a></h1>
<p>Install neuralchat dependencies:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>../../../../../../requirements_cpu.txt
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w">  </span>../../../../../../pipeline/plugins/retrieval/requirements.txt
pip<span class="w"> </span>uninstall<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>intel-extension-for-pytorch<span class="w"> </span>-y
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>torch<span class="w"> </span>torchvision<span class="w"> </span>torchaudio<span class="w"> </span>--index-url<span class="w"> </span>https://download.pytorch.org/whl/cpu
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>intel-extension-for-pytorch
python<span class="w"> </span>-m<span class="w"> </span>pip<span class="w"> </span>install<span class="w"> </span>oneccl_bind_pt<span class="w"> </span>--extra-index-url<span class="w"> </span>https://pytorch-extension.intel.com/release-whl/stable/cpu/us/
pip<span class="w"> </span>install<span class="w"> </span><span class="nv">transformers</span><span class="o">==</span><span class="m">4</span>.31.0<span class="w"> </span><span class="c1"># need to downgrade transformers to 4.31.0 for LLAMA</span>
</pre></div>
</div>
<p>Install Intel MPI:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://registrationcenter-download.intel.com/akdlm/IRC_NAS/749f02a5-acb8-4bbb-91db-501ff80d3f56/l_mpi_oneapi_p_2021.12.0.538.sh
bash<span class="w"> </span>l_mpi_oneapi_p_2021.12.0.538.sh
<span class="nv">torch_ccl_path</span><span class="o">=</span><span class="k">$(</span>python<span class="w"> </span>-c<span class="w"> </span><span class="s2">&quot;import torch; import oneccl_bindings_for_pytorch; import os;  print(os.path.abspath(os.path.dirname(oneccl_bindings_for_pytorch.__file__)))&quot;</span><span class="w"> </span><span class="m">2</span>&gt;<span class="w"> </span>/dev/null<span class="k">)</span>
<span class="nb">source</span><span class="w"> </span><span class="nv">$torch_ccl_path</span>/env/setvars.sh
</pre></div>
</div>
<p>Install Intel(R) Tensor Processing Primitives extension for PyTorch from source code.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install from source code</span>
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/libxsmm/tpp-pytorch-extension.git
<span class="nb">cd</span><span class="w"> </span>tpp-pytorch-extension/
git<span class="w"> </span>checkout<span class="w"> </span>feature_mxfp4_poc
git<span class="w"> </span>submodule<span class="w"> </span>update<span class="w"> </span>--init
python<span class="w"> </span>setup.py<span class="w"> </span>install
</pre></div>
</div>
<p>Currently there are some issues when using BF16, so we need to enable MXFP4 by the below commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">TPP_CACHE_REMAPPED_WEIGHTS</span><span class="o">=</span><span class="m">0</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">USE_MXFP4</span><span class="o">=</span><span class="m">1</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">KV_CACHE_INC_SIZE</span><span class="o">=</span><span class="m">512</span>
</pre></div>
</div>
</section>
<section id="configure-the-codegen-yaml">
<h1>Configure the codegen.yaml<a class="headerlink" href="#configure-the-codegen-yaml" title="Link to this heading"></a></h1>
<p>You can customize the configuration file ‘codegen.yaml’ to match your environment setup. Here’s a table to help you understand the configurable options:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Item</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>host</td>
<td>0.0.0.0</td>
</tr>
<tr>
<td>port</td>
<td>8000</td>
</tr>
<tr>
<td>model_name_or_path</td>
<td>"Phind/Phind-CodeLlama-34B-v2"</td>
</tr>
<tr>
<td>device</td>
<td>"cpu"</td>
</tr>
<tr>
<td>use_tpp</td>
<td>true</td>
</tr>
<tr>
<td>tasks_list</td>
<td>['codegen']</td>
</tr>
</tbody>
</table><p>Note: To switch from code generation to text generation mode, adjust the model_name_or_path settings accordingly, e.g. the model_name_or_path can be set “meta-llama/Llama-2-13b-chat-hf”.</p>
</section>
<section id="using-single-numanode">
<h1>Using Single NumaNode<a class="headerlink" href="#using-single-numanode" title="Link to this heading"></a></h1>
<p>To configure a single NUMA node on Xeon processors, edit the hostfile located at <code class="docutils literal notranslate"><span class="pre">../../../../../../server/config/hostfile</span></code> and set the NUMA node number to 1.</p>
<section id="modify-hostfile">
<h2>Modify hostfile<a class="headerlink" href="#modify-hostfile" title="Link to this heading"></a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>vim<span class="w"> </span>../../../../../../server/config/hostfile
<span class="w">    </span>localhost<span class="w"> </span><span class="nv">slots</span><span class="o">=</span><span class="m">1</span>
</pre></div>
</div>
</section>
<section id="run-the-code-generation-chatbot-server">
<h2>Run the Code Generation Chatbot Server<a class="headerlink" href="#run-the-code-generation-chatbot-server" title="Link to this heading"></a></h2>
<p>To start the code-generating chatbot server, use the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh
</pre></div>
</div>
</section>
</section>
<section id="configure-multi-numanodes">
<h1>Configure Multi-NumaNodes<a class="headerlink" href="#configure-multi-numanodes" title="Link to this heading"></a></h1>
<p>To utilize multi-socket model parallelism on Xeon servers, you’ll need to adjust the hostfile settings.
For instance, to allocate 3 NUMA nodes on a single socket of the GNR server, modify the hostfile as shown below:</p>
<section id="id1">
<h2>Modify hostfile<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>vim<span class="w"> </span>../../../../../../server/config/hostfile
<span class="w">    </span>localhost<span class="w"> </span><span class="nv">slots</span><span class="o">=</span><span class="m">3</span>
</pre></div>
</div>
<p>Afterward, run the run.sh script as previously instructed.</p>
</section>
</section>
<section id="configure-multi-nodes">
<h1>Configure Multi-Nodes<a class="headerlink" href="#configure-multi-nodes" title="Link to this heading"></a></h1>
<p>To use the multi-node model parallelism with Xeon servers, you need to configure a hostfile first and make sure ssh is able between your servers.
For example, you have two servers which have the IP of <code class="docutils literal notranslate"><span class="pre">192.168.1.1</span></code> and <code class="docutils literal notranslate"><span class="pre">192.168.1.2</span></code>, and each of it has 3 numa nodes on single socket.</p>
<section id="id2">
<h2>Modify hostfile<a class="headerlink" href="#id2" title="Link to this heading"></a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>vim<span class="w"> </span>../../../../../../server/config/hostfile
<span class="w">    </span><span class="m">192</span>.168.1.1<span class="w"> </span><span class="nv">slots</span><span class="o">=</span><span class="m">3</span>
<span class="w">    </span><span class="m">192</span>.168.1.2<span class="w"> </span><span class="nv">slots</span><span class="o">=</span><span class="m">3</span>
</pre></div>
</div>
</section>
<section id="configure-ssh-between-servers">
<h2>Configure SSH between Servers<a class="headerlink" href="#configure-ssh-between-servers" title="Link to this heading"></a></h2>
<p>In order to enable this hostfile, you have to make sure <code class="docutils literal notranslate"><span class="pre">192.168.1.1</span></code> and <code class="docutils literal notranslate"><span class="pre">192.168.1.2</span></code> are able to access each other via SSH. Check it with <code class="docutils literal notranslate"><span class="pre">ssh</span> <span class="pre">192.168.1.2</span></code>.</p>
<p>If your servers are not available with SSH, follow the instructions below.</p>
<ol>
<li><p>Generate SSH Key
Execute this command on both servers to generate ssh key in  <code class="docutils literal notranslate"><span class="pre">~/.ssh/</span></code>.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh-keygen
</pre></div>
</div>
</li>
<li><p>Copy SSH Key
Execute this command on both servers. Specify your user on the server.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh-copy-id<span class="w"> </span>user@192.168.1.2
</pre></div>
</div>
</li>
<li><p>Test SSH Connection
Test whether SSH is available now.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ssh<span class="w"> </span>user@192.168.1.2
</pre></div>
</div>
</li>
<li><p>Check Network
Check network communication between servers.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>ping<span class="w"> </span><span class="m">192</span>.168.1.2
</pre></div>
</div>
</li>
</ol>
<p>If you cannot SSH to your local server via IP, configure it with localhost as below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>cat<span class="w"> </span>~/.ssh/id_rsa.pub<span class="w"> </span>&gt;&gt;<span class="w"> </span>~/.ssh/authorized_keys
chmod<span class="w"> </span><span class="m">700</span><span class="w"> </span>~/.ssh
chmod<span class="w"> </span><span class="m">600</span><span class="w"> </span>~/.ssh/authorized_keys
</pre></div>
</div>
<p>Then modify hostfile as below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>localhost<span class="w"> </span><span class="nv">slots</span><span class="o">=</span><span class="m">3</span>
<span class="m">192</span>.168.1.2<span class="w"> </span><span class="nv">slots</span><span class="o">=</span><span class="m">3</span>
</pre></div>
</div>
</section>
<section id="id3">
<h2>Run the Code Generation Chatbot Server<a class="headerlink" href="#id3" title="Link to this heading"></a></h2>
<p>Before running the code-generating chatbot server, make sure you have already deploy the same conda environment and intel-extension-for-tranformers codes on both servers.</p>
<p>To start the code-generating chatbot server, use the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7fae3155b070> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>