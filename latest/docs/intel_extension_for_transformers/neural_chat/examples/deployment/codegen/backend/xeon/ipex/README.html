<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Setup Conda &mdash; Intel® Extension for Transformers 0.1.dev1+g8722443 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../../../../../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../../../../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../../legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../../../../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Setup Conda</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../../../../_sources/docs/intel_extension_for_transformers/neural_chat/examples/deployment/codegen/backend/xeon/ipex/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <p>This README serves as a guide to set up the backend for a code generation chatbot utilizing the NeuralChat framework. You can deploy this code generation chatbot across various platforms, including Intel XEON Scalable Processors, Habana’s Gaudi processors (HPU), Intel Data Center GPU and Client GPU, Nvidia Data Center GPU, and Client GPU.</p>
<p>This code generation chatbot demonstrates how to deploy it specifically on Intel XEON processors using <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">intel exteion for pytorch</a> BFloat16 optimization.</p>
<section id="setup-conda">
<h1>Setup Conda<a class="headerlink" href="#setup-conda" title="Link to this heading"></a></h1>
<p>First, you need to install and configure the Conda environment:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Download and install Miniconda</span>
wget<span class="w"> </span>https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash<span class="w"> </span>Miniconda*.sh
<span class="nb">source</span><span class="w"> </span>~/.bashrc
conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>demo<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.9
conda<span class="w"> </span>activate<span class="w"> </span>demo
</pre></div>
</div>
</section>
<section id="install-numactl">
<h1>Install numactl<a class="headerlink" href="#install-numactl" title="Link to this heading"></a></h1>
<p>Next, install the numactl library:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>sudo<span class="w"> </span>apt<span class="w"> </span>install<span class="w"> </span>numactl
</pre></div>
</div>
</section>
<section id="install-itrex">
<h1>Install ITREX<a class="headerlink" href="#install-itrex" title="Link to this heading"></a></h1>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/intel/intel-extension-for-transformers.git
<span class="nb">cd</span><span class="w"> </span>./intel-extension-for-transformers/
python<span class="w"> </span>setup.py<span class="w"> </span>install
</pre></div>
</div>
</section>
<section id="install-neuralchat-python-dependencies">
<h1>Install NeuralChat Python Dependencies<a class="headerlink" href="#install-neuralchat-python-dependencies" title="Link to this heading"></a></h1>
<p>Install neuralchat dependencies:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>../../../../../../requirements_cpu.txt
</pre></div>
</div>
</section>
<section id="install-python-dependencies">
<h1>Install Python dependencies<a class="headerlink" href="#install-python-dependencies" title="Link to this heading"></a></h1>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>astunparse<span class="w"> </span>ninja<span class="w"> </span>pyyaml<span class="w"> </span>mkl<span class="w"> </span>mkl-include<span class="w"> </span>setuptools<span class="w"> </span>cmake<span class="w"> </span>cffi<span class="w"> </span>typing_extensions<span class="w"> </span>future<span class="w"> </span>six<span class="w"> </span>requests<span class="w"> </span>dataclasses<span class="w"> </span>-y
conda<span class="w"> </span>install<span class="w"> </span>jemalloc<span class="w"> </span>gperftools<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>-y
</pre></div>
</div>
</section>
<section id="configure-the-codegen-yaml">
<h1>Configure the codegen.yaml<a class="headerlink" href="#configure-the-codegen-yaml" title="Link to this heading"></a></h1>
<p>You can customize the configuration file ‘codegen.yaml’ to match your environment setup. Here’s a table to help you understand the configurable options:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Item</th>
<th>Value</th>
</tr>
</thead>
<tbody>
<tr>
<td>host</td>
<td>0.0.0.0</td>
</tr>
<tr>
<td>port</td>
<td>8000</td>
</tr>
<tr>
<td>model_name_or_path</td>
<td>"ise-uiuc/Magicoder-S-DS-6.7B"</td>
</tr>
<tr>
<td>device</td>
<td>"cpu"</td>
</tr>
<tr>
<td>tasks_list</td>
<td>['codegen']</td>
</tr>
</tbody>
</table><p>Note: To switch from code generation to text generation mode, adjust the model_name_or_path settings accordingly, e.g. the model_name_or_path can be set “Intel/neural-chat-7b-v3-3”.</p>
</section>
<section id="run-the-code-generation-chatbot-server">
<h1>Run the Code Generation Chatbot Server<a class="headerlink" href="#run-the-code-generation-chatbot-server" title="Link to this heading"></a></h1>
<p>To start the code-generating chatbot server, use the following command:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>bash<span class="w"> </span>run.sh
</pre></div>
</div>
<p>Note: Please adapt the core number in the commands <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">OMP_NUM_THREADS=48</span></code> and <code class="docutils literal notranslate"><span class="pre">numactl</span> <span class="pre">-l</span> <span class="pre">-C</span> <span class="pre">0-47</span> <span class="pre">python</span> <span class="pre">-m</span> <span class="pre">run_code_gen</span></code> based on your CPU specifications for the <code class="docutils literal notranslate"><span class="pre">run.sh</span></code> script, which can be checked using <code class="docutils literal notranslate"><span class="pre">lscpu</span></code>.</p>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f168a853310> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>