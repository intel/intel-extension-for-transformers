<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NeuralChat &mdash; IntelÂ® Extension for Transformers 0.1.dev1+gdd6dcb4 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../_static/graphviz.css?v=eafc0fe6" />
      <link rel="stylesheet" type="text/css" href="../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            IntelÂ® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../versions.html">latestâ–¼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">IntelÂ® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">NeuralChat</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/docs/intel_extension_for_transformers/neural_chat/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div align="center"><section id="neuralchat">
<h1>NeuralChat<a class="headerlink" href="#neuralchat" title="Link to this heading">ïƒ</a></h1>
<p><h3> A customizable framework to create your own LLM-driven AI apps within minutes</h3></p>
<p>ğŸŒŸ<a class="reference external" href="./docs/neuralchat_api.html">RESTful API</a>Â Â Â |Â Â Â ğŸ’»<a class="reference external" href="./examples">Examples</a>Â Â Â |Â Â Â ğŸ“–<a class="reference external" href="./docs/full_notebooks.html">Notebooks</a></p>
</div></section>
<section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">ïƒ</a></h1>
<p>NeuralChat is a powerful and flexible open framework that empowers you to effortlessly create LLM-centric AI applications, including chatbots and copilots.</p>
<ul class="simple">
<li><p>Support a range of hardware like <a class="reference external" href="https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html">Intel Xeon Scalable processors</a>, <a class="reference external" href="https://habana.ai/products">Intel Gaudi AI processors</a>, <a class="reference external" href="https://www.intel.com/content/www/us/en/products/details/discrete-gpus/data-center-gpu/max-series.html">IntelÂ® Data Center GPU Max Series</a> and NVidia GPUs</p></li>
<li><p>Leverage the leading AI frameworks (e.g., <a class="reference external" href="https://pytorch.org/">PyTorch</a> and popular domain libraries (e.g., <a class="reference external" href="https://github.com/huggingface">Hugging Face</a>, <a class="reference external" href="https://www.langchain.com/">Langchain</a>) with their extensions</p></li>
<li><p>Support the model customizations through parameter-efficient fine-tuning, quantization, and sparsity. Released <a class="reference external" href="https://huggingface.co/Intel/neural-chat-7b-v3-1">Intel NeuralChat-7B LLM</a>, ranking #1 in Hugging Face open LLM leaderboard in Novâ€™23</p></li>
<li><p>Provide a rich set of plugins that can augment the AI applications through retrieval-augmented generation (RAG) (e.g., <a class="reference external" href="https://github.com/IntelLabs/fastRAG/tree/main">fastRAG</a>), content moderation, query caching, more</p></li>
<li><p>Integrate with popular serving frameworks (e.g., <a class="reference external" href="https://github.com/vllm-project/vllm">vLLM</a>, <a class="reference external" href="https://github.com/huggingface/text-generation-inference">TGI</a>, <a class="reference external" href="https://developer.nvidia.com/triton-inference-server">Triton</a>). Support <a class="reference external" href="https://platform.openai.com/docs/introduction">OpenAI</a>-compatible API to simplify the creation or migration of AI applications</p></li>
</ul>
<a target="_blank" href="./docs/images/neuralchat_arch.png">
<p align="center">
  <img src="./docs/images/neuralchat_arch.png" alt="NeuralChat" width=600 height=340>
</p>
</a><blockquote>
<div><p>NeuralChat is under active development. APIs are subject to change.</p>
</div></blockquote>
</section>
<section id="installation">
<h1>Installation<a class="headerlink" href="#installation" title="Link to this heading">ïƒ</a></h1>
<p>NeuralChat is under Intel Extension for Transformers, so ensure the installation of Intel Extension for Transformers first by following the <a class="reference external" href="../../docs/installation.html">installation</a>. After that, install additional dependency for NeuralChat per your device:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># For CPU device</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements_cpu.txt

<span class="c1"># For HPU device</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements_hpu.txt

<span class="c1"># For XPU device</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements_xpu.txt

<span class="c1"># For CUDA device</span>
pip<span class="w"> </span>install<span class="w"> </span>-r<span class="w"> </span>requirements.txt
</pre></div>
</div>
</section>
<section id="getting-started">
<h1>Getting Started<a class="headerlink" href="#getting-started" title="Link to this heading">ïƒ</a></h1>
<section id="openai-compatible-restful-apis">
<h2>OpenAI-Compatible RESTful APIs<a class="headerlink" href="#openai-compatible-restful-apis" title="Link to this heading">ïƒ</a></h2>
<p>NeuralChat provides OpenAI-compatible RESTful APIs for LLM inference, so you can use NeuralChat as a drop-in replacement for OpenAI APIs. NeuralChat service can also be accessible through <a class="reference external" href="https://github.com/openai/openai-python">OpenAI client library</a>, <code class="docutils literal notranslate"><span class="pre">curl</span></code> commands, and <code class="docutils literal notranslate"><span class="pre">requests</span></code> library. See <a class="reference external" href="./docs/neuralchat_api.html">neuralchat_api.html</a>.</p>
<section id="launch-openai-compatible-service">
<h3>Launch OpenAI-compatible Service<a class="headerlink" href="#launch-openai-compatible-service" title="Link to this heading">ïƒ</a></h3>
<p>NeuralChat launches a chatbot service using <a class="reference external" href="https://huggingface.co/Intel/neural-chat-7b-v3-1">Intel/neural-chat-7b-v3-1</a> by default. You can customize the chatbot service by configuring the YAML file.</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>neuralchat_server<span class="w"> </span>start<span class="w"> </span>--config_file<span class="w"> </span>./server/config/neuralchat.yaml
</pre></div>
</div>
</section>
<section id="access-the-service">
<h3>Access the Service<a class="headerlink" href="#access-the-service" title="Link to this heading">ïƒ</a></h3>
<p>Once the service is running, you can observe an OpenAI-compatible endpoint <code class="docutils literal notranslate"><span class="pre">/v1/chat/completions</span></code>. You can use any of below ways to access the endpoint.</p>
<section id="using-openai-client-library">
<h4>Using OpenAI Client Library<a class="headerlink" href="#using-openai-client-library" title="Link to this heading">ïƒ</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">openai</span> <span class="kn">import</span> <span class="n">Client</span>
<span class="c1"># Replace &#39;your_api_key&#39; with your actual OpenAI API key</span>
<span class="n">api_key</span> <span class="o">=</span> <span class="s1">&#39;your_api_key&#39;</span>
<span class="n">backend_url</span> <span class="o">=</span> <span class="s1">&#39;http://127.0.0.1:80/v1/chat/completions&#39;</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">Client</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span> <span class="n">base_url</span><span class="o">=</span><span class="n">backend_url</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">ChatCompletion</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
      <span class="n">model</span><span class="o">=</span><span class="s2">&quot;Intel/neural-chat-7b-v3-1&quot;</span><span class="p">,</span>
      <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
          <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;system&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;You are a helpful assistant.&quot;</span><span class="p">},</span>
          <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Tell me about Intel Xeon Scalable Processors.&quot;</span><span class="p">},</span>
      <span class="p">]</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-curl">
<h4>Using Curl<a class="headerlink" href="#using-curl" title="Link to this heading">ïƒ</a></h4>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://127.0.0.1:80/v1/chat/completions<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;model&quot;: &quot;Intel/neural-chat-7b-v3-1&quot;,</span>
<span class="s1">    &quot;messages&quot;: [</span>
<span class="s1">    {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;},</span>
<span class="s1">    {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me about Intel Xeon Scalable Processors.&quot;}</span>
<span class="s1">    ]</span>
<span class="s1">    }&#39;</span>
</pre></div>
</div>
</section>
<section id="using-python-requests-library">
<h4>Using Python Requests Library<a class="headerlink" href="#using-python-requests-library" title="Link to this heading">ïƒ</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">requests</span>
<span class="n">url</span> <span class="o">=</span> <span class="s1">&#39;http://127.0.0.1:80/v1/chat/completions&#39;</span>
<span class="n">headers</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Content-Type&#39;</span><span class="p">:</span> <span class="s1">&#39;application/json&#39;</span><span class="p">}</span>
<span class="n">data</span> <span class="o">=</span> <span class="s1">&#39;{&quot;model&quot;: &quot;Intel/neural-chat-7b-v3-1&quot;, &quot;messages&quot;: [ </span><span class="se">\</span>
<span class="s1">          {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;}, </span><span class="se">\</span>
<span class="s1">          {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;Tell me about Intel Xeon Scalable Processors.&quot;}] </span><span class="se">\</span>
<span class="s1">       }&#39;</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">post</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">headers</span><span class="o">=</span><span class="n">headers</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">())</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="langchain-extension-apis">
<h2>Langchain Extension APIs<a class="headerlink" href="#langchain-extension-apis" title="Link to this heading">ïƒ</a></h2>
<p>Intel Extension for Transformers provides a comprehensive suite of Langchain-based extension APIs, including advanced retrievers, embedding models, and vector stores. These enhancements are carefully crafted to expand the capabilities of the original langchain API, ultimately boosting overall performance. This extension is specifically tailored to enhance the functionality and performance of RAG.</p>
<section id="vector-stores">
<h3>Vector Stores<a class="headerlink" href="#vector-stores" title="Link to this heading">ïƒ</a></h3>
<p>We introduce enhanced vector store operations, enabling users to adjust and fine-tune their settings even after the chatbot has been initialized, offering a more adaptable and user-friendly experience. For langchain users, integrating and utilizing optimized Vector Stores is straightforward by replacing the original Chroma API in langchain.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_community.llms.huggingface_pipeline</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>
<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="kn">from</span> <span class="nn">langchain_core.vectorstores</span> <span class="kn">import</span> <span class="n">VectorStoreRetriever</span>
<span class="kn">from</span> <span class="nn">intel_extension_for_transformers.langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">VectorStoreRetriever</span><span class="p">(</span><span class="n">vectorstore</span><span class="o">=</span><span class="n">Chroma</span><span class="p">(</span><span class="o">...</span><span class="p">))</span>
<span class="n">retrievalQA</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">HuggingFacePipeline</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="n">retriever</span><span class="o">=</span><span class="n">retriever</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="retrievers">
<h3>Retrievers<a class="headerlink" href="#retrievers" title="Link to this heading">ïƒ</a></h3>
<p>We provide optimized retrievers such as <code class="docutils literal notranslate"><span class="pre">VectorStoreRetriever</span></code>, <code class="docutils literal notranslate"><span class="pre">ChildParentRetriever</span></code> to efficiently handle vectorstore operations, ensuring optimal retrieval performance.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">intel_extension_for_transformers.langchain.retrievers</span> <span class="kn">import</span> <span class="n">ChildParentRetriever</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">ChildParentRetriever</span><span class="p">(</span><span class="n">vectorstore</span><span class="o">=</span><span class="n">Chroma</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">child_documents</span><span class="p">),</span> <span class="n">parentstore</span><span class="o">=</span><span class="n">Chroma</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">parent_documents</span><span class="p">),</span> <span class="n">search_type</span><span class="o">=</span><span class="n">xxx</span><span class="p">,</span> <span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="o">...</span><span class="p">})</span>
<span class="n">docs</span><span class="o">=</span><span class="n">retriever</span><span class="o">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="s2">&quot;Intel&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Please refer to this <a class="reference external" href="./pipeline/plugins/retrieval/README.html">documentation</a> for more details.</p>
</section>
</section>
<section id="advanced-features">
<h2>Advanced Features<a class="headerlink" href="#advanced-features" title="Link to this heading">ïƒ</a></h2>
<p>NeuralChat introduces <code class="docutils literal notranslate"><span class="pre">plugins</span></code> that offer a wide range of useful LLM utilities and features, enhancing the capabilities of the chatbot. Additionally, NeuralChat provides advanced model optimization technologies such as <code class="docutils literal notranslate"><span class="pre">Automatic</span> <span class="pre">Mixed</span> <span class="pre">Precision</span> <span class="pre">(AMP)</span></code> and <code class="docutils literal notranslate"><span class="pre">Weight</span> <span class="pre">Only</span> <span class="pre">Quantization</span></code>. These technologies enable users to run a high-throughput chatbot efficiently. NeuralChat further supports fine-tuning the pretrained LLMs for tasks such as text generation, summarization, code generation, and even Text-to-Speech (TTS) models, allowing users to create customized chatbots tailored to their specific needs.</p>
<p>Please refer to this <a class="reference external" href="./docs/advanced_features.html">documentation</a> for more details.</p>
</section>
</section>
<section id="models">
<h1>Models<a class="headerlink" href="#models" title="Link to this heading">ïƒ</a></h1>
<section id="supported-models">
<h2>Supported  Models<a class="headerlink" href="#supported-models" title="Link to this heading">ïƒ</a></h2>
<p>The table below displays the validated model list in NeuralChat for both inference and fine-tuning.
|Pretrained model| Text Generation (Completions) | Text Generation (Chat Completions) | Summarization | Code Generation |
|â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”|:â€”:|:â€”:|:â€”:|:â€”:|
|Intel/neural-chat-7b-v1-1| âœ…| âœ…| âœ…| âœ…    |
|Intel/neural-chat-7b-v3-1| âœ…| âœ…| âœ…| âœ…    |
|LLaMA series| âœ…| âœ…|âœ…| âœ…    |
|LLaMA2 series| âœ…| âœ…|âœ…| âœ…    |
|GPT-J| âœ…| âœ…|âœ…| âœ…    |
|MPT series| âœ…| âœ…|âœ…| âœ…    |
|Mistral series| âœ…| âœ…|âœ…| âœ…    |
|Mixtral series| âœ…| âœ…|âœ…| âœ…    |
|SOLAR Series| âœ…| âœ…|âœ…| âœ…    |
|ChatGLM series| âœ…| âœ…|âœ…| âœ…    |
|Qwen series| âœ…| âœ…|âœ…| âœ…    |
|StarCoder series|   |   |   | âœ… |
|CodeLLaMA series|   |   |   | âœ… |
|CodeGen series|   |   |   | âœ… |
|MagicCoder series|   |   |   | âœ… |</p>
</section>
</section>
<section id="notebooks">
<h1>Notebooks<a class="headerlink" href="#notebooks" title="Link to this heading">ïƒ</a></h1>
<p>We provide Jupyter notebooks to help users explore how to create, deploy, and customize chatbots on different hardware architecture. The selected notebooks are shown below:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Notebook</th>
<th>Title</th>
<th>Description</th>
<th>Link</th>
</tr>
</thead>
<tbody>
<tr>
<td>#1</td>
<td>Getting Started on Intel CPU SPR</td>
<td>Learn how to create chatbot on SPR</td>
<td><a href="./docs/notebooks/build_chatbot_on_spr.ipynb">Notebook</a></td>
</tr>
<tr>
<td>#2</td>
<td>Getting Started on Habana Gaudi1/Gaudi2</td>
<td>Learn how to create chatbot on Habana Gaudi1/Gaudi2</td>
<td><a href="./docs/notebooks/build_chatbot_on_habana_gaudi.ipynb">Notebook</a></td>
</tr>
<tr>
<td>#3</td>
<td>Deploying Chatbot on Intel CPU SPR</td>
<td>Learn how to deploy chatbot on SPR</td>
<td><a href="./docs/notebooks/deploy_chatbot_on_spr.ipynb">Notebook</a></td>
</tr>
<tr>
<td>#4</td>
<td>Deploying Chatbot on Habana Gaudi1/Gaudi2</td>
<td>Learn how to deploy chatbot on Habana Gaudi1/Gaudi2</td>
<td><a href="./docs/notebooks/deploy_chatbot_on_habana_gaudi.ipynb">Notebook</a></td>
</tr>
<tr>
<td>#5</td>
<td>Deploying Chatbot with Load Balance</td>
<td>Learn how to deploy chatbot with load balance on SPR</td>
<td><a href="./docs/notebooks/chatbot_with_load_balance.ipynb">Notebook</a></td>
</tr>
</tbody>
</table><p>ğŸŒŸPlease refer to <a class="reference external" href="docs/full_notebooks.html">HERE</a> for the full notebooks.</p>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, IntelÂ® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7feb65d54d90> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>