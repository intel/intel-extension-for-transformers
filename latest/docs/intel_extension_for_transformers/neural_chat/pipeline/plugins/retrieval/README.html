<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../../../../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Introduction &mdash; Intel® Extension for Transformers 0.1.dev1+ga864bb2 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/graphviz.css?v=fd3f3429" />
      <link rel="stylesheet" type="text/css" href="../../../../../../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../../../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../../../../../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../user_guide.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../example.html">Example</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html">OpenSSF Badge</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../SECURITY.html#security-policy">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../../../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Introduction</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../../../_sources/docs/intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval/README.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <div align="center">
<h1>Retrieval</h3>
<div align="left"><section id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h1>
<p>Large language models (LLMs) have shown exceptional performance in various natural language processing tasks, establishing them as essential tools for understanding and generating language. These models absorb extensive world knowledge from their vast training datasets, enabling them to produce fluent and coherent responses to user queries without external data resources. However, despite the remarkable breadth of knowledge large language models gain during training, they face limitations in accessing up-to-date information and certain domain-specific data. This limitation can lead to a significant concern: the tendency of large language models to ‘hallucinate’ or create fictitious content in their responses.</p>
<p>This hallucination problem primarily arises from two factors: (1) LLMs are predominantly trained using data from the Internet, which limits their exposure to specific, domain-focused information; and (2) LLMs mainly rely on the training corpus for information extraction. These models remain unaware of events occurring post-training, which can be particularly problematic for topics that change daily. Two methods are recognized as effective method for model hallucination problems, <a class="reference external" href="https://arxiv.org/abs/2311.08401">Finetuning the LLM on task-specific datasets</a> and <a class="reference external" href="https://arxiv.org/abs/2212.10560">Retrieval-Augmented Generation (RAG)</a>. However, finetuning a LLM is impractical for most users due to it requires high-quality datasets, labor-intensive data annotation, and substantial computational resources. Also, it is challenging to collect and maintain an extensive, up-to-date knowledge corpus. Therefore, we propose an economically efficient alternative based on RAG. It retrieves relevant documents from a local database to serve as the reference to enhance the accuracy and reliability of the generated results.</p>
<p>Inspired by the prevent chatbot framework <a class="reference external" href="https://github.com/langchain-ai/langchain">langchain</a>, <a class="reference external" href="https://github.com/run-llama/llama_index">Llama-Index</a> and <a class="reference external" href="https://github.com/deepset-ai/haystack">haystack</a>, our NeuralChat API offers an easy way to create and utilize chatbot models while integrating RAG. Our API provides an easy to use extension for langchain users as well as a convenient deployment code for the general user. Without too much learning effort, the user can build their own RAG-based chatbot with their documents. The details about our langchain extension feature could be see <a class="reference external" href="#langchain-extension">here</a>.</p>
<p>Currently, we concentrate on <a class="reference external" href="https://medium.com/&#64;aikho/deep-learning-in-information-retrieval-part-ii-dense-retrieval-1f9fecb47de9">dense retrieval</a> to construct the RAG pipeline. The dense retrieval will return the documents that share the similar semantic expression with the candidate queries instead of the keywords expression, which is more suitable for the long-context application scenario.</p>
<p>The embedding model plays a crucial factor to influence the retrieval accuracy. We have already provided support for a wide range of open-released pre-trained embedding models featured on the <a class="reference external" href="https://huggingface.co/spaces/mteb/leaderboard">HuggingFace text embedding leaderboard</a>. Users can conveniently choose an embedding model in two ways: they can either specify the model by its name on HuggingFace or download a model and save it under the default name. Below is a list of some supported embedding models available in our plugin. Users can select their preferred embedding model based on various factors such as model size, embedding dimensions, maximum sequence length, and average ranking score.
|  Model   | Model Size (GB)  |Embedding Dimensions  |Max Sequence Length  |Average Ranking Score  |
|  :—-:  | :—-:  | :—-:  | :—-: |:—-: |
| <a class="reference external" href="https://huggingface.co/BAAI/bge-large-en-v1.5">bge-large-en-v1.5</a>  | 1.34 |1024  |512  |64.23|
| <a class="reference external" href="https://huggingface.co/BAAI/bge-base-en-v1.5">bge-base-en-v1.5</a>  | 0.44 |768  |512  |63.55|
| <a class="reference external" href="https://huggingface.co/thenlper/gte-large">	gte-large</a>  | 0.67 |1024  |512  |63.13|
| <a class="reference external" href="https://huggingface.co/infgrad/stella-base-en-v2">stella-base-en-v2</a>  | 0.22 |768  |512 |62.61|
| <a class="reference external" href="https://huggingface.co/thenlper/gte-base">gte-base</a>  | 0.44 |768  |512  |62.39|
| <a class="reference external" href="https://huggingface.co/intfloat/e5-large-v2">	e5-large-v2</a>  | 1.34 |1024  |512  |62.25|
| <a class="reference external" href="https://huggingface.co/hkunlp/instructor-xl">instructor-xl</a>  | 4.96 |768  |512  |61.79|
| <a class="reference external" href="https://huggingface.co/hkunlp/instructor-large">instructor-large</a>  | 1.34 |768  |512  |61.59|</p>
<p>In addition, our plugin seamlessly integrates the online embedding model, Google Palm2 embedding. To set up this feature, please follow the <a class="reference external" href="https://developers.generativeai.google/tutorials/embeddings_quickstart">Google official guideline</a> to obtain your API key. Once you have your API key, you can activate the Palm2 embedding service by setting the <code class="docutils literal notranslate"><span class="pre">embedding_model</span></code> parameter to ‘Google’.</p>
<blockquote>
<div><p>Due to the recent code refactorization of <code class="docutils literal notranslate"><span class="pre">sentence-transformers</span></code> will impact the operation behaviour of the embedding models, please check and install the latest <code class="docutils literal notranslate"><span class="pre">sentence-transformers</span></code> from source!</p>
</div></blockquote>
<p>This plugin streamlines three key processes: parsing documents, identifying user intentions, and fetching relevant information. Initially, the <code class="docutils literal notranslate"><span class="pre">Agent_QA</span></code> sets itself up by building a local database from the data at input_path. In the midst of a conversation, when a user poses a question, it first goes through the <code class="docutils literal notranslate"><span class="pre">IntentDetector</span></code>. This step is crucial to figure out if the user is just making casual conversation or looking for specific information. If the IntentDetector concludes that the user is seeking an answer, it triggers the <code class="docutils literal notranslate"><span class="pre">retrieval</span></code> process. This involves scouring the database with the user’s question to find pertinent information. The information thus obtained forms the basis for crafting responses with the help of Large Language Models (LLMs).</p>
<p>To ensure a smooth experience, we’ve made sure this plugin is compatible with common file formats like xlsx, csv, and json/jsonl. It’s important to note that these files need to follow a specific structure for optimal functioning.
|  File Type   | Predefined Structure  |
|  :—-:  | :—-:  |
| xlsx  | [’Questions’, ‘Answers’]<br>[’question’, ‘answer’, ‘link’]<br>[’context’, ‘link’] |
| csv  | [’question’, ‘correct_answer’] |
| json/jsonl  | {’content’:xxx, ‘link’:xxx}|
| txt  | No format required |
| html  | No format required |
| markdown  | No format required |
| word  | No format required |
| pdf  | No format required |</p>
</section>
<section id="usage">
<h1>Usage<a class="headerlink" href="#usage" title="Link to this heading"></a></h1>
<p>Before using RAG in NeuralChat, please install the necessary dependencies in <a class="reference external" href="https://github.com/intel/intel-extension-for-transformers/blob/main/intel_extension_for_transformers/neural_chat/pipeline/plugins/retrieval/requirements.txt">requirements.txt</a> to avoid the import errors. The most convenient way to use is this plugin is via our <code class="docutils literal notranslate"><span class="pre">build_chatbot</span></code> api as introduced in the <a class="reference external" href="https://github.com/intel/intel-extension-for-transformers/tree/main/intel_extension_for_transformers/neural_chat/examples/plugins/retrieval">example code</a>. The user could refer to it for a simple test.</p>
<p>We support multiple file formats for retrieval, including unstructured file formats such as pdf, docx, html, txt, and markdown, as well as structured file formats like jsonl/json, csv, xlsx. For structured file formats, they must adhere to predefined structures. We also support to upload the knowledge base via a http web link.</p>
<p>In the case of jsonl files, they should be formatted as dictionaries, such as: {’content’:xxx, ‘link’:xxx}. The support for xlsx files is specifically designed for Question-Answer (QA) tasks. Users can input QA pairs for retrieval. Therefore, the table’s header should include items labeled as “Question” and “Answer”. The reference files could be found <a class="reference external" href="https://github.com/intel/intel-extension-for-transformers/tree/main/intel_extension_for_transformers/neural_chat/assets/docs">here</a>.</p>
<section id="import-the-module-and-set-the-retrieval-config">
<h2>Import the module and set the retrieval config:<a class="headerlink" href="#import-the-module-and-set-the-retrieval-config" title="Link to this heading"></a></h2>
<blockquote>
<div><p>The user can download the <a class="reference external" href="https://d1io3yog0oux5.cloudfront.net/_897efe2d574a132883f198f2b119aa39/intel/db/888/8941/file/412439%281%29_12_Intel_AR_WR.pdf">Intel 2022 Annual Report</a> for a quick test.</p>
</div></blockquote>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">intel_extension_for_transformers.neural_chat</span> <span class="kn">import</span> <span class="n">PipelineConfig</span>
<span class="kn">from</span> <span class="nn">intel_extension_for_transformers.neural_chat</span> <span class="kn">import</span> <span class="n">plugins</span>
<span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">enable</span><span class="o">=</span><span class="kc">True</span>
<span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;input_path&quot;</span><span class="p">]</span><span class="o">=</span><span class="s2">&quot;./Annual_report.pdf&quot;</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">PipelineConfig</span><span class="p">(</span><span class="n">plugins</span><span class="o">=</span><span class="n">plugins</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="build-the-chatbot-and-interact-with-the-chatbot">
<h2>Build the chatbot and interact with the chatbot:<a class="headerlink" href="#build-the-chatbot-and-interact-with-the-chatbot" title="Link to this heading"></a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">intel_extension_for_transformers.neural_chat</span> <span class="kn">import</span> <span class="n">build_chatbot</span>
<span class="n">chatbot</span> <span class="o">=</span> <span class="n">build_chatbot</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">chatbot</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="s2">&quot;What is IDM 2.0?&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Checkout the full example <a class="reference external" href="../../../examples/retrieval/retrieval_chat.py">retrieval_chat.py</a> and have a try!</p>
</section>
</section>
<section id="parameters">
<h1>Parameters<a class="headerlink" href="#parameters" title="Link to this heading"></a></h1>
<p>Users have the flexibility to tailor the retrieval configuration to meet their individual needs and adapt to their local files. To customize a particular aspect of the retrieval plugin, you can adjust its settings as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;xxx&quot;</span><span class="p">]</span><span class="o">=</span><span class="n">xxx</span>
</pre></div>
</div>
<p>Below are the description for the available parameters in <code class="docutils literal notranslate"><span class="pre">agent_QA</span></code>,</p>
<table border="1" class="docutils">
<thead>
<tr>
<th>Parameters</th>
<th>Type</th>
<th>Description</th>
<th>Options</th>
</tr>
</thead>
<tbody>
<tr>
<td>vector_database</td>
<td>str</td>
<td>The vector database for constructing the knowledge base.</td>
<td>"Chroma", "Qdrant"</td>
</tr>
<tr>
<td>input_path</td>
<td>str</td>
<td>The path of the file/folder/link of the content to formulate the knowledge base</td>
<td>-</td>
</tr>
<tr>
<td>embedding_model</td>
<td>str</td>
<td>The name or path for the text embedding model</td>
<td>-</td>
</tr>
<tr>
<td>response_template</td>
<td>str</td>
<td>Default response when there is no available relevant documents for RAG</td>
<td>-</td>
</tr>
<tr>
<td>mode</td>
<td>str</td>
<td>The RAG behavior for different use case. Please check <a href="#rag-mode">here</a></td>
<td>"accuracy", "general"</td>
</tr>
<tr>
<td>retrieval_type</td>
<td>str</td>
<td>The type of the retriever. Please check <a href="#retrievers">here</a> for more details</td>
<td>"default", "child_parent", "bm25"</td>
</tr>
<tr>
<td>process</td>
<td>bool</td>
<td>Whether to split the long documents into small chucks. The size of each chuck is defined by <code>max_chuck_size</code> and <code>min_chuck_size</code></td>
<td>True, False</td>
</tr>
<tr>
<td>max_chuck_size</td>
<td>int</td>
<td>The max token length for a single chuck in the knowledge base</td>
<td>-</td>
</tr>
<tr>
<td>min_chuck_size</td>
<td>int</td>
<td>The min token length for a single chuck in the knowledge base</td>
<td>-</td>
</tr>
<tr>
<td>append</td>
<td>bool</td>
<td>Whether the new knowledge will be append to the existing knowledge base or directly load the existing knowledge base</td>
<td>True, False</td>
</tr>
<tr>
<td>polish</td>
<td>bool</td>
<td>Whether to polish the input query before processing</td>
<td>True, False</td>
</tr>
<tr>
<td>enable_rerank</td>
<td>bool</td>
<td>Whether to enable retrieval then rerank pipeline</td>
<td>True, False</td>
</tr>
<tr>
<td>reranker_model</td>
<td>str</td>
<td>The name of the reranker model from the Huggingface or a local path</td>
<td>-</td>
</tr>
<tr>
<td>top_n</td>
<td>int</td>
<td>The return number of the reranker model</td>
<td>-</td>
</tr>
</tbody>
</table><p>More retriever- and vectorstore-related parameters please check <a class="reference external" href="#langchain-extension">here</a></p>
</section>
<section id="rag-mode">
<h1>RAG Mode<a class="headerlink" href="#rag-mode" title="Link to this heading"></a></h1>
<p>Our system offers two distinct modes for the Retrieval-Augmented Generation (RAG) feature, catering to different user expectations: “accuracy” and “general.” These modes are designed to accommodate various application scenarios.</p>
<p>In “general” mode, the system primarily utilizes the output of the <code class="docutils literal notranslate"><span class="pre">IntentDetector</span></code> to determine the appropriate response prompt. If the predicted intent of the user’s query is “chitchat,” the system engages in a casual conversation. For other intents, it crafts a response augmented by retrieval results. This mode leverages the Large Language Model’s (LLM’s) inherent capabilities to predict user intent and generate relevant responses. However, it may occasionally misinterpret user intent, leading to reliance on the LLM’s inherent knowledge for response generation, which could result in inaccuracies or model hallucination issues.</p>
<p>Conversely, “accuracy” mode combines the <code class="docutils literal notranslate"><span class="pre">IntentDetector</span></code>’s output with retrieval results to enhance the accuracy of intent prediction. We implement a retrieval threshold to balance free generation with reliance on relevant documents. In this mode, the system will first search for relevant content to support the response. Casual conversation (”chitchat”) only occurs if there are no relevant documents and the intent is determined as such. This approach helps mitigate model hallucination problems but may limit the LLM’s free generation capacity.</p>
<p>Users are encouraged to choose the RAG mode that best suits their specific needs and application scenario.</p>
</section>
<section id="langchain-extension">
<h1>Langchain Extension<a class="headerlink" href="#langchain-extension" title="Link to this heading"></a></h1>
<p>To fully leverage the capabilities of our mutual chatbot platform, we have developed a comprehensive range of langchain-based extension APIs. These enhancements include advanced retrievers, embedding models, and vector stores, all designed to expand the functionality of the original langchain API. Our goal with these additions is to enrich user experience and provide a more robust and versatile chatbot platform.</p>
<section id="vector-stores">
<h2>Vector Stores<a class="headerlink" href="#vector-stores" title="Link to this heading"></a></h2>
<section id="chroma">
<h3>Chroma<a class="headerlink" href="#chroma" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://docs.trychroma.com/getting-started">Chroma</a> stands out as an AI-native, open-source vector database, placing a strong emphasis on boosting developer productivity and satisfaction. It’s available under the Apache 2.0 license. Initially, the original Chroma API within langchain was designed to accept settings only once, at the chatbot’s startup. This approach lacked flexibility, as it didn’t allow users to modify settings post-initialization. To address this limitation, we’ve revamped the Chroma API. Our updated version introduces enhanced vector store operations, enabling users to adjust and fine-tune their settings even after the chatbot has been initialized, offering a more adaptable and user-friendly experience.</p>
<p>The user can select Chroma as the vectorstore for RAG with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;vector_database&quot;</span><span class="p">]</span><span class="o">=</span><span class="s2">&quot;Chroma&quot;</span>
</pre></div>
</div>
<p>Our Chroma API is easy to use and can be generalized to langchain platform. For a quick Chroma configuration, the user can directly set the parameters following the same step for <a class="reference external" href="#parameters">agent_QA</a>. Some of parameters for Chroma share the same value with agent_QA. The extra parameters for Chroma are:
|  Parameters   |  Type | Description| Options|
|  —-  | —-  | –| –|
| collection_name  | str | The collection name for the local Chroma database instance. |-|
| persist_directory   | str | The path for saving the knowledge base. |-|
| collection_metadata   | dict | Collection configurations. Can set the retrieval distance type and indexing structure. |-|</p>
<p>For the langchain users, it can be easily imported and used by replacing the origin Chroma API in langchain.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_community.llms.huggingface_pipeline</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>
<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="kn">from</span> <span class="nn">langchain_core.vectorstores</span> <span class="kn">import</span> <span class="n">VectorStoreRetriever</span>
<span class="kn">from</span> <span class="nn">intel_extension_for_transformers.langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">VectorStoreRetriever</span><span class="p">(</span><span class="n">vectorstore</span><span class="o">=</span><span class="n">Chroma</span><span class="p">(</span><span class="o">...</span><span class="p">))</span>
<span class="n">retrievalQA</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">HuggingFacePipeline</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="n">retriever</span><span class="o">=</span><span class="n">retriever</span><span class="p">)</span>
</pre></div>
</div>
<p>More independent langchain-based examples can be found <a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/chroma">here</a>.</p>
</section>
<section id="qdrant">
<h3>Qdrant<a class="headerlink" href="#qdrant" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://qdrant.tech/documentation/">Qdrant</a> is a state-of-the-art vector similarity search engine, designed for production-ready services. It features an easy-to-use API that enables the storage, search, and management of points - vectors that come with additional payload data. Qdrant stands out for its advanced filtering capabilities, making it ideal for a wide range of uses such as neural network or semantic-based matching, faceted search, and other similar applications.</p>
<p>Originally, the Qdrant API within langchain was set up to allow configuration only once, at the time of the chatbot’s initialization. This setup limited flexibility, as it didn’t permit users to modify settings after the initial setup. Recognizing this limitation, we have redeveloped the Qdrant API. Our enhanced version offers expanded vector store operations, providing users with the ability to adjust and refine their settings post-initialization, thereby delivering a more adaptable and user-friendly experience.</p>
<p>The user can select Qdrant as the vectorstore for RAG with:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;vector_database&quot;</span><span class="p">]</span><span class="o">=</span><span class="s2">&quot;Qdrant&quot;</span>
</pre></div>
</div>
<p>Our Qdrant API is easy to use and can be generalized to langchain platform. For a quick Qdrant configuration, the user can directly set the parameters following the same step for <a class="reference external" href="#parameters">agent_QA</a>. Some of parameters for Qdrant share the same value with agent_QA. The extra parameters for Qdrant are:
|  Parameters   |  Type | Description| Options|
|  —-  | —-  | –| –|
| collection_name  | str | The collection name for the local Qdrant database instance. |-|
| location  | str | If <code class="docutils literal notranslate"><span class="pre">:memory:</span></code> - use in-memory Qdrant instance. If <code class="docutils literal notranslate"><span class="pre">str</span></code> - use it as a <code class="docutils literal notranslate"><span class="pre">url</span></code> parameter. If <code class="docutils literal notranslate"><span class="pre">None</span></code> - fallback to relying on <code class="docutils literal notranslate"><span class="pre">host</span></code> and <code class="docutils literal notranslate"><span class="pre">port</span></code> parameters.  |-|
| url   | str | Either host or str of “Optional[scheme], host, Optional[port], Optional[prefix]” |-|
| host   | str | Host name of Qdrant service. If url and host are None, set to ‘localhost’. |-|
| persist_directory   | str | Path in which the vectors will be stored while using local mode.|-|</p>
<p>For the langchain users, it can be easily imported and used by replacing the origin Qdrant API in langchain.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">langchain_community.llms.huggingface_pipeline</span> <span class="kn">import</span> <span class="n">HuggingFacePipeline</span>
<span class="kn">from</span> <span class="nn">langchain.chains</span> <span class="kn">import</span> <span class="n">RetrievalQA</span>
<span class="kn">from</span> <span class="nn">langchain_core.vectorstores</span> <span class="kn">import</span> <span class="n">VectorStoreRetriever</span>
<span class="kn">from</span> <span class="nn">intel_extension_for_transformers.langchain_community.vectorstores</span> <span class="kn">import</span> <span class="n">Qdrant</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">VectorStoreRetriever</span><span class="p">(</span><span class="n">vectorstore</span><span class="o">=</span><span class="n">Qdrant</span><span class="p">(</span><span class="o">...</span><span class="p">))</span>
<span class="n">retrievalQA</span> <span class="o">=</span> <span class="n">RetrievalQA</span><span class="o">.</span><span class="n">from_llm</span><span class="p">(</span><span class="n">llm</span><span class="o">=</span><span class="n">HuggingFacePipeline</span><span class="p">(</span><span class="o">...</span><span class="p">),</span> <span class="n">retriever</span><span class="o">=</span><span class="n">retriever</span><span class="p">)</span>
</pre></div>
</div>
<p>More independent langchain-based examples can be found <a class="reference external" href="https://python.langchain.com/docs/integrations/vectorstores/qdrant">here</a>.</p>
</section>
</section>
<section id="retrievers">
<h2>Retrievers<a class="headerlink" href="#retrievers" title="Link to this heading"></a></h2>
<p>Retrievers play a crucial role for RAG. They are responsible for implementing the basic retrieval configuration and accessing the vectorstore using the specified retrieval method and settings. Currently, we offer two types of retrievers: <code class="docutils literal notranslate"><span class="pre">VectorStoreRetriever</span></code> and <code class="docutils literal notranslate"><span class="pre">ChildParentRetriever</span></code>.</p>
<p>We’ve chosen VectorStoreRetriever as the default retriever. This decision aligns the retrieval process seamlessly with langchain’s functionality. The VectorStoreRetriever is designed to efficiently handle vectorstore operations, ensuring optimal retrieval performance. Meanwhile, the ChildParentRetriever offers a special solution for the long-context scenario.</p>
<p>Our approach ensures that users have access to versatile and effective retrieval tools, tailored to a variety of requirements and preferences within the system.</p>
<section id="vectorstoreretriever">
<h3>VectorStoreRetriever<a class="headerlink" href="#vectorstoreretriever" title="Link to this heading"></a></h3>
<p>We’ve maintained most of the retrieval behaviors consistent with langchain. The user can select this retriever by:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;retrieval_type&quot;</span><span class="p">]</span><span class="o">=</span><span class="s2">&quot;default&quot;</span>
</pre></div>
</div>
<p>The basic parameters for <code class="docutils literal notranslate"><span class="pre">VectorStoreRetriever</span></code> are:
|  Parameters   |  Type | Description| Options|
|  —-  | —-  | –| –|
| search_type  | str | Type of search to perform. |”mmr”, “similarity_score_threshold”, and “similarity”|
| search_kwargs  | dict | Keyword arguments to pass to the search function.|-|</p>
<p>The user can set the parameters for the retriever by:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;search_type&quot;</span><span class="p">]</span><span class="o">=</span><span class="n">xxx</span>
<span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;search_kwargs&quot;</span><span class="p">]</span><span class="o">=</span><span class="n">xxx</span>
</pre></div>
</div>
<p>If “search_type”=”similarity”:</p>
<blockquote>
<div><p>search_kwargs={”k”:xxx}</p>
</div></blockquote>
<p>“k” is the number of the returned most similar documents.</p>
<p>If “search_type”=”mmr”:</p>
<blockquote>
<div><p>search_kwargs={”k”:xxx, “fetch_k”:xxx, “lamabda_mult”:xxx}</p>
</div></blockquote>
<p>“k” is the number of the returned most similar documents. “fetch_k” is the number of Documents to fetch to pass to MMR algorithm. “Lamabda_mult” is a number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.</p>
<p>If “search_type”=”similarity_score_threshold”:</p>
<blockquote>
<div><p>search_kwargs={”k”:xxx, “score_threshold”:xxx}</p>
</div></blockquote>
<p>“k” is the number of the returned most similar documents. “score_threshold” is the similar score threshold for the retrieved documents.</p>
</section>
<section id="childparentretriever">
<h3>ChildParentRetriever<a class="headerlink" href="#childparentretriever" title="Link to this heading"></a></h3>
<p>We’ve specifically designed this retriever to address challenges in long-context retrieval scenarios. Commonly, in many applications, the documents being retrieved are lengthier than the user’s query. This discrepancy leads to an imbalance in context information between the query and the documents, often resulting in reduced retrieval accuracy. The reason is that the documents typically contain a richer semantic expression compared to the brief user query.</p>
<p>An ideal solution would be to segment the user-uploaded documents for the RAG knowledgebase into suitably sized chunks. However, this approach is not always feasible due to the lack of consistent guidelines for automatically and accurately dividing the context. Too short a division can result in partial, contextually incomplete answers to user queries. Conversely, excessively long segments can significantly lower retrieval accuracy.</p>
<p>To navigate this challenge, we’ve developed a unique solution involving the <code class="docutils literal notranslate"><span class="pre">ChildParentRetriever</span></code> to optimize the RAG process. Our strategy involves initially splitting the user-uploaded files into larger chunks, termed ‘parent chunks’, to preserve the integrity of each concept. Then, these parent chunks are further divided into smaller ‘child chunks’. Both child and parent chunks are interconnected using a unique identification ID. This approach enhances the likelihood and precision of matching the user query with a relevant, concise context chunk. When a highly relevant child chunk is identified, we use the ID to trace back to its parent chunk. The context from this parent chunk is then utilized in the RAG process, thereby improving the overall effectiveness and accuracy of retrieval.</p>
<p>The user can select this retriever by:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;retrieval_type&quot;</span><span class="p">]</span><span class="o">=</span><span class="s2">&quot;child_parent&quot;</span>
</pre></div>
</div>
<p>Most parameters for <code class="docutils literal notranslate"><span class="pre">ChildParentRetriever</span></code> are will be automatically set by <code class="docutils literal notranslate"><span class="pre">agent_QA</span></code>. The user only needs to decide the <code class="docutils literal notranslate"><span class="pre">search_type</span></code> and <code class="docutils literal notranslate"><span class="pre">search_kwargs</span></code>.
|  Parameters   |  Type | Description| Options|
|  —-  | —-  | –| –|
| search_type  | str | Type of search to perform. |”mmr”, and “similarity”|
| search_kwargs  | dict | Keyword arguments to pass to the search function.|-|</p>
<p>The user can set the parameters for the retriever by:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;search_type&quot;</span><span class="p">]</span><span class="o">=</span><span class="n">xxx</span>
<span class="n">plugins</span><span class="o">.</span><span class="n">retrieval</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="s2">&quot;search_kwargs&quot;</span><span class="p">]</span><span class="o">=</span><span class="n">xxx</span>
</pre></div>
</div>
<p>If “search_type”=”similarity”:</p>
<blockquote>
<div><p>search_kwargs={”k”=xxx}</p>
</div></blockquote>
<p>If “search_type”=”mmr”:</p>
<blockquote>
<div><p>search_kwargs={”k”=xxx, “fetch_k”=xxx, “lamabda_mult”=xxx}</p>
</div></blockquote>
<p>“k” is the number of the returned most similar documents. “fetch_k” is the number of Documents to fetch to pass to MMR algorithm. “Lamabda_mult” is a number between 0 and 1 that determines the degree of diversity among the results with 0 corresponding to maximum diversity and 1 to minimum diversity. Defaults to 0.5.</p>
<p>This new retriever is also available for langchain users. Below is a toy example that using our <code class="docutils literal notranslate"><span class="pre">ChildParentRetriever</span></code> in the langchain framework:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">intel_extension_for_transformers.langchain_community.retrievers</span> <span class="kn">import</span> <span class="n">ChildParentRetriever</span>
<span class="kn">from</span> <span class="nn">langchain.vectorstores</span> <span class="kn">import</span> <span class="n">Chroma</span>
<span class="n">retriever</span> <span class="o">=</span> <span class="n">ChildParentRetriever</span><span class="p">(</span><span class="n">vectorstore</span><span class="o">=</span><span class="n">Chroma</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">child_documents</span><span class="p">),</span> <span class="n">parentstore</span><span class="o">=</span><span class="n">Chroma</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">parent_documents</span><span class="p">),</span> <span class="n">search_type</span><span class="o">=</span><span class="n">xxx</span><span class="p">,</span> <span class="n">search_kwargs</span><span class="o">=</span><span class="p">{</span><span class="o">...</span><span class="p">})</span>
<span class="n">docs</span><span class="o">=</span><span class="n">retriever</span><span class="o">.</span><span class="n">get_relevant_documents</span><span class="p">(</span><span class="s2">&quot;Intel&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f8b2a564dc0> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>