<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Examples &mdash; Intel® Extension for Transformers 0.1.dev1+gf97b936 documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=eafc0fe6" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=68dfede1" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-transformers"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Neural Engine Support Matrix" href="intel_extension_for_transformers/llm/runtime/deprecated/docs/validated_model.html" />
    <link rel="prev" title="Example" href="../example.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Intel® Extension for Transformers
          </a>
            <div class="version">
              <a href="../../versions.html">latest▼</a>
              <p>Click link above to switch version</p>
            </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="get_started.html">Getting Started</a></li>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide.html">User Guide</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../example.html">Example</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Examples</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#quantization">Quantization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#stock-pytorch-examples">Stock PyTorch Examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="#intel-extension-for-pytorch-ipex-examples">Intel Extension for Pytorch (IPEX) examples</a></li>
<li class="toctree-l4"><a class="reference internal" href="#intel-tensorflow-examples">Intel TensorFlow Examples</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#length-adaptive-transformers">Length Adaptive Transformers</a></li>
<li class="toctree-l3"><a class="reference internal" href="#pruning">Pruning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distillation">Distillation</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#knowledge-distillation">Knowledge Distillation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#auto-distillation-nas-based">Auto Distillation (NAS Based)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#orchestrate">Orchestrate</a></li>
<li class="toctree-l3"><a class="reference internal" href="#reference-deployment-on-neural-engine">Reference Deployment on Neural Engine</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#dense-reference-deployment-on-neural-engine">Dense Reference Deployment on Neural Engine</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sparse-reference-deployment-on-neural-engine">Sparse Reference Deployment on Neural Engine</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#early-exit">Early-Exit</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="intel_extension_for_transformers/llm/runtime/deprecated/docs/validated_model.html">Neural Engine Support Matrix</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="api_doc/api.html">API</a></li>
<li class="toctree-l1"><a class="reference internal" href="SECURITY.html">Security Policy</a></li>
<li class="toctree-l1"><a class="reference internal" href="release.html">Release</a></li>
<li class="toctree-l1"><a class="reference internal" href="legal.html">Legal Information</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/intel/intel-extension-for-transformers">Repo</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel® Extension for Transformers</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../example.html">Example</a></li>
      <li class="breadcrumb-item active">Examples</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/docs/examples.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="examples">
<h1>Examples<a class="headerlink" href="#examples" title="Link to this heading"></a></h1>
<ol>
<li><p><a class="reference external" href="#quantization">Quantization</a></p>
<p>1.1 <a class="reference external" href="#stock-pytorch-examples">Stock PyTorch Examples</a></p>
<p>1.2 <a class="reference external" href="#intel-extension-for-pytorch-ipex-examples">Intel Extension for Pytorch (IPEX) Examples</a></p>
<p>1.3 <a class="reference external" href="#intel-tensorflow-examples">Intel TensorFlow Examples</a></p>
</li>
<li><p><a class="reference external" href="#length-adaptive-transformers">Length Adaptive Transformers</a></p></li>
<li><p><a class="reference external" href="#pruning">Pruning</a></p></li>
<li><p><a class="reference external" href="#distillation">Distillation</a></p>
<p>4.1 <a class="reference external" href="#knowledge-distillation">Knowledge Distillation</a></p>
<p>4.2 <a class="reference external" href="#auto-distillation-nas-based">Auto Distillation (NAS Based)</a></p>
</li>
<li><p><a class="reference external" href="#orchestrate">Orchestrate</a></p></li>
<li><p><a class="reference external" href="#reference-deployment-on-neural-engine">Reference Deployment on Neural Engine</a></p>
<p>6.1 <a class="reference external" href="#dense-reference-deployment-on-neural-engine">Dense Reference</a></p>
<p>6.2 <a class="reference external" href="#sparse-reference-deployment-on-neural-engine">Sparse Reference</a></p>
</li>
<li><p><a class="reference external" href="#early-exit">Early-Exit</a></p></li>
</ol>
<p>Intel Extension for Transformers is a powerful toolkit with multiple model optimization techniques for Natural Language Processing Models, including quantization, pruning, distillation, auto distillation and orchestrate. Meanwhile Intel Extension for Transformers provides Transformers-accelerated Neural Engine, an optimized backend for NLP models to demonstrate the deployment.</p>
<section id="quantization">
<h2>Quantization<a class="headerlink" href="#quantization" title="Link to this heading"></a></h2>
<section id="stock-pytorch-examples">
<h3>Stock PyTorch Examples<a class="headerlink" href="#stock-pytorch-examples" title="Link to this heading"></a></h3>
<table>
<thead>
  <tr>
    <th>Model</th>
    <th>Task</th>
    <th>Dataset</th>
    <th>PostTrainingDynamic</th>
    <th>PostTrainingStatic</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td><a href="https://huggingface.co/EleutherAI/gpt-j-6B">gpt-j-6B</a></td>
    <td>language-modeling(CLM)</td>
    <td><a href="https://huggingface.co/datasets/wikitext">wikitext</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/sysresearch101/t5-large-finetuned-xsum-cnn">t5-large-finetuned-xsum-cnn</a></td>
    <td>summarization</td>
    <td><a href="https://huggingface.co/datasets/cnn_dailymail">cnn_dailymail</a></td>
    <td>&#10004;</td>
    <td> </td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/flax-community/t5-base-cnn-dm">t5-base-cnn-dm</a></td>
    <td>summarization</td>
    <td><a href="https://huggingface.co/datasets/cnn_dailymail">cnn_dailymail</a></td>
    <td>&#10004;</td>
    <td> </td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/lambdalabs/sd-pokemon-diffusers">lambdalabs/sd-pokemon-diffusers</a></td>
    <td>text-to-image</td>
    <td>image</td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/bert-base-uncased">bert-base-uncased</a></td>
    <td>language-modeling(MLM)</td>
    <td><a href="https://huggingface.co/datasets/wikitext">wikitext</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/xlnet-base-case">xlnet-base-cased</a></td>
    <td>language-modeling(PLM)</td>
    <td><a href="https://huggingface.co/datasets/wikitext">wikitext</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/EleutherAI/gpt-neo-125M">EleutherAI/gpt-neo-125M</a></td>
    <td>language-modeling(CLM)</td>
    <td><a href="https://huggingface.co/datasets/wikitext">wikitext</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/sshleifer/tiny-ctrl">sshleifer/tiny-ctrl</a></td>
    <td>language-modeling(CLM)</td>
    <td><a href="https://huggingface.co/datasets/wikitext">wikitext</a></td>
    <td>WIP :star:</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/ehdwns1516/bert-base-uncased_SWAG">ehdwns1516/bert-base-uncased_SWAG</a></td>
    <td>multiple-choice</td>
    <td><a href="https://huggingface.co/datasets/swag">swag</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased-distilled-squad">distilbert-base-uncased-distilled-squad</a></td>
    <td>question-answering</td>
    <td><a href="https://huggingface.co/datasets/squad">SQuAD</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/valhalla/longformer-base-4096-finetuned-squadv1">valhalla/longformer-base-4096-finetuned-squadv1</a></td>
    <td>question-answering</td>
    <td><a href="https://huggingface.co/datasets/squad">SQuAD</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/lvwerra/pegasus-samsum">lvwerra/pegasus-samsum</a></td>
    <td>summarization</td>
    <td><a href="https://huggingface.co/datasets/samsum">samsum</a></td>
    <td>&#10004;</td>
    <td>WIP :star:</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/textattack/bert-base-uncased-MRPC">textattack/bert-base-uncased-MRPC</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mrpc/train">MRPC</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/echarlaix/bert-base-uncased-sst2-acc91.1-d37-hybrid">echarlaix/bert-base-uncased-sst2-acc91.1-d37-hybrid</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english">distilbert-base-uncased-finetuned-sst-2-english</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/elastic/distilbert-base-uncased-finetuned-conll03-english">elastic/distilbert-base-uncased-finetuned-conll03-english</a></td>
    <td>token-classification</td>
    <td><a href="https://huggingface.co/datasets/conll2003">conll2003</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/t5-small">t5-small</a></td>
    <td>translation</td>
    <td><a href="https://huggingface.co/datasets/wmt16">wmt16</a></td>
    <td>&#10004;</td>
    <td>WIP :star:</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/Helsinki-NLP/opus-mt-en-ro">Helsinki-NLP/opus-mt-en-ro</a></td>
    <td>translation</td>
    <td><a href="https://huggingface.co/datasets/wmt16">wmt16</a></td>
    <td>&#10004;</td>
    <td>WIP :star:</td>
  </tr>
</tbody>
</table><table>
<thead>
  <tr>
    <th>Model</th>
    <th>Task</th>
    <th>Dataset</th>
    <th>QuantizationAwareTraining</th>
    <th>No Trainer quantization</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td><a href="https://huggingface.co/textattack/bert-base-uncased-MRPC">textattack/bert-base-uncased-MRPC</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mrpc/train">MRPC</a></td>
    <td>&#10004;</td>
    <td> </td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/echarlaix/bert-base-uncased-sst2-acc91.1-d37-hybrid">echarlaix/bert-base-uncased-sst2-acc91.1-d37-hybrid</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
    <td> </td>
    <td>&#10004;</td>
  </tr>
</tbody>
</table></section>
<section id="intel-extension-for-pytorch-ipex-examples">
<h3>Intel Extension for Pytorch (IPEX) examples<a class="headerlink" href="#intel-extension-for-pytorch-ipex-examples" title="Link to this heading"></a></h3>
<table>
<thead>
  <tr>
    <th>Model</th>
    <th>Task</th>
    <th>Dataset</th>
    <th>PostTrainingStatic</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased-distilled-squad">distilbert-base-uncased-distilled-squad</a></td>
    <td>question-answering</td>
    <td><a href="https://huggingface.co/datasets/squad">SQuAD</a></td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/bert-large-uncased-whole-word-maskinuned-squad">bert-large-uncased-whole-word-maskinuned-squad</a></td>
    <td>question-answering</td>
    <td><a href="https://huggingface.co/datasets/squad">SQuAD</a></td>
    <td>&#10004;</td>
  </tr>
</tbody>
</table></section>
<section id="intel-tensorflow-examples">
<h3>Intel TensorFlow Examples<a class="headerlink" href="#intel-tensorflow-examples" title="Link to this heading"></a></h3>
<table>
<thead>
  <tr>
    <th>Model</th>
    <th>Task</th>
    <th>Dataset</th>
    <th>PostTrainingStatic</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td><a href="https://huggingface.co/bert-base-cased-finetuned-mrpc">bert-base-cased-finetuned-mrpc</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mrpc/train">MRPC</a></td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/xlnet-base-cased">xlnet-base-cased</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mrpc/train">MRPC</a></td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilgpt2">distilgpt2</a></td>
    <td>language-modeling(CLM)</td>
    <td><a href="https://huggingface.co/datasets/wikitext">wikitext</a></td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-cased">distilbert-base-cased</a></td>
    <td>language-modeling(MLM)</td>
    <td><a href="https://huggingface.co/datasets/wikitext">wikitext</a></td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/Rocketknight1/bert-base-uncased-finetuned-swag">Rocketknight1/bert-base-uncased-finetuned-swag</a></td>
    <td>multiple-choice</td>
    <td><a href="https://huggingface.co/datasets/swag">swag</a></td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/dslim/bert-base-NER">dslim/bert-base-NER</a></td>
    <td>token-classification</td>
    <td><a href="https://huggingface.co/datasets/conll2003">conll2003</a></td>
    <td>&#10004;</td>
  </tr>
</tbody>
</table></section>
</section>
<section id="length-adaptive-transformers">
<h2>Length Adaptive Transformers<a class="headerlink" href="#length-adaptive-transformers" title="Link to this heading"></a></h2>
<table>
<thead>
  <tr>
    <th rowspan="2">Model Name</th>
    <th rowspan="2">Datatype</th>
    <th rowspan="2">Optimization Method</th>
    <th rowspan="2">Modelsize (MB)</th>
    <th colspan="4">Inference Result</th>
  </tr>
  <tr>
    <th>Accuracy(F1)</th>
    <th>Latency(ms)</th>
    <th>GFLOPS**</th>
    <th>Speedup(compared with BERT Base)</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>BERT Base</td>
    <td>fp32</td>
    <td>None</td>
    <td>415.47</td>
    <td>88.58</td>
    <td>56.56</td>
    <td>35.3</td>
    <td>1x</td>
  </tr>
  <tr>
    <td>LA-MiniLM</td>
    <td>fp32</td>
    <td>Drop and restore base MiniLMv2</td>
    <td>115.04</td>
    <td>89.28</td>
    <td>16.99</td>
    <td>4.76</td>
    <td>3.33x</td>
  </tr>
  <tr>
    <td>LA-MiniLM(269, 253, 252, 202, 104, 34)*</td>
    <td>fp32</td>
    <td>Evolution search (best config)</td>
    <td>115.04</td>
    <td>87.76</td>
    <td>11.44</td>
    <td>2.49</td>
    <td>4.94x</td>
  </tr>
  <tr>
    <td>QuaLA-MiniLM</td>
    <td>int8</td>
    <td>Quantization base LA-MiniLM</td>
    <td>84.85</td>
    <td>88.85</td>
    <td>7.84</td>
    <td>4.76</td>
    <td>7.21x</td>
  </tr>
  <tr>
    <td>QuaLA-MiniLM(315,251,242,159,142,33)*</td>
    <td>int8</td>
    <td>Evolution search (best config)</td>
    <td>84.86</td>
    <td>87.68</td>
    <td>6.41</td>
    <td>2.55</td>
    <td>8.82x</td>
  </tr>
</tbody>
</table><blockquote>
<div><p><strong>Note</strong>: * length config apply to Length Adaptive model</p>
</div></blockquote>
<blockquote>
<div><p><strong>Note</strong>: ** the multiplication and addition operation amount when model inference  (GFLOPS is obtained from torchprofile tool)</p>
</div></blockquote>
<p>Data is tested on Intel Xeon Platinum 8280 Scalable processor. Configuration detail please refer to <a class="reference external" href="../examples/huggingface/pytorch/question-answering/dynamic/README.html">examples</a></p>
</section>
<section id="pruning">
<h2>Pruning<a class="headerlink" href="#pruning" title="Link to this heading"></a></h2>
<table>
<thead>
  <tr>
    <th>Model</th>
    <th>Task</th>
    <th>Dataset</th>
    <th>Pruning Approach</th>
    <th>Pruning Type</th>
    <th>Framework</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased-distilled-squad">distilbert-base-uncased-distilled-squad</a></td>
    <td>question-answering</td>
    <td><a href="https://huggingface.co/datasets/squad">SQuAD</a></td>
    <td>BasicMagnitude</td>
    <td>Unstructured</td>
    <td>Stock PyTorch</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/bert-large-uncased">bert-large-uncased</a></td>
    <td>question-answering</td>
    <td><a href="https://huggingface.co/datasets/squad">SQuAD</a></td>
    <td>Group LASSO</td>
    <td>Structured</td>
    <td>Stock PyTorch</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english">distilbert-base-uncased-finetuned-sst-2-english</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
    <td>BasicMagnitude</td>
    <td>Unstructured</td>
    <td>Stock PyTorch/&nbsp;&nbsp; Intel TensorFlow</td>
  </tr>
</tbody>
</table></section>
<section id="distillation">
<h2>Distillation<a class="headerlink" href="#distillation" title="Link to this heading"></a></h2>
<section id="knowledge-distillation">
<h3>Knowledge Distillation<a class="headerlink" href="#knowledge-distillation" title="Link to this heading"></a></h3>
<table>
<thead>
  <tr>
    <th>Student Model</th>
    <th>Teacher Model</th>
    <th>Task</th>
    <th>Dataset</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased">distilbert-base-uncased</a></td>
    <td><a href="https://huggingface.co/textattack/bert-base-uncased-SST-2">bert-base-uncased-SST-2</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased">distilbert-base-uncased</a></td>
    <td><a href="https://huggingface.co/textattack/bert-base-uncased-QNLI">bert-base-uncased-QNLI</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/qnli/train">QNLI</a></td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased">distilbert-base-uncased</a></td>
    <td><a href="https://huggingface.co/textattack/bert-base-uncased-QQP">bert-base-uncased-QQP</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/qqp/train">QQP</a></td>
  </tr>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased">distilbert-base-uncased</a></td>
    <td><a href="https://huggingface.co/blackbird/bert-base-uncased-MNLI-v1">bert-base-uncased-MNLI-v1</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mnli/train">MNLI</a></td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased">distilbert-base-uncased</a></td>
    <td><a href="https://huggingface.co/csarron/bert-base-uncased-squad-v1">bert-base-uncased-squad-v1</a></td>
    <td>question-answering</td>
    <td><a href="https://huggingface.co/datasets/squad">SQuAD</a></td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D">TinyBERT_General_4L_312D</a></td>
    <td><a href="https://huggingface.co/blackbird/bert-base-uncased-MNLI-v1">bert-base-uncased-MNLI-v1</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mnli">MNLI</a></td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilroberta-base">distilroberta-base</a></td>
    <td><a href="https://huggingface.co/cointegrated/roberta-large-cola-krishna2020">roberta-large-cola-krishna2020</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/cola">COLA</a></td>
  </tr>
</tbody>
</table></section>
<section id="auto-distillation-nas-based">
<h3>Auto Distillation (NAS Based)<a class="headerlink" href="#auto-distillation-nas-based" title="Link to this heading"></a></h3>
<table>
<thead>
  <tr>
    <th>Model</th>
    <th>Task</th>
    <th>Dataset</th>
    <th>Distillation Teacher</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td><a href="https://huggingface.co/google/mobilebert-uncased">google/mobilebert-uncased</a></td>
    <td>language-modeling(MLM)</td>
    <td><a href="https://huggingface.co/datasets/wikipedia">wikipedia</a></td>
    <td><a href="https://huggingface.co/bert-large-uncased">bert-large-uncased</a></td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/prajjwal1/bert-tiny">prajjwal1/bert-tiny</a></td>
    <td>language-modeling(MLM)</td>
    <td><a href="https://huggingface.co/datasets/wikipedia">wikipedia</a></td>
    <td><a href="https://huggingface.co/bert-base-uncased">bert-base-uncased</a></td>
  </tr>
</tbody>
</table></section>
</section>
<section id="orchestrate">
<h2>Orchestrate<a class="headerlink" href="#orchestrate" title="Link to this heading"></a></h2>
<table>
<thead>
  <tr>
    <th>Model</th>
    <th>Task</th>
    <th>Dataset</th>
    <th>Distillation Teacher</th>
    <th>Pruning Approach</th>
    <th>Pruning Type</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td rowspan="4"><a href="https://huggingface.co/Intel/distilbert-base-uncased-sparse-90-unstructured-pruneofa">Intel/distilbert-base-uncased-sparse-90-unstructured-pruneofa</a></td>
    <td rowspan="2">question-answering</td>
    <td rowspan="2"><a href="https://huggingface.co/datasets/squad">SQuAD</a></td>
    <td rowspan="2"><a href="https://huggingface.co/distilbert-base-uncased-distilled-squad">distilbert-base-uncased-distilled-squad</a></td>
    <td>PatternLock</td>
    <td>Unstructured</td>
  </tr>
  <tr>
    <td>BasicMagnitude</td>
    <td>Unstructured</td>
  </tr>
  <tr>
    <td rowspan="2">text-classification</td>
    <td rowspan="2"><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
    <td rowspan="2"><a href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english">distilbert-base-uncased-finetuned-sst-2-english</a></td>
    <td>PatternLock</td>
    <td>Unstructured</td>
  </tr>
  <tr>
    <td>BasicMagnitude</td>
    <td>Unstructured</td>
  </tr>
</tbody>
</table></section>
<section id="reference-deployment-on-neural-engine">
<h2>Reference Deployment on Neural Engine<a class="headerlink" href="#reference-deployment-on-neural-engine" title="Link to this heading"></a></h2>
<section id="dense-reference-deployment-on-neural-engine">
<h3>Dense Reference Deployment on Neural Engine<a class="headerlink" href="#dense-reference-deployment-on-neural-engine" title="Link to this heading"></a></h3>
<table>
<thead>
  <tr>
    <th rowspan="2">Model</th>
    <th rowspan="2">Task</th>
    <th rowspan="2">Dataset</th>
    <th colspan="2">Datatype</th>
  </tr>
  <tr>
    <th>INT8</th>
    <th>BF16</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased-distilled-squad">bert-large-uncased-whole-word-masking-finetuned-squad</a></td>
    <td>question-answering</td>
    <td><a href="https://huggingface.co/datasets/squad">SQuAD</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion">bhadresh-savani/distilbert-base-uncased-emotion</td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/emotion">emotion</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/textattack/bert-base-uncased-MRPC">textattack/bert-base-uncased-MRPC</td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mrpc/train">MRPC</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/textattack/distilbert-base-uncased-MRPC">textattack/distilbert-base-uncased-MRPC</td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mrpc/train">MRPC</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/Intel/roberta-base-mrpc">Intel/roberta-base-mrpc</td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mrpc/train">MRPC</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/M-FAC/bert-mini-finetuned-mrpc">M-FAC/bert-mini-finetuned-mrpc</td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mrpc/train">MRPC</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/gchhablani/bert-base-cased-finetuned-mrpc">gchhablani/bert-base-cased-finetuned-mrpc</td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mrpc/train">MRPC</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english">distilbert-base-uncased-finetuned-sst-2-english</td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/philschmid/MiniLM-L6-H384-uncased-sst2">philschmid/MiniLM-L6-H384-uncased-sst2</td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/moshew/bert-mini-sst2-distilled">moshew/bert-mini-sst2-distilled</td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
    <td>&#10004;</td>
    <td>&#10004;</td>
  </tr>
</tbody>
</table></section>
<section id="sparse-reference-deployment-on-neural-engine">
<h3>Sparse Reference Deployment on Neural Engine<a class="headerlink" href="#sparse-reference-deployment-on-neural-engine" title="Link to this heading"></a></h3>
<table>
<thead>
  <tr>
    <th rowspan="2">Model</th>
    <th rowspan="2">Task</th>
    <th rowspan="2">Dataset</th>
    <th colspan="2">Datatype</th>
  </tr>
  <tr>
    <th>INT8</th>
    <th>BF16</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td><a href="https://huggingface.co/Intel/distilbert-base-uncased-squadv1.1-sparse-80-1x4-block-pruneofa">Intel/distilbert-base-uncased-squadv1.1-sparse-80-1x4-block-pruneofa</td>
    <td>question-answering</td>
    <td><a href="https://huggingface.co/datasets/squad">SQuAD</a></td>
    <td>&#10004;</td>
    <td>WIP :star:</td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/Intel/bert-mini-sst2-distilled-sparse-90-1X4-block">Intel/bert-mini-sst2-distilled-sparse-90-1X4-block</td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
    <td>&#10004;</td>
    <td>WIP :star:</td>
  </tr>
</tbody>
</table></section>
</section>
<section id="early-exit">
<h2>Early-Exit<a class="headerlink" href="#early-exit" title="Link to this heading"></a></h2>
<table>
<thead>
  <tr>
    <th>Model</th>
    <th>Task</th>
    <th>Dataset</th>
    <th>Early-Exit Type</th>
  </tr>
</thead>
<tbody align="center">
  <tr>
    <td><a href="https://huggingface.co/bert-base-uncased">bert-base-uncased</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/mnli/train">MNLI</a></td>
    <td><a href="https://github.com/intel/intel-extension-for-transformers/blob/main/examples/huggingface/pytorch/text-classification/early-exit/README.html">SWEET</a><br><a href="https://github.com/intel/intel-extension-for-transformers/blob/main/docs/tutorials/pytorch/text-classification/SWEET.ipynb">notebook</a></td>
  </tr>
  <tr>
    <td><a href="https://huggingface.co/philschmid/tiny-bert-sst2-distilled">philschmid/tiny-bert-sst2-distilled</a><br><a href="https://huggingface.co/textattack/roberta-base-SST-2">textattack/roberta-base-SST-2</a></td>
    <td>text-classification</td>
    <td><a href="https://huggingface.co/datasets/glue/viewer/sst2/train">SST-2</a></td>
    <td><a href="https://github.com/intel/intel-extension-for-transformers/blob/main/examples/huggingface/pytorch/text-classification/cascade-models/README.html">TangoBERT</a><br><a href="https://github.com/intel/intel-extension-for-transformers/blob/main/docs/tutorials/pytorch/text-classification/TangoBERT.ipynb">notebook</a></td>
  </tr>
</tbody>
</table></section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../example.html" class="btn btn-neutral float-left" title="Example" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="intel_extension_for_transformers/llm/runtime/deprecated/docs/validated_model.html" class="btn btn-neutral float-right" title="Neural Engine Support Matrix" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Intel® Extension for Transformers, Intel.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f8516e4b730> 
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>