Chat with the NeuralChat
============

This document showcases the utilization of the fine-tuned model for conversing with NeuralChat. To obtaining the fine-tuned model, please refer to the [fine-tuning](../fine_tuning/README.md) section. The inference of the fine-tuned models has been validated on the 4th Gen Intel® Xeon® Processors, Sapphire Rapids(SPR) and Habana® Gaudi® Deep Learning Processors.

# Prerequisite​

## Environment​

We recommend python 3.9 or higher version. Install the required dependencies using the following command:

```shell
pip install -r requirements.txt
```

# Document Indexing

Document indexing aims to aid users in efficiently parsing locally uploaded files and storing them in a document store for future content retrieval. We have developed two distinct indexing methods: sparse retrieval and dense retrieval. For more information, please consult the [README](./document_indexing/README.md) file.

# Document Reranker

The purpose of a document reranker is to improve the relevance and quality of search results generated by an information retrieval system. It is a component within the inference pipeline that reorders the initially retrieved documents based on their relevance to the user's query. We provided a [ColBERTRanker](./document_ranker/colbert.py) which uses the ColBERT model for reordering search results based on their relevance to a given query. Its purpose is to enhance the accuracy and quality of search results by applying a more sophisticated ranking algorithm.

# Inference

## Inference on Xeon SPR

We provide the [generate.py](./generate.py) script for performing inference on Intel® CPUs. We have enabled IPEX BF16 to speed up the inference. Please use the following commands for inference.

For [MPT](https://huggingface.co/mosaicml/mpt-7b-chat), it uses the gpt-neox-20b tokenizer, so you need to explicitly define it in the command line. This model also requires passing trust_remote_code=True to the from_pretrained method. This is because we use a custom MPT model architecture that is not yet part of the Hugging Face transformers package.

If you don't have a fine-tuned model, please remove the 'peft_model_path' parameter.

```bash
python generate.py \
        --base_model_path "mosaicml/mpt-7b-chat" \
        --peft_model_path "./mpt_peft_finetuned_model" \
        --tokenizer_name "EleutherAI/gpt-neox-20b" \
        --use_kv_cache \
        --trust_remote_code \
        --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."
```

If you want to accelerate the generation, you can use the key/value cache for decoding by adding the flag `--use_kv_cache`, and use jit trace by `pip install optimum-intel` and adding the flag `--jit`.

```bash
python generate.py \
        --base_model_path "mosaicml/mpt-7b-chat" \
        --trust_remote_code \
        --instructions "Transform the following sentence into one that shows contrast. The tree is rotten." \
        --use_kv_cache \
        --jit
```

The [generate.py](./generate.py) script accepts different arguments to set the inference behavior of the model.

```bash
python generate.py  \
          --temperature 0.2 \
          --top_p 0.8 \
          --top_k 45 \
          --num_beams 1 \
          --repetition_penalty 1.2 \
          --max_new_tokens 512 \
          --base_model_path "mosaicml/mpt-7b-chat" \
          --tokenizer_name "EleutherAI/gpt-neox-20b" \
          --use_kv_cache \
          --trust_remote_code \
          --instructions "Tell me about Intel Xeon."
```

Here are the explanations of each parameter:
`--temperature`: Controls the diversity of generated text. Lower values result in more deterministic outputs. The default value is 0.1.
`--top_p`: During text generation, only consider tokens with cumulative probability up to this value. This parameter helps to avoid extremely low probability tokens. The default value is 0.75.
`--top_k`: The number of highest probability vocabulary tokens to consider for each step of text generation. The default value is 40.
`--num_beams`: The number of beams to use for beam search decoding. This parameter helps to generate multiple possible completions. The default value is 1.
`--repetition_penalty`: This value controls the penalty for repeating tokens in the generated text. Higher values encourage the model to produce more diverse outputs. The default value is 1.1.
`--max_new_tokens`: The maximum number of tokens allowed in the generated output. This parameter helps to limit the length of the generated text. The default value is 128.


For Llama, use the below command line to chat with it.
If you encounter a failure with the Llama fast tokenizer while using the latest transformers, add the option "--use_slow_tokenizer".
The `tokenizer_class` in `tokenizer_config.json` should be changed from `LLaMATokenizer` to `LlamaTokenizer`.
The `architectures` in `config.json` should be changed from `LLaMAForCausalLM` to `LlamaForCausalLM`.

```bash
python generate.py \
        --base_model_path "decapoda-research/llama-7b-hf" \
        --peft_model_path "./llama_peft_finetuned_model" \
        --use_slow_tokenizer \
        --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."
```

```bash
python generate.py \
        --temperature 0.2 \
        --top_p 0.8 \
        --top_k 45 \
        --num_beams 1 \
        --repetition_penalty 1.2 \
        --base_model_path "decapoda-research/llama-7b-hf" \
        --use_slow_tokenizer \
        --instructions "Tell me about China."
```

## Deployment on Xeon SPR

Apart from direct inference using the [generate.py](./generate.py) script, you can also run a backend and use the RESTful API for inference. We have enabled IPEX BF16 to speed up the inference.

### Retrieval-free Chatbot

Please refer to the [README](./backend/chat/README.md) for instructions on running the model as a service. Once it is up and running, you will see a generated URL. The RESTful API `worker_generate_stream` can be used to generate text. 

You can use the following command to trigger inference:

```bash
curl -X POST -H "Content-Type: application/json" -d '{"model": "mpt-7b-chat", "prompt": "A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human questions.\nHuman: What are the potential benefits and risks of cryptocurrency investments?\nAssistant:", "stop":"<|endoftext|>"}' http://localhost:80/worker_generate_stream
```

You can also use [chatcli](../demo/chatcli/) to access the service.

### Retrieval-augmented Chatbot

Please refer to the [README](./backend/fastrag/README.md) for instructions on running the model as a service. Once it is up and running, you will see a generated URL. The RESTful API `fastrag/query` can be used to generate text.

You can use the following command to trigger inference:

``` bash
curl -X POST -H "Content-Type: application/json" -d '{"query": ""Could you elaborate some measures or policies that company is taking on the public rental House and talent housing allowance?", "domain": "ASK_GM", "embedding": "sparse"}' http://localhost:80/fastrag/query
```

You can also use [chatcli](../demo/chatcli/) to access the service.

## Inference on Habana Gaudi

Use this [link](https://docs.habana.ai/en/latest/AWS_EC2_DL1_and_PyTorch_Quick_Start/AWS_EC2_DL1_and_PyTorch_Quick_Start.html) to get started with setting up Gaudi-based Amazon EC2 DL1 instances.

### Setup Habana Environment

```bash
git clone https://github.com/intel/intel-extension-for-transformers.git
cd ./intel-extension-for-transformers/
apt-get update
apt-get install git-lfs
git-lfs install
```

Copy the [generate.py](./generate.py) script to Gaudi instance and place it in the current directory.
Run the Docker container with Habana runtime and necessary environment variables:

```bash
docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host -v $(pwd):/intel-extension-for-transformers vault.habana.ai/gaudi-docker/1.10.0/ubuntu22.04/habanalabs/pytorch-installer-2.0.1:latest
cd /intel-extension-for-transformers/workflows/chatbot/inference/
git clone https://huggingface.co/mosaicml/mpt-7b-chat
pip install datasets
pip install optimum
pip install git+https://github.com/huggingface/optimum-habana.git
pip install peft
pip install einops
pip install git+https://github.com/HabanaAI/DeepSpeed.git@1.10.0
```

### Run the inference

You can use the [generate.py](./generate.py) script for performing direct inference on Habana Gaudi instance. We have enabled BF16 to speed up the inference. Please use the following command for inference.

```bash
python generate.py --base_model_path "./mpt-7b-chat" \
             --habana \
             --tokenizer_name "EleutherAI/gpt-neox-20b" \
             --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."
```

And you can use `deepspeed` to speedup the inference.

```bash
python ../gaudi_spawn.py --use_deepspeed --world_size 8 generate.py \
        --base_model_path "./mpt-7b-chat" \
        --habana \
        --tokenizer_name "EleutherAI/gpt-neox-20b" \
        --instructions "Transform the following sentence into one that shows contrast. The tree is rotten."
```
