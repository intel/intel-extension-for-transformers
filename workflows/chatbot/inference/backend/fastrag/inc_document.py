# Copyright (c) 2024 Intel Corporation
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

inc_examples = [{"doc": "## FX Introduction\nFX is a PyTorch toolkit for developers to use to transform nn.Module instance. FX consists of three main components: a symbolic tracer, an intermediate representation, and Python code generation.\n\nWith converted torch.fx.GraphModule, we can resolve three problems in quantization:\n1. Automatically insert quant/dequant operation within PyTorch.\n2. Use FloatFunctional to wrap tensor operations that require special handling for quantization into modules. Examples are operations like add and cat which require special handling to determine output quantization parameters.\n3. Fuse modules: combine operations/modules into a single module to obtain higher accuracy and performance. This is done using the fuse_modules() API, which takes in lists of modules to be fused. We currently support the following fusions: [Conv, Relu], [Conv, BatchNorm], [Conv, BatchNorm, Relu], [Linear, Relu].\n\nFor detailed description, please refer to [PyTorch FX](https://pytorch.org/docs/stable/fx.html) and [FX Graph Mode Quantization](https://pytorch.org/docs/master/quantization.html#prototype-fx-graph-mode-quantization)", "doc_id": 0},
{"doc": "## FX Mode Support Matrix in Neural Compressor\n\n|quantization           |FX           |\n|-----------------------|:-----------:|\n|Static Quantization    |&#10004;     |\n|Dynamic Quantization   |&#10004;     |\n|Quantization-Aware Training         |&#10004;     |", "doc_id": 1},
{"doc": "## Get Start with FX in Neural Compressor\n\n**Note:** \"backend\" field indicates the backend used by the user in configure. And the \"default\" value means it will quantization model with fx backend for PyTorch model.\n\n### Post Training Static Quantization\n\n```\n    from neural_compressor import quantization, PostTrainingQuantConfig\n    conf = PostTrainingQuantConfig(backend=\"default\")\n    model.eval()\n    q_model = quantization.fit(model, conf, calib_dataloader=dataloader, eval_func=eval_func)\n    q_model.save(\"save/to/path\")\n```", "doc_id": 2},
{"doc": "## Get Start with FX in Neural Compressor\n### Post Training Dynamic Quantization\n\n```\n    from neural_compressor import quantization, PostTrainingQuantConfig\n    conf = PostTrainingQuantConfig(backend=\"default\", approach=\"dynamic\")\n    model.eval()\n    q_model = quantization.fit(model, conf, eval_func=eval_func)\n    q_model.save(\"save/to/path\")\n```", "doc_id": 3},
{"doc": "## Get Start with FX in Neural Compressor\n### Quantization-Aware Training\n\n```\n    from neural_compressor import QuantizationAwareTrainingConfig\n    from neural_compressor.training import prepare_compression\n    conf = QuantizationAwareTrainingConfig(backend=\"default\")\n    compression_manager = prepare_compression(model, conf)\n    compression_manager.callbacks.on_train_begin()\n    model = compression_manager.model\n    model.train()\n    ####### Training loop #####\n\n    compression_manager.save(\"save/to/path\")\n```", "doc_id": 4},
{"doc": "## Examples for quantizing a model with FX backend\n\nUser could refer to [examples](https://github.com/intel/neural-compressor/blob/master/examples/pytorch/nlp/huggingface_models/question-answering/quantization/ptq_static/fx/README.md) on how to quantize a model with FX backend.", "doc_id": 5},
{"doc": "## Common Problem with FX backend for quantization\n### *Dynamic Quantization*  \n\n - PyTorch Version: 1.9 or higher  \n\n    You can use pytorch backend for dynamic quantization, there is no difference between pytorch and pytorch_fx. we don't need to trace model because we don't need to modify the source code of the model.", "doc_id": 6},
{"doc": "## Common Problem with FX backend for quantization\n### *Static Quantization* & *Quantization Aware Training*  \n\n - PyTorch Version: 1.8 or higher\n\n    As symbolic trace cannot handle dynamic control, tensor iteration and so on, we might meet trace failure sometimes. In order to quantize the model successfully, we suggest user to trace model with below two approaches first and then pass it to neural_compressor.\n\n    1. Non_traceable_module_class/name\n\n        Select module classes or names that cannot be traced by proxy object, and pass them into prepare_fx as a dict. \n        **Please refer to:** https://pytorch.org/tutorials/prototype/fx_graph_mode_quant_guide.html?highlight=non_traceable_module_class\n\n    2. Decorator: @torch.fx.wrap\n\n        If untraceable part is not a part of a module, like a global function called, or you want to move untraceable part out of model to keep other parts get quantized, you should try to use Decorator `@torch.fx.wrap`. The wrapped function must be in the global, not in the class.\n\n        **For example:** examples/pytorch/fx/object_detection/ssd_resnet34/ptq/python/models/ssd_r34.py\n\n        ``` python\n        @torch.fx.wrap\n        def bboxes_labels_scores(bboxes, probs, criteria = 0.45, max_output=200):\n            boxes = []; labels=[]; scores=[]\n            for bbox, prob in zip(bboxes.split(1, 0), probs.split(1, 0)):\n                bbox = bbox.squeeze(0)\n                prob = prob.squeeze(0)\n                dbox,dlabel,dscore=decode_single(bbox, prob, criteria, max_output)\n                boxes.append(dbox)\n                labels.append(dlabel)\n                scores.append(dscore)\n            return [boxes,labels,scores]\n            ```", "doc_id": 7},
{"doc": "# Neural Architecture Search (NAS)\n## Introduction\nNeural Architecture Search (NAS) is the process of automating the design of artificial neural networks (ANN) architecture. NAS has been used to design networks that are on par with or outperform hand-designed architectures. Intel\u00ae Neural Compressor has supported two different NAS methods: Basic NAS and Dynamic NAS.", "doc_id": 8},
{"doc": "# Neural Architecture Search (NAS)\n### Basic NAS\nOur Basic NAS method leverages a specific search algorithm from built-in search algorithms (grid search, random search and bayesian optimization are supported in Intel\u00ae Neural Compressor now) or user defined search algorithms to propose the model architecture based on the given search space, then perform the train evaluation process to evaluate the potential of the proposed model architecture, after several iterations of such procedure, best performing model architectures which lie in pareto front will be returned.", "doc_id": 9},
{"doc": "# Neural Architecture Search (NAS)\n### Dynamic NAS\nDynamic Neural Architecture Search (DyNAS) is a super-network-based NAS approach which use the metric predictors for predicting the metrics of the model architecture, it is >4x more sample efficient than typical one-shot predictor-based NAS approaches.", "doc_id": 10},
{"doc": "# Neural Architecture Search (NAS)\n## NAS Support Matrix\n\n|NAS Algorithm     |PyTorch   |TensorFlow |\n|------------------|:--------:|:---------:|\n|Basic NAS         |&#10004;  |Not supported yet|\n|Dynamic NAS       |&#10004;  |Not supported yet|", "doc_id": 11},
{"doc": "# Neural Architecture Search (NAS)\n## Get Started with NAS API\n### Basic Usage\n#### 1. Python code + YAML\n\nSimplest launcher code if NAS configuration is defined in user-defined yaml.\n\n```python\nfrom neural_compressor.experimental import NAS\nagent = NAS('/path/to/user/yaml')\nresults = agent.search()\n```", "doc_id": 12},
{"doc": "# Neural Architecture Search (NAS)\n## Get Started with NAS API\n### Basic Usage\n#### 2. Python code only\n\nNAS class also support `NASConfig` class as it's argument.\n\n```python\nfrom neural_compressor.conf.config import NASConfig\nfrom neural_compressor.experimental import NAS\nconfig = NASConfig(approach='dynas', search_algorithm='nsga2')\nconfig.dynas.supernet = 'ofa_mbv3_d234_e346_k357_w1.2'\nconfig.dynas.metrics = ['acc', 'macs']\nconfig.dynas.population = 50\nconfig.dynas.num_evals = 250\nconfig.dynas.results_csv_path = 'search_results.csv'\nconfig.dynas.batch_size = 64\nconfig.dynas.dataset_path = '/datasets/imagenet-ilsvrc2012' #example\nagent = NAS(config)\nresults = agent.search()\n```", "doc_id": 13},
{"doc": "# Neural Architecture Search (NAS)\n## Get Started with NAS API\n### Advanced Usage (Custom NAS)\n\nIntel\u00ae Neural Compressor NAS API is defined under `neural_compressor.experimental.nas`, which takes a user defined yaml file or a [NASConfig](../../neural_compressor/conf/config.py#NASConfig) object as input. The user defined yaml or the [NASConfig](../../neural_compressor/conf/config.py#NASConfig) object defines necessary configuration of the NAS process. The [NAS](../../neural_compressor/experimental/nas/nas.py#NAS) class aims to create an object according to the defined NAS approach in the configuration, please note this NAS approach should be registered in the Intel\u00ae Neural Compressor.\n\nCurrently, Intel\u00ae Neural Compressor supported two built-in NAS methods: [Basic NAS](../../neural_compressor/experimental/nas/basic_nas.py#BasicNAS) and [Dynamic NAS](../../neural_compressor/experimental/nas/dynas.py#DyNAS). Both methods are inherited from a base class called [NASBase](../../neural_compressor/experimental/nas/nas.py#NASBase). User can also customize their own NAS approach in Intel\u00ae Neural Compressor just by decorating their NAS approach class with function [nas_registry](../../neural_compressor/experimental/nas/nas_utils.py#nas_registry) as well as following the API in [NASBase](../../neural_compressor/experimental/nas/nas.py#NASBase), like the way used in the two built-in NAS methods.", "doc_id": 14},
{"doc": "# Neural Architecture Search (NAS)\n## Get Started with NAS API\n### Advanced Usage (Custom NAS)\n## Examples\n\nFollowing examples are supported in Intel\u00ae Neural Compressor:\n\n- DyNAS MobileNetV3 supernet Example:\n  - [DyNAS MobileNetV3 supernet Example](../../examples/notebook/dynas/MobileNetV3_Supernet_NAS.ipynb): DyNAS with MobileNetV3 supernet on ImageNet dataset.\n- DyNAS Transformer LT supernet Example:\n  - [DyNAS Transformer LT supernet Example](../../examples/notebook/dynas/Transformer_LT_Supernet_NAS.ipynb): DyNAS with Transformer LT supernet on WMT En-De dataset.", "doc_id": 15},
{"doc": "# Intel Neural Compressor Adaptor\n## Introduction\n\nIntel\u00ae Neural Compressor builds the low-precision inference\nsolution on popular deep learning frameworks such as TensorFlow, PyTorch,\nMXNet, and ONNX Runtime. The adaptor layer is the bridge between the \ntuning strategy and vanilla framework quantization APIs.", "doc_id": 16},
{"doc": "# Intel Neural Compressor Adaptor\n## Adaptor Support Matrix\n\n|Framework     |Adaptor      |\n|--------------|:-----------:|\n|TensorFlow    |&#10004;     |\n|PyTorch       |&#10004;     |\n|ONNX          |&#10004;     |\n|MXNet         |&#10004;     |", "doc_id": 17},
{"doc": "# Intel Neural Compressor Adaptor\n## Working Flow\nAdaptor only provide framework API for tuning strategy. So we can find complete working flow in [tuning strategy working flow](./tuning_strategies.md).", "doc_id": 18},
{"doc": "# Intel Neural Compressor Adaptor\n## Get Start with Adaptor API\n\nNeural Compressor supports a new adaptor extension by\nimplementing a subclass `Adaptor` class in the neural_compressor.adaptor package\nand registering this adaptor by the `adaptor_registry` decorator.\nFor example, a user can implement an `Abc` adaptor like below:\n```python\n@adaptor_registry\nclass AbcAdaptor(Adaptor):\n    def __init__(self, framework_specific_info):\n        ...\n\n    def quantize(self, tune_cfg, model, dataloader, q_func=None):\n        ...\n\n    def evaluate(self, model, dataloader, postprocess=None,\n                 metric=None, measurer=None, iteration=-1, tensorboard=False):\n        ...\n\n    def query_fw_capability(self, model):\n        ...\n\n    def query_fused_patterns(self, model):\n        ...\n```\n* `quantize` function is used to perform quantization for post-training quantization and quantization-aware training. Quantization processing includes calibration and conversion processing for post-training quantization, while for quantization-aware training, it includes training and conversion processing.\n* `evaluate` function is used to run an evaluation on a validation dataset. It is a built-in function, if user wants to use specific evaluation function, he can pass the evaluation function to quantizer.\n* `query_fw_capability` function is used to run a query framework quantization capability and intersects with the user yaml configuration.\n* `query_fused_patterns` function is used to run a query framework graph fusion capability and decide the fusion tuning space.", "doc_id": 19},
{"doc": "# Intel Neural Compressor Adaptor\n### Query API\n#### Background\nBesides the adaptor API, we also introduced the Query API which describes the\nbehavior of a specific framework. With this API, Neural Compressor can easily query the\nfollowing information on the current runtime framework.\n*  The runtime version information.\n*  The Quantizable ops type.\n*  The supported sequence of each quantizable op.\n*  The instance of each sequence.\nIn the past, the above information was generally defined and hidden in every corner of the code which made effective maintenance difficult. With the Query API, we only need to create one unified yaml file and call the corresponding API to get the information. For example, the [tensorflow.yaml](../neural_compressor/adaptor/tensorflow.yaml) keeps the current Tensorflow framework ability. We recommend that the end user not make modifications if requirements are not clear.\nBelow is a fragment of the Tensorflow configuration file.\n* **precisions** field defines the supported precision for Neural Compressor.\n    -  valid_mixed_precision enumerates all supported precision combinations for specific scenario. For example, if one hardware doesn't support bf16\uff0c it should be `int8 + fp32`.\n* **ops** field defines the valid OP type list for each precision.\n* **capabilities** field focuses on the quantization ability of specific ops such as granularity, scheme, and algorithm. The activation assumes the same data type for both input and output activation by default based on op semantics defined by frameworks.\n* **patterns** field defines the supported fusion sequence of each op.", "doc_id": 20},
{"doc": "# Intel Neural Compressor Adaptor\n#### Query API Introduction\n\nThe abstract class `QueryBackendCapability` is defined in [query.py](../neural_compressor/adaptor/query.py#L21). Each framework should inherit it and implement the member function if needed. Refer to Tensorflow implementation [TensorflowQuery](../neural_compressor/adaptor/tensorflow.py#L628).", "doc_id": 21},
{"doc": "# Intel Neural Compressor Adaptor\n## Example of Adding a New Backend Support\n\nLook at onnxruntime as an example. ONNX Runtime is a backend proposed by Microsoft, and is based on the MLAS kernel by default.\nOnnxruntime already has [quantization tools](https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/python/tools/quantization), so the question becomes how to integrate onnxruntime quantization tools into Neural Compressor.", "doc_id": 22},
{"doc": "# Intel Neural Compressor Adaptor\n## Example of Adding a New Backend Support\n### Capability\n   The user should explore quantization capability first. According to [onnx_quantizer](https://github.com/microsoft/onnxruntime/blob/503b61d897074a494f5798069308ee67d8fb9ace/onnxruntime/python/tools/quantization/onnx_quantizer.py#L76), the quantization tools support the following attributes:\n   * whether per_channel\n   * whether reduce_range\n   * QLinear mode, QDQ mode or Integer mode (which is only seen in onnxruntime)\n   * whether static (static quantization or dynamic quantization)\n   * weight_qtype (choices are float32, int8 and uint8)\n   * input_qtype (choices are float32, int8 and uint8)\n   * quantization_params (None if dynamic quantization)\n   * nodes_to_quantize, nodes_to_exclude\n   * op_types_to_quantize\n   We define three configuration files to describe the capability of ONNXRT. Please refer to [onnxrt_qlinear.yaml](../neural_compressor/adaptor/onnxrt_qlinear.yaml), [onnxrt_integer.yaml](../neural_compressor/adaptor/onnxrt_integer.yaml) and [onnxrt_qdq.yaml](../neural_compressor/adaptor/onnxrt_qdq.yaml).", "doc_id": 23},
{"doc": "# Intel Neural Compressor Adaptor\n## Example of Adding a New Backend Support\n### Implement ONNXRTAdaptor Class\n   The base class ONNXRTAdaptor inherits from the Adaptor class. Please refer to [onnxrt.py](../neural_compressor/adaptor/onnxrt.py).\n   ```python\n    @adaptor_registry\n    class ONNXRT_QLinearOpsAdaptor(ONNXRTAdaptor):\n      @dump_elapsed_time(\"Pass quantize model\")\n      def quantize(self, tune_cfg, model, data_loader, q_func=None):\n        ......\n\n      @dump_elapsed_time(\"Pass recover model\")\n      def recover(self, model, q_config):\n        ......\n\n      def inspect_tensor(self, model, dataloader, op_list=[],\n                       iteration_list=[],\n                       inspect_type='activation',\n                       save_to_disk=False,\n                       save_path=None,\n                       quantization_cfg=None):\n        ......\n\n      def set_tensor(self, model, tensor_dict):\n        ......\n\n      def query_fw_capability(self, model):\n        ......\n\n      def evaluate(self, input_graph, dataloader, postprocess=None,\n                 metrics=None, measurer=None, iteration=-1,\n                 tensorboard=False, fp32_baseline=False):\n        ......\n\n      def diagnosis_helper(self, fp32_model, int8_model, tune_cfg=None, save_path=None):\n        ......\n\n      def save(self, model, path):\n        ......\n   ```", "doc_id": 24},
{"doc": "# Intel Neural Compressor Benchmarking\n## Introduction\nThe benchmarking feature of Neural Compressor is used to measure the model performance with the objective settings. \nUsers can get the performance of the float32 model and the optimized low precision model in the same scenarios.", "doc_id": 25},
{"doc": "# Intel Neural Compressor Benchmarking\n## Get Started with Benchmark API\n\nBenchmark provide capability to automatically run with multiple instance through `cores_per_instance` and `num_of_instance` config (CPU only). \nAnd please make sure `cores_per_instance * num_of_instance` must be less than CPU physical core numbers. \n`benchmark.fit` accept `b_dataloader` or `b_func` as input. \n`b_func` is customized benchmark function. If user passes the `b_dataloader`, then `b_func` is not required.\n\n```python\nfrom neural_compressor.config import BenchmarkConfig\nfrom neural_compressor.benchmark import fit\nconf = BenchmarkConfig(warmup=10, iteration=100, cores_per_instance=4, num_of_instance=7)\nfit(model='./int8.pb', config=conf, b_dataloader=eval_dataloader)", "doc_id": 26},
{"doc": "# Intel Neural Compressor Benchmarking\n## Examples\n\nRefer to the [Benchmark example](../../examples/helloworld/tf_example5).", "doc_id": 27},
{"doc": "# Intel Neural Compressor DataLoader\n## Introduction\n\nDeep Learning often encounters large datasets that are memory-consuming. Previously, working with large datasets required loading them into memory all at once. The constant lack of memory resulted in the need for an efficient data generation scheme. This is not only about handling the lack of memory in large datasets, but also about making the process of loading data faster using a multi-processing thread. We call the data generation object a DataLoader.\n\nWith the importance of a dataloader, different frameworks can have their own DataLoader module. As for Intel\u00ae Neural Compressor, it has implemented an internal dataloader and provides a unified DataLoader API for the following three reasons:\n\n- The framework-specific dataloader has different features and APIs that will make it hard to use them same way in Neural Compressor.\n\n- Neural Compressor treats batch size as a tuning parameter which means it can dynamically change the batch size to reach the accuracy goal.\n\n- Internal dataloader makes it easy to config dataloaders in a yaml file without any code modification.\n\nThe internal dataloader takes a [dataset](./dataset.md) as the input parameter and loads data from the dataset when needed. In special cases, users can also define their own dataloader classes, which must have `batch_size` attribute and `__iter__` function.", "doc_id": 28},
{"doc": "# Intel Neural Compressor DataLoader\n## Supported Framework Dataloader Matrix\n\n| Framework     | Status     |\n|---------------|:----------:|\n| TensorFlow    |  &#10004;  |\n| PyTorch       |  &#10004;  |\n| ONNX Runtime   |  &#10004;  |\n| MXNet         |  &#10004;  |", "doc_id": 29},
{"doc": "# Intel Neural Compressor DataLoader\n## Get Start with Dataloader API\n### Config Dataloader in a Yaml File\nUsers can use internal dataloader in the following manners. In this case, the dataloader is created after the Quantization object is initialized. As calibration and evaluation may have different transforms and datasets, users can config different dataloaders in a yaml file.\n\n```yaml\nquantization:\n  approach: post_training_static_quant\n  calibration:\n    dataloader:\n      dataset:\n        COCORaw:\n          root: /path/to/calibration/dataset\n      filter:\n        LabelBalance:\n          size: 1\n      transform:\n        Resize:\n          size: 300\n\nevaluation:\n  accuracy:\n    metric: \n      ...\n    dataloader:\n      batch_size: 16\n      dataset:\n        COCORaw:\n          root: /path/to/evaluation/dataset\n      transform:\n        Resize:\n          size: 300\n  performance:\n    dataloader:\n      batch_size: 16\n      dataset:\n        dummy_v2:\n          input_shape: [224, 224, 3] \n```", "doc_id": 30},
{"doc": "# Intel Neural Compressor DataLoader\n## Get Start with Dataloader API\n### Create a User-specific Dataloader\nUsers can define their own dataloaders as shown as below:\n\n```python\n\nclass Dataloader:\n    def __init__(self, batch_size, **kwargs):\n        self.batch_size = batch_size\n        self.dataset = []\n        # operations to add (input_data, label) pairs into self.dataset\n\n    def __iter__(self):\n        for input_data, label in self.dataset:\n            yield input_data, label\n\nfrom neural_compressor import quantization, PostTrainingQuantConfig\nconfig = PostTrainingQuantConfig()\ndataloader = Dataloader(batch_size, **kwargs)\nq_model = quantization.fit(model, config, calib_dataloader=dataloader,\n    eval_func=eval)\nq_model.save(args.output_model)\n```", "doc_id": 31},
{"doc": "# Intel Neural Compressor DataLoader\n## Examples\n\n- Refer to this [example](https://github.com/intel/neural-compressor/tree/master/examples/onnxrt/body_analysis/onnx_model_zoo/ultraface/quantization/ptq) for how to define a customised dataloader.\n\n- Refer to this [example](https://github.com/intel/neural-compressor/tree/v1.14.2/examples/onnxrt/image_recognition/resnet50/quantization/ptq) for how to use internal dataloader.", "doc_id": 32},
{"doc": "# Intel Neural Compressor Distillation\n## Introduction\nDistillation is one of popular approaches of network compression, which transfers knowledge from a large model to a smaller one without loss of validity. As smaller models are less expensive to evaluate, they can be deployed on less powerful hardware (such as a mobile device). Graph shown below is the workflow of the distillation, the teacher model will take the same input that feed into the student model to produce the output that contains knowledge of the teacher model to instruct the student model.", "doc_id": 33},
{"doc": "# Intel Neural Compressor Distillation\n### Knowledge Distillation\nKnowledge distillation is proposed in [Distilling the Knowledge in a Neural Network](https://arxiv.org/abs/1503.02531). It leverages the logits (the input of softmax in the classification tasks) of teacher and student model to minimize the the difference between their predicted class distributions, this can be done by minimizing the below loss function. \n\n$$L_{KD} = D(z_t, z_s)$$\n\nWhere $D$ is a distance measurement, e.g. Euclidean distance and Kullback\u2013Leibler divergence, $z_t$ and $z_s$ are the logits of teacher and student model, or predicted distributions from softmax of the logits in case the distance is measured in terms of distribution.", "doc_id": 34},
{"doc": "# Intel Neural Compressor Distillation\n### Intermediate Layer Knowledge Distillation\n\nThere are more information contained in the teacher model beside its logits, for example, the output features of the teacher model's intermediate layers often been used to guide the student model, as in [Patient Knowledge Distillation for BERT Model Compression](https://arxiv.org/pdf/1908.09355) and [MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices](https://arxiv.org/abs/2004.02984). The general loss function for this approach can be summarized as follow.\n\n$$L_{KD} = \\sum\\limits_i D(T_t^{n_i}(F_t^{n_i}), T_s^{m_i}(F_s^{m_i}))$$\n\nWhere $D$ is a distance measurement as before, $F_t^{n_i}$ the output feature of the $n_i$'s layer of the teacher model, $F_s^{m_i}$ the output feature of the $m_i$'s layer of the student model. Since the dimensions of $F_t^{n_i}$ and $F_s^{m_i}$ are usually different, the transformations $T_t^{n_i}$ and $T_s^{m_i}$ are needed to match dimensions of the two features. Specifically, the transformation can take the forms like identity, linear transformation, 1X1 convolution etc.", "doc_id": 35},
{"doc": "# Intel Neural Compressor Distillation\n### Self Distillation\n\nSelf-distillation ia a one-stage training method where the teacher model and student models can be trained together. It attaches several attention modules and shallow classifiers at different depths of neural networks and distills knowledge from the deepest classifier to the shallower classifiers. Different from the conventional knowledge distillation methods where the knowledge of the teacher model is transferred to another student model, self-distillation can be considered as knowledge transfer in the same model, from the deeper layers to the shallower layers.\nThe additional classifiers in self-distillation allow the neural network to work in a dynamic manner, which leads to a much higher acceleration.", "doc_id": 36},
{"doc": "# Intel Neural Compressor Distillation\n## Distillation Support Matrix\n\n|Distillation Algorithm                          |PyTorch   |TensorFlow |\n|------------------------------------------------|:--------:|:---------:|\n|Knowledge Distillation                          |&#10004;  |&#10004;   |\n|Intermediate Layer Knowledge Distillation       |&#10004;  |Will be supported|\n|Self Distillation                               |&#10004;  |&#10006;   |", "doc_id": 37},
{"doc": "# Intel Neural Compressor Distillation\n## Get Started with Distillation API \n\nUser can pass the customized training/evaluation functions to `Distillation` for flexible scenarios. In this case, distillation process can be done by pre-defined hooks in Neural Compressor. User needs to put those hooks inside the training function.\n\nthe launcher code for Knowledge Distillation is like the following:\n\n```python\nfrom neural_compressor.training import prepare_compression\nfrom neural_compressor.config import DistillationConfig, KnowledgeDistillationLossConfig\n\ndistil_loss_conf = KnowledgeDistillationLossConfig()\nconf = DistillationConfig(teacher_model=teacher_model, criterion=distil_loss_conf)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\ncompression_manager = prepare_compression(model, conf)\nmodel = compression_manager.model\n\nmodel = training_func_for_nc(model)\neval_func(model)\n```", "doc_id": 38},
{"doc": "# Intel Neural Compressor Distillation\n## Intermediate Layer Knowledge Distillation or Self Distillation API\nFor Intermediate Layer Knowledge Distillation or Self Distillation, the only difference to above launcher code is that `distil_loss_conf` should be set accordingly as shown below. More detailed settings can be found in this [example](../../examples/pytorch/nlp/huggingface_models/text-classification/optimization_pipeline/distillation_for_quantization/fx/run_glue_no_trainer.py#L510) for Intermediate Layer Knowledge Distillation and this [example](../../examples/pytorch/image_recognition/torchvision_models/self_distillation/eager/main.py#L344) for Self Distillation.\n\n```python\nfrom neural_compressor.config import IntermediateLayersKnowledgeDistillationLossConfig, SelfKnowledgeDistillationLossConfig\n\n# for Intermediate Layer Knowledge Distillation\ndistil_loss_conf = IntermediateLayersKnowledgeDistillationLossConfig(layer_mappings=layer_mappings)\n\n# for Self Distillation\ndistil_loss_conf = SelfKnowledgeDistillationLossConfig(layer_mappings=layer_mappings)\n```", "doc_id": 39},
{"doc": "# Intel Neural Compressor Distillation\n## Examples\n[Distillation PyTorch Examples](../../examples/README.md#distillation-1)\n<br>\n[Distillation TensorFlow Examples](../../examples/README.md#distillation)\n<br>\n[Distillation Examples Results](./validated_model_list.md#validated-knowledge-distillation-examples)", "doc_id": 40},
{"doc": "# Intel Neural Compressor Distillation for Quantization\n### Introduction\n\nDistillation and quantization are both promising methods to reduce the computational and memory footprint that huge transformer-based networks require. Quantization refers to a process of reducing the bit precision for both activations and weights. Distillation method transfers knowledge from a heavy teacher model to a light one (student) and it could be used as a performance-booster in lower-bits quantizations. Quantization-aware training recovers accuracy degradation from representation loss in the retraining process and typically provides better performance compared to post-training quantization. \nIntel provides a quantization-aware training (QAT) method that incorporates a novel layer-by-layer knowledge distillation step for INT8 quantization pipelines.", "doc_id": 41},
{"doc": "# Intel Neural Compressor Distillation for Quantization\n### Distillation for Quantization Support Matrix\n\n|**Algorithm**                      |**PyTorch**   |**TensorFlow** |\n|---------------------------------|:--------:|:---------:|\n|Distillation for Quantization    |&#10004;  |&#10006;   |", "doc_id": 42},
{"doc": "# Intel Neural Compressor Distillation for Quantization\n### Get Started with Distillation for Quantization API\n\nUser can pass the customized training/evaluation functions to `Distillation` for quantization tasks. In this case, distillation process can be done by pre-defined hooks in Neural Compressor. Users could place those hooks inside the quantization training function.\n\nNeural Compressor defines several hooks for user pass\n\n```\non_train_begin() : Hook executed before training begins\non_after_compute_loss(input, student_output, student_loss) : Hook executed after each batch inference of student model\non_epoch_end() : Hook executed at each epoch end\n```\n\nFollowing section illustrates how to use hooks in user pass-in training function:\n\n```python\ndef training_func_for_nc(model):\n    compression_manager.on_train_begin()\n    for epoch in range(epochs):\n        compression_manager.on_epoch_begin(epoch)\n        for i, batch in enumerate(dataloader):\n            compression_manager.on_step_begin(i)\n            ......\n            output = model(batch)\n            loss = output.loss\n            loss = compression_manager.on_after_compute_loss(batch, output, loss)\n            loss.backward()\n            compression_manager.on_before_optimizer_step()\n            optimizer.step()\n            compression_manager.on_step_end()\n        compression_manager.on_epoch_end()\n    compression_manager.on_train_end()\n...\n```\n\nIn this case, the launcher code is like the following:\n\n```python\nfrom neural_compressor.experimental import common, Distillation, Quantization\nfrom neural_compressor.config import DistillationConfig, KnowledgeDistillationLossConfig\nfrom neural_compressor import QuantizationAwareTrainingConfig\nfrom neural_compressor.training import prepare_compression\ncombs = []\ndistillation_criterion = KnowledgeDistillationLossConfig()\nd_conf = DistillationConfig(teacher_model=teacher_model, criterion=distillation_criterion)\ncombs.append(d_conf)\nq_conf = QuantizationAwareTrainingConfig()\ncombs.append(q_conf)\ncompression_manager = prepare_compression(model, combs)\nmodel = compression_manager.model\n\nmodel = training_func_for_nc(model)\neval_func(model)\n```", "doc_id": 43},
{"doc": "# Intel Neural Compressor Distillation for Quantization\n### Examples\n\nFor examples of distillation for quantization, please refer to [distillation-for-quantization examples](../../examples/pytorch/nlp/huggingface_models/text-classification/optimization_pipeline/distillation_for_quantization/fx/README.md)", "doc_id": 44},
{"doc": "# Intel Neural Compressor Distributed Training and Inference (Evaluation)\n## Introduction\n\nNeural Compressor uses [horovod](https://github.com/horovod/horovod) for distributed training.\n\nPlease check horovod installation documentation and use following commands to install horovod:\n```\npip install horovod\n```\n\n# Intel Neural Compressor Distributed Training and Inference (Evaluation)\n## Supported Feature Matrix\nDistributed training and inference are supported in PyTorch and TensorFlow currently.\n| Framework  | Type    | Distributed Support |\n|------------|---------|:-------------------:|\n| PyTorch    | QAT     |       &#10004;      |\n| PyTorch    | PTQ     |       &#10004;      |\n| TensorFlow | PTQ     |       &#10004;      |\n| Keras      | Pruning |       &#10004;      |", "doc_id": 45},
{"doc": "# Intel Neural Compressor Distributed Training and Inference (Evaluation)\n## Get Started with Distributed Training and Inference API\nTo enable distributed training or inference, the steps are:\n\n1. Setting up distributed training or inference scripts. We have 2 options here:\n    - Option 1: Enable distributed training or inference with pure yaml configuration. In this case, Neural Compressor builtin training function is used.\n    - Option 2: Pass the user defined training function to Neural Compressor. In this case, please follow the horovod documentation and below example to know how to write such training function with horovod on different frameworks.\n2. use horovodrun to execute your program.", "doc_id": 46},
{"doc": "# Intel Neural Compressor Distributed Training and Inference (Evaluation)\n## Get Started with Distributed Training and Inference API\n### Option 1: Pure Yaml Configuration\n\nTo enable distributed training in Neural Compressor, user only need to add a field: `Distributed: True` in dataloader configuration:\n\n```\ndataloader:\n  batch_size: 256\n  distributed: True\n  dataset:\n    ImageFolder:\n      root: /path/to/dataset\n```\n\nIn user's code, pass the yaml file to Neural Compressor components, in which it constructs the real dataloader for the distributed training or inference. The example codes are as following. (TensorFlow 1.x additionally needs to enable Eager execution):\n\nDo quantization based on distributed training/inference\n``` \n# import tensorflow as tf                      (Only TensorFlow 1.x needs)\n# tf.compat.v1.enable_eager_execution()        (Only TensorFlow 1.x needs)\nfrom neural_compressor.experimental import Quantization, common\nquantizer = Quantization(\"yaml_file_path\")\nquantizer.model = model\nq_model = quantizer.fit()                          # q_model -> quantized low-precision model \n```\n\nOnly do model accuracy evaluation based on distributed inference\n``` \n# import tensorflow as tf                      (Only TensorFlow 1.x needs)\n# tf.compat.v1.enable_eager_execution()        (Only TensorFlow 1.x needs)\nfrom neural_compressor.experimental import Quantization, common\nquantizer = Quantization(\"yaml_file_path\")\nquantizer.model = model \nquantizer.pre_process()                        # If you simply want to do model evaluation with no need quantization, you should preprocess the quantizer before evaluation.\nresult = quantizer.strategy.evaluation_result  # result -> (accuracy, evaluation_time_cost)\n```", "doc_id": 47},
{"doc": "# Intel Neural Compressor Distributed Training and Inference (Evaluation)\n## Get Started with Distributed Training and Inference API\n### Option 2: User Defined Training Function\n\nNeural Compressor supports User defined PyTorch training function for distributed training which requires user to modify training script following horovod requirements. We provide a MNIST example to show how to do that and following are the steps for PyTorch.\n- Partition dataset via DistributedSampler:\n\n```\ntrain_sampler = torch.utils.data.distributed.DistributedSampler(\n    train_dataset, num_replicas=hvd.size(), rank=hvd.rank())\ntrain_loader = torch.utils.data.DataLoader(train_dataset,**train_kwargs, sampler=train_sampler)\n```\n- Wrap Optimizer:\n\n```\noptimizer = optim.Adadelta(model.parameters(), lr=args.lr * hvd.size())\noptimizer = hvd.DistributedOptimizer(optimizer, named_parameters=model.named_parameters())\n```\n- Broadcast parameters to processes from rank 0:\n\n```\nhvd.broadcast_parameters(model.state_dict(), root_rank=0)\nhvd.broadcast_optimizer_state(optimizer, root_rank=0)\n```\n- Prepare training function:\n\n```\ndef train(args, model, train_loader, optimizer):\n    model.train()\n    for epoch in range(1, args.epochs + 1):\n        train_loader.sampler.set_epoch(epoch)\n        for batch_idx, (data, target) in enumerate(train_loader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            loss.backward()\n            optimizer.step()\n            if batch_idx % args.log_interval == 0:\n                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch, batch_idx * len(data), len(train_loader.sampler),\n                    100. * batch_idx / len(train_loader.sampler), loss.item()))\n                if args.dry_run:\n                    break\n\ndef train_func(model):\n    return train(args, model, train_loader, optimizer)\n```\n\n- Use user defined training function in Neural Compressor:\n\n```\nfrom neural_compressor.experimental import Component, common\ncomponent = Component(yaml_file)\ncomponent.model = model\ncomponent.train_func = train_func\ncomponent.eval_func = test_func\nmodel = component()\n```", "doc_id": 48},
{"doc": "# Intel Neural Compressor Distributed Training and Inference (Evaluation)\n## Get Started with Distributed Training and Inference API\n### Horovodrun Execution\n\nUser needs to use horovodrun to execute distributed training. For more usage, please refer to [horovod documentation](https://horovod.readthedocs.io/en/stable/running_include.html).\n\nFollowing command specified the number of processes and hosts to do distributed training.\n```\nhorovodrun -np <num_of_processes> -H <hosts> python example.py\n```\n\nFor example, the following command means that two processes will be assigned to the two nodes 'node-001' and 'node-002'. The two processes will execute 'example.py' at the same time. One process is executed on node 'node-001' and one process is executed on node 'node-002'.\n```\nhorovodrun -np 2 -H node-001:1,node-002:1 python example.py\n```", "doc_id": 49},
{"doc": "# Intel Neural Compressor Distributed Training and Inference (Evaluation)\n## Examples\n### PyTorch Examples:\n- PyTorch example-1: MNIST\n  - Please follow this README.md exactly\uff1a[MNIST](../../examples/pytorch/image_recognition/mnist)\n\n- PyTorch example-2: QAT (Quantization Aware Training)\n  - Please follow this README.md exactly\uff1a[QAT](../../examples/pytorch/image_recognition/torchvision_models/quantization/qat/eager/distributed)", "doc_id": 50},
{"doc": "# Intel Neural Compressor Distributed Training and Inference (Evaluation)\n### TensorFlow Examples:\n- TensorFlow example-1: 'ResNet50 V1.0' PTQ (Post Training Quantization) with distributed inference    \n  - Step-1: Please cd (change directory) to the [TensorFlow Image Recognition Example](../../examples/tensorflow/image_recognition) and follow the readme to run PTQ, ensure that PTQ of 'ResNet50 V1.0' can be successfully executed.\n  - Step-2: We only need to modify the [resnet50_v1.yaml](../../examples/tensorflow/image_recognition/tensorflow_models/quantization/ptq/resnet50_v1.yaml), add a line 'distributed: True' in the 'evaluation' field.\n    ```\n    # only need to modify the resnet50_v1.yaml, add a line 'distributed: True'\n    ......\n    ......\n    evaluation:                                          # optional. required if user doesn't provide eval_func in neural_compressor.Quantization.\n      accuracy:                                          # optional. required if user doesn't provide eval_func in neural_compressor.Quantization.\n        metric:\n          topk: 1                                        # built-in metrics are topk, map, f1, allow user to register new metric.\n        dataloader:\n          batch_size: 32\n          distributed: True                              # add a line 'distributed: True'\n          dataset:\n            ImageRecord:\n              root: /path/to/evaluation/dataset          # NOTE: modify to evaluation dataset location if needed\n          transform:\n            ResizeCropImagenet: \n              height: 224\n              width: 224\n    ......\n    ......\n    ```\n  - Step-3: Execute 'main.py' with horovodrun as following command, it will realize PTQ based on two-node tow-process distributed inference. Please replace these fields according to your actual situation: 'your_node1_name', 'your_node2_name', '/PATH/TO/'. (Note that if you use TensorFlow 1.x now, you need to add a line 'tf.compat.v1.enable_eager_execution()' into 'main.py' to enable Eager execution.)\n    ```\n    horovodrun -np 2 -H your_node1_name:1,your_node2_name:1 python main.py --tune --config=resnet50_v1.yaml --input-graph=/PATH/TO/resnet50_fp32_pretrained_model.pb --output-graph=./nc_resnet50_v1.pb\n    ```\n- TensorFlow example-2: 'resnet_v2' pruning on Keras backend with distributed training and inference\n   - Please follow this README.md exactly\uff1a[Pruning](../../examples/tensorflow/image_recognition/resnet_v2)", "doc_id": 51},
{"doc": "# Intel Neural Compressor Model Export\n# Introduction\nOpen Neural Network Exchange (ONNX) is an open standard format for representing machine learning models. Exporting FP32 PyTorch/Tensorflow models has become popular and easy to use. However, for Intel Neural Compressor, we hope to export the INT8 model into the ONNX format to achieve higher applicability in multiple frameworks.\n\nHere we briefly introduce our export API for PyTorch FP32/INT8 models. First, the INT8 ONNX model is not directly exported from the INT8 PyTorch model, but quantized after obtaining the FP32 ONNX model using the mature torch.onnx.export API. To ensure the majority of the quantization process of ONNX is consistent with PyTorch, we reuse three key pieces of information from the Neural Compressor model to perform ONNX quantization.\n\n - Quantized operations: Only operations quantized in PyTorch will be quantized in the quantization process of ONNX.\n - Scale info: Scale information is collected from the quantization process of PyTorch.\n - Weights of quantization aware training(QAT): For quantization aware training, the updated weights are passed to the ONNX model.", "doc_id": 52},
{"doc": "# Intel Neural Compressor Model Export\n# Supported Framework Model Matrix\n\n| Export | PyTorch | TensorFlow |\n| :---: | :---: |:----------:|\n| FP32 Model -> FP32 ONNX Model | &#10004; |  &#10004;  |\n| INT8 Model -> INT8 QDQ ONNX Model | &#10004; |  &#10004;  |\n| INT8 Model -> INT8 QLinear ONNX Model | &#10004; | :x: |", "doc_id": 53},
{"doc": "# Intel Neural Compressor Model Export\n# Examples\n\n## FP32 Model Export\n```python\nfrom neural_compressor.experimental.common import Model\nfrom neural_compressor.config import Torch2ONNXConfig\ninc_model = Model(model)\nfp32_onnx_config = Torch2ONNXConfig(\n    dtype=\"fp32\",\n    example_inputs=torch.randn(1, 3, 224, 224),\n    input_names=['input'],\n    output_names=['output'],\n    dynamic_axes={\"input\": {0: \"batch_size\"},\n                    \"output\": {0: \"batch_size\"}},\n)\ninc_model.export('fp32-model.onnx', fp32_onnx_config)\n```", "doc_id": 54},
{"doc": "# Intel Neural Compressor Model Export\n# Examples\n## INT8 Model Export\n\n```python\n# q_model is a Neural Compressor model after performing quantization.\nfrom neural_compressor.config import Torch2ONNXConfig\nint8_onnx_config = Torch2ONNXConfig(\n    dtype=\"int8\",\n    opset_version=14,\n    quant_format=\"QDQ\", # or QLinear\n    example_inputs=torch.randn(1, 3, 224, 224),\n    input_names=['input'],\n    output_names=['output'],\n    dynamic_axes={\"input\": {0: \"batch_size\"},\n                    \"output\": {0: \"batch_size\"}},\n)\nq_model.export('int8-model.onnx', int8_onnx_config)\n```\n> **Note**: Two export examples covering computer vision and natural language processing tasks exist in examples. Users can leverage them to verify the accuracy and performance of the exported ONNX model.\n - [Image recognition](/examples/pytorch/image_recognition/torchvision_models/export/fx/)\n - [Text classification](/examples/pytorch/nlp/huggingface_models/text-classification/export/fx/)", "doc_id": 55},
{"doc": "# Intel Neural Compressor Frequently Asked Questions\n## Common Build Issues\n#### Issue 1: \nLack of toolchain in a bare metal linux ENV.   \n**Solution:** \n```shell\nsudo apt-get update && sudo apt-get install -y python3 python3-pip python3-dev python3-distutils build-essential git libgl1-mesa-glx libglib2.0-0 numactl wget\nln -sf $(which python3) /usr/bin/python\n```", "doc_id": 56},
{"doc": "# Intel Neural Compressor Frequently Asked Questions\n## Common Build Issues\n#### Issue 2:  \nValueError: numpy.ndarray size changed, may indicate binary incompatibility. Expected 88 from C header, got 80 from PyObject     \n**Solution:** reinstall pycocotools by \"pip install pycocotools --no-cache-dir\"", "doc_id": 57},
{"doc": "# Intel Neural Compressor Frequently Asked Questions\n## Common Build Issues\n#### Issue 3:\nImportError: libGL.so.1: cannot open shared object file: No such file or directory   \n**Solution:** apt install or yum install python3-opencv", "doc_id": 58},
{"doc": "# Intel Neural Compressor Frequently Asked Questions\n## Common Build Issues\n#### Issue 4:  \nConda package *neural-compressor-full* dependency conflict will pending conda installation for a long time.\n**Solution:** run *conda install sqlalchemy=1.4.27 alembic=1.7.7 -c conda-forge* before install *neural-compressor-full*.", "doc_id": 59},
{"doc": "# Intel Neural Compressor Framework YAML Configuration Files\n## Introduction\n\nIntel\u00ae Neural Compressor uses YAML files for quick \nand user-friendly configurations. There are two types of YAML files - \nuser YAML files and framework YAML files, which are used in \nrunning user cases and setting up framework capabilities, respectively. \n\nHere, we introduce the framework YAML file, which describes the behavior of \na specific framework. There is a corresponding framework YAML file for each framework supported by \nIntel\u00ae Neural Compressor - TensorFlow\n, Intel\u00ae Extension for TensorFlow*, PyTorch, Intel\u00ae Extension for PyTorch*, ONNX Runtime, and MXNet. \n\n>**Note**: Before diving to the details, we recommend that the end users do NOT make modifications\nunless they have clear requirements that can only be met by modifying the attributes.", "doc_id": 60},
{"doc": "# Intel Neural Compressor Framework YAML Configuration Files\n## Supported Feature Matrix\n\n| Framework  | YAML Configuration Files |\n|------------|:------------------------:|\n| TensorFlow |         &#10004;         |\n| PyTorch    |         &#10004;         |\n| ONNX       |         &#10004;         |\n| MXNet      |         &#10004;         |", "doc_id": 61},
{"doc": "# Intel Neural Compressor Framework YAML Configuration Files\n## Get started with Framework YAML Files\n\nFor the purpose of framework setup, let's take a look at a tensorflow framework YAML file;\nother framework YAML files follow same syntax. A framework YAML file specifies following\ninformation and capabilities for current runtime framework. Let's go through \nthem one by one: \n\n* ***version***: This specifies the supported versions. \n```yaml\n  version:\n    name: ['2.1.0', '2.2.0', '2.3.0', '2.4.0', '2.5.0', '2.6.0', '2.6.1', '2.6.2', '2.7.0', '2.8.0', '1.15.0-up1', '1.15.0-up2']\n```\n\n* ***precisions***: This defines the supported precisions of specific versions. \n```yaml\n  precisions: \n    names: int8, uint8, bf16, fp32\n    valid_mixed_precisions: []\n```\n* ***op***: This defines a list of valid OP types for each precision.\n```yaml\n  ops: \n    int8: ['Conv2D', 'MatMul', 'ConcatV2', 'MaxPool', 'AvgPool']\n    uint8: ['Conv2D', 'DepthwiseConv2dNative', 'MatMul', 'ConcatV2', 'MaxPool', 'AvgPool']\n    bf16: ['Conv2D']  \n    fp32: ['*'] # '*' means all op types\n```\n* ***capabilities***: This defines the quantization ability of specific ops, such as\ngranularity, scheme, and algorithm. The activation assumes that input and output activations\nshare the same data type by default, which is based on op semantics defined by\nframeworks. \n```yaml\n  capabilities: \n    int8: {\n          'Conv2D': {\n            'weight': {\n                        'dtype': ['int8', 'fp32'],\n                        'scheme': ['sym'],\n                        'granularity': ['per_channel','per_tensor'],\n                        'algorithm': ['minmax']\n                        },\n            'activation': {\n                        'dtype': ['int8', 'fp32'],\n                        'scheme': ['sym'],\n                        'granularity': ['per_tensor'],\n                        'algorithm': ['minmax', 'kl']\n                        }\n                    },\n          'MatMul': {\n            'weight': {\n                        'dtype': ['int8', 'fp32'],\n                        'scheme': ['sym'],\n                        'granularity': ['per_tensor'],\n                        'algorithm': ['minmax']\n                        },\n            'activation': {\n                        'dtype': ['int8', 'fp32'],\n                        'scheme': ['asym', 'sym'],\n                        'granularity': ['per_tensor'],\n                        'algorithm': ['minmax']\n                        }\n                    },\n          'default': {\n            'activation': {\n                        'dtype': ['uint8', 'fp32'],\n                        'algorithm': ['minmax'],\n                        'scheme': ['sym'],\n                        'granularity': ['per_tensor']\n                        }\n                    },\n          }\n\n    uint8: {\n          'Conv2D': {\n            'weight': {\n                        'dtype': ['int8', 'fp32'],\n                        'scheme': ['sym'],\n                        'granularity': ['per_channel','per_tensor'],\n                        'algorithm': ['minmax']\n                        },\n            'activation': {\n                        'dtype': ['uint8', 'fp32'],\n                        'scheme': ['sym'],\n                        'granularity': ['per_tensor'],\n                        'algorithm': ['minmax', 'kl']\n                        }\n                    },\n          'MatMul': {\n            'weight': {\n                        'dtype': ['int8', 'fp32'],\n                        'scheme': ['sym'],\n                        'granularity': ['per_tensor'],\n                        'algorithm': ['minmax']\n                        },\n            'activation': {\n                        'dtype': ['uint8', 'fp32'],\n                        'scheme': ['asym', 'sym'],\n                        'granularity': ['per_tensor'],\n                        'algorithm': ['minmax']\n                        }\n                    },\n          'default': {\n            'activation': {\n                        'dtype': ['uint8', 'fp32'],\n                        'algorithm': ['minmax'],\n                        'scheme': ['sym'],\n                        'granularity': ['per_tensor']\n                        }\n                    },\n          }\n```\n* ***patterns***: This defines the supported fusion sequence for each op. \n```yaml\n  patterns: \n    fp32: [ \n        'Conv2D + Add + Relu',\n        'Conv2D + Add + Relu6',\n        'Conv2D + Relu',\n        'Conv2D + Relu6',\n        'Conv2D + BiasAdd'\n        ]\n    int8: [\n        'Conv2D + BiasAdd',\n        'Conv2D + BiasAdd + Relu',\n        'Conv2D + BiasAdd + Relu6'\n        ]\n    uint8: [\n        'Conv2D + BiasAdd + AddN + Relu',\n        'Conv2D + BiasAdd + AddN + Relu6',\n        'Conv2D + BiasAdd + AddV2 + Relu',\n        'Conv2D + BiasAdd + AddV2 + Relu6',\n        'Conv2D + BiasAdd + Add + Relu',\n        'Conv2D + BiasAdd + Add + Relu6',\n        'Conv2D + BiasAdd + Relu',\n        'Conv2D + BiasAdd + Relu6',\n        'Conv2D + Add + Relu',\n        'Conv2D + Add + Relu6',\n        'Conv2D + Relu',\n        'Conv2D + Relu6',\n        'Conv2D + BiasAdd',\n        'DepthwiseConv2dNative + BiasAdd + Relu6',\n        'DepthwiseConv2dNative + BiasAdd + Relu',\n        'DepthwiseConv2dNative + Add + Relu6',\n        'DepthwiseConv2dNative + BiasAdd',\n        'MatMul + BiasAdd + Relu',\n        'MatMul + BiasAdd',\n  ]\n```\n\n* ***grappler_optimization***: This defines the grappler optimization. \n```yaml\n  grappler_optimization: \n    pruning: True                                    # optional. grappler pruning optimizer,default value is True.\n    shape: True                                      # optional. grappler shape optimizer,default value is True.\n    constfold: False                                 # optional. grappler constant folding optimizer, default value is True.\n    arithmetic: False                                # optional. grappler arithmetic optimizer,default value is False.\n    dependency: True                                 # optional. grappler dependency optimizer,default value is True.\n    debug_stripper: True                             # optional. grappler debug_stripper optimizer,default value is True.\n    loop: True                                       # optional. grappler loop optimizer,default value is True.\n\n```", "doc_id": 62},
{"doc": "# Intel Neural Compressor examples\n## Quick Samples\n### Quantization with Python API\n\n```shell\n# Install Intel Neural Compressor and TensorFlow\npip install neural-compressor\npip install tensorflow\n# Prepare fp32 model\nwget https://storage.googleapis.com/intel-optimized-tensorflow/models/v1_6/mobilenet_v1_1.0_224_frozen.pb\n```\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig\nfrom neural_compressor.data import DataLoader\nfrom neural_compressor.data import Datasets\n\ndataset = Datasets('tensorflow')['dummy'](shape=(1, 224, 224, 3))\ndataloader = DataLoader(framework='tensorflow', dataset=dataset)\n\nfrom neural_compressor.quantization import fit\nconfig = PostTrainingQuantConfig()\nq_model = fit(\n    model=\"./mobilenet_v1_1.0_224_frozen.pb\",\n    conf=config,\n    calib_dataloader=dataloader,\n    eval_dataloader=dataloader)\n```", "doc_id": 63},
{"doc": "# Intel Neural Compressor examples\n## Quick Samples\n### Quantization with [JupyterLab Extension](/neural_coder/extensions/neural_compressor_ext_lab/README.md)\n\nSearch for ```jupyter-lab-neural-compressor``` in the Extension Manager in JupyterLab and install with one click:\n\n<a target=\"_blank\" href=\"/neural_coder/extensions/screenshots/extmanager.png\">\n  <img src=\"/neural_coder/extensions/screenshots/extmanager.png\" alt=\"Extension\" width=\"35%\" height=\"35%\">\n</a>", "doc_id": 64},
{"doc": "# Intel Neural Compressor examples\n## Quick Samples\n### Quantization with [GUI](./bench.md)\n\n```shell\n# Install Intel Neural Compressor and ONNX\npip install neural-compressor-full\npip install onnx==1.12.0 onnxruntime==1.12.1 onnxruntime-extensions\n# Prepare fp32 model\nwget https://github.com/onnx/models/raw/main/vision/classification/resnet/model/resnet50-v1-12.onnx\n# Start GUI\ninc_bench\n```", "doc_id": 65},
{"doc": "# Intel Neural Compressor examples\n## Validated Models\nIntel\u00ae Neural Compressor validated the quantization for 10K+ models from popular model hubs (e.g., HuggingFace Transformers, Torchvision, TensorFlow Model Hub, ONNX Model Zoo). \nOver 30 pruning, knowledge distillation and model export samples are also available. \nMore details for validated typical models are available [here](/examples/README.md).", "doc_id": 66},
{"doc": "# Intel Neural Compressor Installation\n## Linux Installation\n### Prerequisites\nYou can install Neural Compressor using one of three options: Install single component from binary or source, or get the Intel-optimized framework together with the library by installing the [Intel\u00ae oneAPI AI Analytics Toolkit](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html).\n\nThe following prerequisites and requirements must be satisfied for a successful installation:\n\n- Python version: 3.7 or 3.8 or 3.9 or 3.10\n\n> Notes:\n> - Please choose one of the basic or full installation mode for your environment, **DO NOT** install both. If you want to re-install with the other mode, please uninstall the current package at first.\n> - If you get some build issues, please check [frequently asked questions](faq.md) at first.", "doc_id": 67},
{"doc": "# Intel Neural Compressor Installation\n## Linux Installation\n### Install from Binary\n\n  ```Shell\n  # install stable basic version from pypi\n  pip install neural-compressor\n  # or install stable full version from pypi (including GUI)\n  pip install neural-compressor-full\n  ```\n\n  ```Shell\n  # install nightly version\n  git clone https://github.com/intel/neural-compressor.git\n  cd neural-compressor\n  pip install -r requirements.txt\n  # install nightly basic version from pypi\n  pip install -i https://test.pypi.org/simple/ neural-compressor\n  # or install nightly full version from pypi (including GUI)\n  pip install -i https://test.pypi.org/simple/ neural-compressor-full\n  ```\n  ```Shell\n  # install stable basic version from from conda\n  conda install neural-compressor -c conda-forge -c intel\n  # or install stable full version from from conda (including GUI)\n  conda install sqlalchemy=1.4.27 alembic=1.7.7 -c conda-forge\n  conda install neural-compressor-full -c conda-forge -c intel\n  ```", "doc_id": 68},
{"doc": "# Intel Neural Compressor Installation\n## Linux Installation\n### Install from Source\n\n  ```Shell\n  git clone https://github.com/intel/neural-compressor.git\n  cd neural-compressor\n  pip install -r requirements.txt\n  # build with basic functionality\n  python setup.py install\n  # build with full functionality (including GUI)\n  python setup.py --full install\n  ```", "doc_id": 69},
{"doc": "# Intel Neural Compressor Installation\n## Linux Installation\n### Install from AI Kit\n\nThe Intel\u00ae Neural Compressor library is released as part of the [Intel\u00ae oneAPI AI Analytics Toolkit](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit.html) (AI Kit). The AI Kit provides a consolidated package of Intel's latest deep learning and machine optimizations all in one place for ease of development. Along with Neural Compressor, the AI Kit includes Intel-optimized versions of deep learning frameworks (such as TensorFlow and PyTorch) and high-performing Python libraries to streamline end-to-end data science and AI workflows on Intel architectures.\n\nThe AI Kit is distributed through many common channels, including from Intel's website, YUM, APT, Anaconda, and more. Select and [download](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit/download.html) the AI Kit distribution package that's best suited for you and follow the [Get Started Guide](https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html) for post-installation instructions.\n\n|Download|Guide|\n|-|-|\n|[Download AI Kit](https://software.intel.com/content/www/us/en/develop/tools/oneapi/ai-analytics-toolkit/) |[AI Kit Get Started Guide](https://software.intel.com/content/www/us/en/develop/documentation/get-started-with-ai-linux/top.html) |", "doc_id": 70},
{"doc": "# Intel Neural Compressor Installation\n## Windows Installation\n### Prerequisites\nThe following prerequisites and requirements must be satisfied for a successful installation:\n- Python version: 3.7 or 3.8 or 3.9 or 3.10", "doc_id": 71},
{"doc": "# Intel Neural Compressor Installation\n## Windows Installation\n### Install from Binary\n\n  ```Shell\n  # install stable basic version from pypi\n  pip install neural-compressor\n  # or install stable full version from pypi (including GUI)\n  pip install neural-compressor-full\n  ```\n\n  ```Shell\n  # install stable basic version from from conda\n  conda install pycocotools -c esri\n  conda install neural-compressor -c conda-forge -c intel\n  # or install stable full version from from conda (including GUI)\n  conda install pycocotools -c esri\n  conda install sqlalchemy=1.4.27 alembic=1.7.7 -c conda-forge\n  conda install neural-compressor-full -c conda-forge -c intel\n  ```", "doc_id": 72},
{"doc": "# Intel Neural Compressor Installation\n## Windows Installation\n### Install from Source\n\n```Shell\n  git clone https://github.com/intel/neural-compressor.git\n  cd neural-compressor\n  pip install -r requirements.txt\n  # build with basic functionality\n  python setup.py install\n  # build with full functionality (including GUI)\n  python setup.py --full install\n  ```", "doc_id": 73},
{"doc": "# Intel Neural Compressor Installation\n## System Requirements\n### Validated Hardware Environment\n#### Intel\u00ae Neural Compressor supports CPUs based on [Intel 64 architecture or compatible processors](https://en.wikipedia.org/wiki/X86-64):\n\n* Intel Xeon Scalable processor (formerly Skylake, Cascade Lake, Cooper Lake, Ice Lake, and Sapphire Rapids)\n* Intel Xeon CPU Max Series (formerly Sapphire Rapids HBM)", "doc_id": 74},
{"doc": "# Intel Neural Compressor Installation\n## System Requirements\n### Validated Hardware Environment\n#### Intel\u00ae Neural Compressor supports GPUs built on Intel's Xe architecture:\n\n* Intel Data Center GPU Flex Series (formerly Arctic Sound-M)\n* Intel Data Center GPU Max Series (formerly Ponte Vecchio)", "doc_id": 75},
{"doc": "# Intel Neural Compressor Installation\n## System Requirements\n### Validated Hardware Environment\n#### Intel\u00ae Neural Compressor quantized ONNX models support multiple hardware vendors through ONNX Runtime:\n\n* Intel CPU, AMD/ARM CPU, and NVidia GPU. Please refer to the validated model [list](./validated_model_list.md#validated-onnx-qdq-int8-models-on-multiple-hardware-through-onnx-runtime).", "doc_id": 76},
{"doc": "# Intel Neural Compressor Installation\n## System Requirements\n### Validated Software Environment\n* OS version: CentOS 8.4, Ubuntu 22.04\n* Python version: 3.7, 3.8, 3.9, 3.10\n<table class=\"docutils\">\n<thead>\n  <tr style=\"vertical-align: middle; text-align: center;\">\n    <th>Framework</th>\n    <th>TensorFlow</th>\n    <th>Intel<br>TensorFlow</th>\n    <th>Intel\u00ae<br>Extension for<br>TensorFlow*</th>\n    <th>PyTorch</th>\n    <th>Intel\u00ae<br>Extension for<br>PyTorch*</th>\n    <th>ONNX<br>Runtime</th>\n    <th>MXNet</th>\n  </tr>\n</thead>\n<tbody>\n  <tr align=\"center\">\n    <th>Version</th>\n    <td class=\"tg-7zrl\"><a href=https://github.com/tensorflow/tensorflow/tree/v2.12.0>2.12.0</a><br>\n    <a href=https://github.com/tensorflow/tensorflow/tree/v2.11.0>2.11.0</a><br>\n    <a href=https://github.com/tensorflow/tensorflow/tree/v2.10.1>2.10.1</a><br></td>\n    <td class=\"tg-7zrl\"><a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.11.0>2.11.0</a><br>\n    <a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.10.0>2.10.0</a><br>\n    <a href=https://github.com/Intel-tensorflow/tensorflow/tree/v2.9.1>2.9.1</a><br></td>\n    <td class=\"tg-7zrl\"><a href=https://github.com/intel/intel-extension-for-tensorflow/tree/v1.1.0>1.1.0</a><br>\n    <a href=https://github.com/intel/intel-extension-for-tensorflow/tree/v1.0.0>1.0.0</a></td>\n    <td class=\"tg-7zrl\"><a href=https://download.pytorch.org/whl/torch_stable.html>2.0.0+cpu</a><br>\n    <a href=https://download.pytorch.org/whl/torch_stable.html>1.13.0+cpu</a><br>\n    <a href=https://download.pytorch.org/whl/torch_stable.html>1.12.1+cpu</a><br></td>\n    <td class=\"tg-7zrl\"><a href=https://github.com/intel/intel-extension-for-pytorch/tree/v2.0.0+cpu>2.0.0+cpu</a><br>\n    <a href=https://github.com/intel/intel-extension-for-pytorch/tree/v1.13.0+cpu>1.13.0+cpu</a><br>\n    <a href=https://github.com/intel/intel-extension-for-pytorch/tree/v1.12.100>1.12.1+cpu</a><br></td>\n    <td class=\"tg-7zrl\"><a href=https://github.com/microsoft/onnxruntime/tree/v1.14.1>1.14.1</a><br>\n    <a href=https://github.com/microsoft/onnxruntime/tree/v1.13.1>1.13.1</a><br>\n    <a href=https://github.com/microsoft/onnxruntime/tree/v1.12.1>1.12.1</a><br></td>\n    <td class=\"tg-7zrl\"><a href=https://github.com/apache/incubator-mxnet/tree/1.9.1>1.9.1</a><br></td>\n  </tr>\n</tbody>\n</table>\n\n> **Note:**\n> Set the environment variable ``TF_ENABLE_ONEDNN_OPTS=1`` to enable oneDNN optimizations if you are using TensorFlow before v2.9. oneDNN is the default for TensorFlow since [v2.9](https://github.com/tensorflow/tensorflow/releases/tag/v2.9.0) ([Intel Cascade Lake](https://www.intel.com/content/www/us/en/products/platforms/details/cascade-lake.html) and newer CPUs).", "doc_id": 77},
{"doc": "# Intel Neural Compressor Metrics\n## Introduction\n\nIn terms of evaluating the performance of a specific model, we should have general metrics to measure the performance of different models. Different frameworks always have their own Metric module but with different APIs and parameters. Neural Compressor Metrics supports code-free configuration through a yaml file, with built-in metrics, so that Neural Compressor can achieve performance and accuracy without code changes from the user. In special cases, users can also register their own metric classes through [building custom metric in code](#build-custom-metric-in-code).", "doc_id": 78},
{"doc": "# Intel Neural Compressor Metrics\n## Get Start with Metrics\n\n### Support Single-metric and Multi-metrics\nUsers can specify an Neural Compressor built-in metric such as shown below:\n\n```yaml\nevaluation:\n  accuracy:\n    metric:\n      topk: 1\n```\n\nIn some cases, users want to use more than one metric to evaluate the performance of a specific model and they can realize it with multi_metrics of Neural Compressor. Currently multi_metrics supports built-in metrics.\n\nThere are two usages for multi_metrics of Neural Compressor:\n\n1. Evaluate performance of a model with metrics one by one\n```yaml\nevaluation:\n  accuracy:\n    multi_metrics:\n      topk: 1\n      MSE:\n        compare_label: False\n      higher_is_better: [True, False] # length of higher_is_better should be equal to num of metric, default is True\n```\n2. Evaluate performance of a model with weighted metric results\n\n```yaml\nevaluation:\n  accuracy:\n    multi_metrics:\n      topk: 1\n      MSE:\n        compare_label: False\n      weight: [0.5, 0.5] # length of weight should be equal to num of metric\n      higher_is_better: [True, False] # length of higher_is_better should be equal to num of metric, default is True\n```", "doc_id": 79},
{"doc": "# Intel Neural Compressor Metrics\n## Get Start with Metrics\n### Build Custom Metric with Python API\n\nPlease refer to [Metrics code](../neural_compressor/experimental/metric), users can also register their own metric as follows:\n\n```python\nclass NewMetric(object):\n    def __init__(self):\n        # init code here\n\n    def update(self, preds, labels):\n        # add preds and labels to storage\n\n    def reset(self):\n        # clear preds and labels storage\n\n    def result(self):\n        # calculate accuracy\n        return accuracy\n\n```\nThe result() function returns a higher-is-better scalar to reflect model accuracy on an evaluation dataset.\nAfter defining the metric class, users can initialize it and pass it to quantizer:\n\n```python\nfrom neural_compressor.quantization import Quantization\nquantizer = Quantization(yaml_file)\nquantizer.model = graph\nquantizer.metric = NewMetric()\nquantizer.calib_dataloader = dataloader\nq_model = quantizer.fit()\n```", "doc_id": 80},
{"doc": "# Code Migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X\nIntel Neural Compressor is a powerful open-source Python library that runs on Intel CPUs and GPUs, which delivers unified interfaces across multiple deep-learning frameworks for popular network compression technologies such as quantization, pruning, and knowledge distillation. We conduct several changes to our old APIs based on Intel Neural Compressor 1.X to make it more user-friendly and convenient for using. Just some simple steps could upgrade your code from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X.", "doc_id": 81},
{"doc": "# Code Migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X\n## Model Quantization\n\nModel Quantization is a very popular deep learning model optimization technique designed for improving the speed of model inference, which is a fundamental function in Intel Neural Compressor. There are two model quantization methods, Quantization Aware Training (QAT) and Post-training Quantization (PTQ). Our tool has provided comprehensive supports for these two kinds of model quantization methods in both Intel Neural Compressor 1.X and Intel Neural Compressor 2.X.", "doc_id": 82},
{"doc": "# Code Migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X\n## Model Quantization\n### Post-training Quantization\n\nPost-training Quantization is the most easy way to quantize the model from FP32 into INT8 format offline. \n\n**Quantization with Intel Neural Compressor 1.X**\n\nIn Intel Neural Compressor 1.X, we resort to a `conf.yaml` to inject the config of the quantization settings.\n\n```python\n# main.py\n\n# Basic settings of the model and the experimental settings for GLUE tasks.\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nval_dataset = ...\nval_dataloader = torch.utils.data.Dataloader(\n                     val_dataset,\n                     batch_size=args.batch_size, shuffle=False,\n                     num_workers=args.workers, ping_memory=True)\ndef eval_func(model):\n    ...\n\n# Quantization code\nfrom neural_compressor.experimental import Quantization, common\ncalib_dataloader = eval_dataloader\nquantizer = Quantization('conf.yaml')\nquantizer.eval_func = eval_func\nquantizer.calib_dataloader = calib_dataloader\nquantizer.model = common.Model(model)\nmodel = quantizer.fit()\n\nfrom neural_compressor.utils.load_huggingface import save_for_huggingface_upstream\n    save_for_huggingface_upstream(model, tokenizer, output_dir)\n\n```\n\nWe formulate the `conf.yaml` as in (https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/ptq.yaml)\n\n**Quantization with Intel Neural Compressor 2.X**\n\nIn Intel Neural Compressor 2.X, we integrate the `conf.yaml` into `main.py` to save the user's effort to write the `conf.yaml`, that most of config information could be set via the `PostTrainingQuantConfig`. The corresponding information should be written as follows (Note: the corresponding names of the parameters in 1.X yaml file are attached in the comment.),\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion, AccuracyCriterion\n\nPostTrainingQuantConfig(\n  ## model: this parameter does not need to specially be defined;\n  backend=\"default\",        # framework: set as \"default\" when framework was tensorflow, pytorch, pytorch_fx, onnxrt_integer and onnxrt_qlinear. Set as \"ipex\" when framework was pytorch_ipex, mxnet is currently unsupported;\n  inputs=\"image_tensor\",    # input: same as in the conf.yaml;\n  outputs=\"num_detections,detection_boxes,detection_scores,detection_classes\" # output: same as in the conf.yaml;\n  device=\"cpu\",             # device: same as in the conf.yaml;\n  approach=\"static\",        # approach: set as \"static\" when approach was \"post_training_static_quant\". Set as \"dynamic\" when approach was \"post_training_dynamic_quant\";\n  ## recipes: this parameter does not need to specially be defined;\n  calibration_sampling_size=[1000, 2000],   # sampling_size: same as in the conf.yaml;\n  ## transform: this parameter does not need to specially be defined;\n  ## model_wise: this parameter does not need to specially be defined;\n  op_name_dict=op_dict,     # op_wise: same as in the conf.yaml;\n  ## evaluation: these parameters do not need to specially be defined;\n  strategy=\"basic\",         # tuning.strategy.name: same as in the conf.yaml;\n  ## tuning.strategy.sigopt_api_token, tuning.strategy.sigopt_project_id and tuning.strategy.sigopt_experiment_name do not need to specially defined;\n  objective=\"performance\",  # tuning.objective: same as in the conf.yaml;\n  performance_only=False,    # tuning.performance_only: same as in the conf.yaml;\n  tuning_criterion=tuning_criterion,\n  accuracy_criterion=accuracy_criterion,\n  ## tuning.random_seed and tuning.tensorboard: these parameters do not need to specially be defined;\n)\n\naccuracy_criterion=AccuracyCriterion(\n  tolerable_loss=0.01,      # relative: same as in the conf.yaml;\n)\ntuning_criterion=TuningCriterion(\n  timeout=0,                # timeout: same as in the conf.yaml;\n  max_trials=100,           # max_trials: same as in the conf.yaml;\n)          \n```\nFollowing is a simple demo about how to quantize the model with PTQ in Intel Neural Compressor 2.X.\n```python\n# Basic settings of the model and the experimental settings for GLUE tasks.\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\nval_dataset = ...\nval_dataloader = torch.utils.data.Dataloader(\n                     val_dataset,\n                     batch_size=args.batch_size, shuffle=False,\n                     num_workers=args.workers, ping_memory=True)\ndef eval_func(model):\n    ...\n\n# Quantization code\nfrom neural_compressor.quantization import fit\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\ntuning_criterion = TuningCriterion(max_trials=600)\nconf = PostTrainingQuantConfig(approach=\"static\", tuning_criterion=tuning_criterion)\nq_model = fit(model, conf=conf, calib_dataloader=eval_dataloader, eval_func=eval_func)\nfrom neural_compressor.utils.load_huggingface import save_for_huggingface_upstream\nsave_for_huggingface_upstream(q_model, tokenizer, training_args.output_dir)\n```", "doc_id": 83},
{"doc": "# Code Migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X\n## Model Quantization\n### Quantization Aware Training\n\nQuantization aware training emulates inference-time quantization in the forward pass of the training process by inserting `fake quant` ops before those quantizable ops. With `quantization aware training`, all weights and activations are `fake quantized` during both the forward and backward passes of training. \n\n**Quantization with Intel Neural Compressor 1.X**\n\nIn Intel Neural Compressor 1.X, the difference between the QAT and PTQ is that we need to define the `train_func` in QAT to emulate the training process. The code is compiled as follows,\n\n```python\n# main.py\n\n# Basic settings of the model and the experimental settings for GLUE tasks.\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\ndef eval_func(model):\n    ...\n\ndef train_func(model):\n    ...\n\ntrainer = Trainer(...)\n\n# Quantization code\nfrom neural_compressor.experimental import Quantization, common\nquantizer = Quantization('conf.yaml')\nquantizer.eval_func = eval_func\nquantizer.q_func = train_func\nquantizer.model = common.Model(model)\nmodel = quantizer.fit()\n\nfrom neural_compressor.utils.load_huggingface import save_for_huggingface_upstream\n    save_for_huggingface_upstream(model, tokenizer, output_dir)\n```\n\nSimilar to PTQ, it requires a `conf.yaml` (https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/qat.yaml) to define the quantization configuration in Intel Neural Compressor 1.X.\n\n**Quantization with Intel Neural Compressor 2.X**\n\nIn Intel Neural Compressor 2.X, this `conf.yaml` is set via the `QuantizationAwareTrainingConfig`. The corresponding information should be written as follows (Note: the corresponding names of the parameters in 1.X yaml file are attached in the comment.)\uff0c\n\n```python\nfrom neural_compressor.config import QuantizationAwareTrainingConfig\n\nQuantizationAwareTrainingConfig(\n  ## model: this parameter does not need to specially be defined;\n  backend=\"default\",        # framework: set as \"default\" when framework was tensorflow, pytorch, pytorch_fx, onnxrt_integer and onnxrt_qlinear. Set as \"ipex\" when framework was pytorch_ipex, mxnet is currently unsupported;\n  inputs=\"image_tensor\",    # input: same as in the conf.yaml;\n  outputs=\"num_detections,detection_boxes,detection_scores,detection_classes\" # output: same as in the conf.yaml;\n  device=\"cpu\",             # device: same as in the conf.yaml;\n  ## approach: this parameter does not need to specially be defined;\n  ## train: these parameters do not need to specially be defined;\n  ## model_wise: this parameter does not need to specially be defined;\n  op_name_dict=op_dict,     # op_wise: same as in the conf.yaml;\n  ## evaluation: these parameters do not need to specially be defined;\n  strategy=\"basic\",         # tuning.strategy.name: same as in the conf.yaml;\n  ## tuning.strategy.sigopt_api_token, tuning.strategy.sigopt_project_id and tuning.strategy.sigopt_experiment_name do not need to specially defined;\n  relative=0.01,            # relative: same as in the conf.yaml;\n  timeout=0,                # timeout: same as in the conf.yaml;\n  max_trials=100,           # max_trials: same as in the conf.yaml;\n  objective=\"performance\",  # tuning.objective: same as in the conf.yaml;\n  performance_only=False,    # tuning.performance_only: same as in the conf.yaml;\n  ## tuning.random_seed and tuning.tensorboard: these parameters do not need to specially be defined;\n  ## diagnosis: these parameters do not need to specially be defined;\n)       \n```\nIn Intel Neural Compressor 2.X, we introduce a `compression manager`  to control the training process. It requires to insert a pair of hook `callbacks.on_train_begin` and `callbacks.on_train_end` at the begin of the training and the end of the training. Thus, the quantization code is updated as:\n\n```python\n# main.py\n\n# Basic settings of the model and the experimental settings for GLUE tasks.\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\ntokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\ndef eval_func(model):\n    ...\n\ndef train_func(model):\n    ...\n\ntrainer = Trainer(...)\n\n# Quantization code\nfrom neural_compressor.training import prepare_compression\nfrom neural_compressor.config import QuantizationAwareTrainingConfig\nconf = QuantizationAwareTrainingConfig()\ncompression_manager = prepare_compression(model, conf)\ncompression_manager.callbacks.on_train_begin()\ntrainer.train()\ncompression_manager.callbacks.on_train_end()\n\nfrom neural_compressor.utils.load_huggingface import save_for_huggingface_upstream\nsave_for_huggingface_upstream(compression_manager.model, tokenizer, training_args.output_dir)\n\n```", "doc_id": 84},
{"doc": "# Code Migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X\n## Pruning\n\nNeural network pruning (briefly known as pruning or sparsity) is one of the most promising model compression techniques. It removes the least important parameters in the network and achieves compact architectures with minimal accuracy drop and maximal inference acceleration.\n\n**Pruning with Intel Neural Compressor 1.X**\n\nIn Intel Neural Compressor 1.X, the Pruning config is still defined by an extra `conf.yaml`. The pruning code should be written as:\n\n```python\nfrom neural_compressor.experimental import Pruning, common\nprune = Pruning('conf.yaml')\nprune.model = model\nprune.train_func = pruning_func\nmodel = prune.fit()\n```\nThe `conf.yaml` is written as (https://github.com/intel/neural-compressor/blob/master/neural_compressor/template/pruning.yaml).\n\nThe pruning code requires the user to insert a series of pre-defined hooks in the training function to activate the pruning with Intel Neural Compressor. The pre-defined hooks are listed as follows,\n\n```\non_epoch_begin(epoch) : Hook executed at each epoch beginning\non_step_begin(batch) : Hook executed at each batch beginning\non_step_end() : Hook executed at each batch end\non_epoch_end() : Hook executed at each epoch end\non_before_optimizer_step() : Hook executed after gradients calculated and before backward\n```\nFollowing is an example to show how we use the hooks in the training function,\n```python\ndef pruning_func(model):\n    for epoch in range(int(args.num_train_epochs)):\n        model.train()\n        prune.on_epoch_begin(epoch)\n        for step, batch in enumerate(train_dataloader):\n            prune.on_step_begin(step)\n            batch = tuple(t.to(args.device) for t in batch)\n            inputs = {'input_ids': batch[0],\n                      'attention_mask': batch[1],\n                      'labels': batch[3]}\n            #inputs['token_type_ids'] = batch[2]\n            outputs = model(**inputs)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                prune.on_before_optimizer_step()\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n    \n            prune.on_step_end()\n...\n```\n**Pruning with Intel Neural Compressor 2.X**\n\nIn Intel Neural Compressor 2.X, the training process is activated by a `compression manager`. And the configuration information is included in the `WeightPruningConfig`. The corresponding config should be set via (Note: the corresponding names of the parameters in 1.X yaml file are attached in the comment.):\n```python\nfrom neural_compressor.config import WeightPruningConfig\n\nWeightPruningConfig(\n  ## model, device: these parameters do not need to specially be defined;\n  start_step=0,             # start_epoch: same as in the conf.yaml;\n  end_step=10,              # end_epoch: same as in the conf.yaml;\n  pruning_frequency=2,      # frequency: same as in the conf.yaml;        \n  ## dataloader, criterion, optimizer: these parameters do not need to specially be defined;\n  target_sparsity=0.97      # target_sparsity: same as in the conf.yaml;     \n  pruning_configs: [pruning_config] # Pruner: same as in the conf.yaml;           \n  ## evaluation and tuning: these parameters do not need to specially be defined;\n)       \n```\n\nWe also need to replace the hooks in the training code. The newly defined hooks are included in `compression manager` and listed as follows,\n\n```python\n    on_train_begin() : Execute at the beginning of training phase.\n    on_epoch_begin(epoch) : Execute at the beginning of each epoch.\n    on_step_begin(batch) : Execute at the beginning of each batch.\n    on_step_end() : Execute at the end of each batch.\n    on_epoch_end() : Execute at the end of each epoch.\n    on_before_optimizer_step() : Execute before optimization step.\n    on_after_optimizer_step() : Execute after optimization step.\n    on_train_end() : Execute at the ending of training phase.\n```\n\nThe final Pruning code is updated as follows,\n\n```python\n    config = { ## pruner\n                'target_sparsity': 0.9,   # Target sparsity ratio of modules.\n                'pruning_type': \"snip_momentum\", # Default pruning type.\n                'pattern': \"4x1\", # Default pruning pattern.\n                'op_names': ['layer1.*'],  # A list of modules that would be pruned.\n                'excluded_op_names': ['layer3.*'],  # A list of modules that would not be pruned.\n                'start_step': 0,  # Step at which to begin pruning.\n                'end_step': 10,   # Step at which to end pruning.\n                'pruning_scope': \"global\", # Default pruning scope.\n                'pruning_frequency': 1, # Frequency of applying pruning.\n                'min_sparsity_ratio_per_op': 0.0,  # Minimum sparsity ratio of each module.\n                'max_sparsity_ratio_per_op': 0.98, # Maximum sparsity ratio of each module.\n                'sparsity_decay_type': \"exp\", # Function applied to control pruning rate.\n                'pruning_op_types': ['Conv', 'Linear'], # Types of op that would be pruned.\n            }\n    \n    from neural_compressor.training import prepare_compression, WeightPruningConfig\n    ##setting configs\n    pruning_configs=[\n    {\"op_names\": ['layer1.*']\uff0c\"pattern\":'4x1'},\n    {\"op_names\": ['layer2.*']\uff0c\"pattern\":'1x1', 'target_sparsity':0.5}\n    ]\n    configs = WeightPruningConfig(\n        pruning_configs=pruning_configs,\n        target_sparsity=config.target_sparsity,\n        pattern=config.pattern,\n        pruning_frequency=config.pruning_frequency,\n        start_step=config.start_step,\n        end_step=config.end_step,\n        pruning_type=config.pruning_type,\n    )\n    compression_manager = prepare_compression(model=model, confs=configs)\n    compression_manager.callbacks.on_train_begin()  ## insert hook\n    for epoch in range(num_train_epochs):\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            compression_manager.callbacks.on_step_begin(step) ## insert hook\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            compression_manager.callbacks.on_before_optimizer_step()  ## insert hook\n            optimizer.step()\n            compression_manager.callbacks.on_after_optimizer_step() ## insert hook\n            lr_scheduler.step()\n            model.zero_grad()\n    ...\n    compression_manager.callbacks.on_train_end()\n```", "doc_id": 85},
{"doc": "# Code Migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X\n## Distillation\nDistillation is one of popular approaches of network compression, which transfers knowledge from a large model to a smaller one without loss of validity. As smaller models are less expensive to evaluate, they can be deployed on less powerful hardware (such as a mobile device).\n\n**Distillation with Intel Neural Compressor 1.X**\n\nIntel Neural Compressor distillation API is defined under `neural_compressor.experimental.Distillation`, which takes a user yaml file `conf.yaml` as input.\n\n```yaml\n    distillation:\n    train:                    # optional. No need if user implements `train_func` and pass to `train_func` attribute of pruning instance.\n        start_epoch: 0\n        end_epoch: 10\n        iteration: 100\n        \n        dataloader:\n          batch_size: 256\n        criterion:\n          KnowledgeDistillationLoss:\n              temperature: 1.0\n              loss_types: ['CE', 'KL']\n              loss_weights: [0.5, 0.5]\n        optimizer:\n          SGD:\n              learning_rate: 0.1\n              momentum: 0.9\n              weight_decay: 0.0004\n              nesterov: False\n    evaluation:                              # optional. required if user doesn't provide eval_func in neural_compressor.Quantization.\n    accuracy:                              # optional. required if user doesn't provide eval_func in neural_compressor.Quantization.\n        metric:\n        topk: 1                            # built-in metrics are topk, map, f1, allow user to register new metric.\n        dataloader:\n        batch_size: 256\n        dataset:\n            ImageFolder:\n            root: /path/to/imagenet/val\n        transform:\n            RandomResizedCrop:\n            size: 224\n            RandomHorizontalFlip:\n            ToTensor:\n            Normalize:\n            mean: [0.485, 0.456, 0.406]\n            std: [0.229, 0.224, 0.225] \n```\n\nWe insert a series of pre-defined hooks to activate the training process of distillation,\n\n```python\ndef train_func(model):\n    distiller.on_train_begin()\n    for nepoch in range(epochs):\n        model.train()\n        cnt = 0\n        loss_sum = 0.\n        iter_bar = tqdm(train_dataloader, desc='Iter (loss=X.XXX)')\n        for batch in iter_bar:\n            teacher_logits, input_ids, segment_ids, input_mask, target = batch\n            cnt += 1\n            output = model(input_ids, segment_ids, input_mask)\n            loss = criterion(output, target)\n            loss = distiller.on_after_compute_loss(\n                {'input_ids':input_ids, 'segment_ids':segment_ids, 'input_mask':input_mask},\n                output,\n                loss,\n                teacher_logits)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            if cnt >= iters:\n                break\n        print('Average Loss: {}'.format(loss_sum / cnt))\n        distiller.on_epoch_end()\n\nfrom neural_compressor.experimental import Distillation, common\nfrom neural_compressor.experimental.common.criterion import PyTorchKnowledgeDistillationLoss\ndistiller = Distillation(conf.yaml)\ndistiller.student_model = model\ndistiller.teacher_model = teacher\ndistiller.criterion = PyTorchKnowledgeDistillationLoss()\ndistiller.train_func = train_func\nmodel = distiller.fit()\n```\n\n**Distillation with Intel Neural Compressor 2.X**\n\nThe new distillation API also introduce `compression manager` to conduct the training process. We continuously use the hooks in `compression manager` to activate the distillation process. We replace the `conf.yaml` with `DistillationConfig` API and clean the unnecessary parameters (Note: the corresponding names of the parameters in 1.X yaml file are attached in the comment.). The dataloader is directly inputted via `train_fun`. The updated code is shown as follows,\n\n```python\nfrom neural_compressor.config import DistillationConfig\n\nDistillationConfig(\n  criterion=KnowledgeDistillationLoss,  # criterion: same as in the conf.yaml;\n  optimizer=SGD,                        # optimizer: same as in the conf.yaml;\n)       \n```\n\nThe newly updated distillation code is shown as follows,\n```python\ndef training_func(model, dataloader):\n    compression_manager.on_train_begin()\n    for epoch in range(epochs):\n        compression_manager.on_epoch_begin(epoch)\n        for i, batch in enumerate(dataloader):\n            compression_manager.on_step_begin(i)\n            ......\n            output = model(batch)\n            loss = ......\n            loss = compression_manager.on_after_compute_loss(batch, output, loss)\n            loss.backward()\n            compression_manager.on_before_optimizer_step()\n            optimizer.step()\n            compression_manager.on_step_end()\n        compression_manager.on_epoch_end()\n    compression_manager.on_train_end()\n\nfrom neural_compressor.training import prepare_compression\nfrom neural_compressor.config import DistillationConfig, SelfKnowledgeDistillationLossConfig\n\ndistil_loss = SelfKnowledgeDistillationLossConfig()\nconf = DistillationConfig(teacher_model=model, criterion=distil_loss)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\ncompression_manager = prepare_compression(model, conf)\nmodel = compression_manager.model\n\nmodel = training_func(model, dataloader)\neval_func(model)\n```", "doc_id": 86},
{"doc": "# Code Migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X\n## Mix Precision\nThe recent growth of Deep Learning has driven the development of more complex models that require significantly more compute and memory capabilities. Mixed precision training and inference using low precision formats have been developed to reduce compute and bandwidth requirements. Intel Neural Compressor supports BF16 + FP32 mixed precision conversion by MixedPrecision API\n\n**Mix Precision with Intel Neural Compressor 1.X**\n\nThe user can add dataloader and metric in `conf.yaml` to execute evaluation.\n```python\nfrom neural_compressor.experimental import MixedPrecision, common\ndataset = Dataset()\nconverter = MixedPrecision('conf.yaml')\nconverter.metric = Metric()\nconverter.precisions = 'bf16'\nconverter.eval_dataloader = common.DataLoader(dataset)\nconverter.model = './model.pb'\noutput_model = converter()\n```\n\nThe configuration `conf.yaml` should be,\n```yaml\nmodel:\n  name: resnet50_v1\n  framework: tensorflow\n  inputs: image_tensor\n  outputs: num_detections,detection_boxes,detection_scores,detection_classes\n\ndevice: cpu\n\nmixed_precision:\n  precisions: 'bf16'  \n\nevaluation:\n  ...\n\ntuning:\n  accuracy_criterion:\n    relative:  0.01                                  # optional. default value is relative, other value is absolute. this example allows relative accuracy loss: 1%.\n  objective: performance                             # optional. objective with accuracy constraint guaranteed. default value is performance. other values are modelsize and footprint.\n\n  exit_policy:\n    timeout: 0                                       # optional. tuning timeout (seconds). default value is 0 which means early stop. combine with max_trials field to decide when to exit.\n    max_trials: 100                                  # optional. max tune times. default value is 100. combine with timeout field to decide when to exit.\n    performance_only: False                          # optional. max tune times. default value is False which means only generate fully quantized model.\n  random_seed: 9527                                  # optional. random seed for deterministic tuning.\n\n```\n**Mix Precision with Intel Neural Compressor 2.X**\nIn 2.X version, we integrate the config information in `MixedPrecisionConfig`, leading to the updates in the code as follows (Note: the corresponding names of the parameters in 1.X yaml file are attached in the comment.),\n\n```python\nfrom neural_compressor.config import MixedPrecisionConfig, TuningCriterion, AccuracyCriterion\n\nMixedPrecisionConfig(\n  ## model: this parameter does not need to specially be defined;\n  backend=\"default\",        # framework: set as \"default\" when framework was tensorflow, pytorch, pytorch_fx, onnxrt_integer and onnxrt_qlinear. Set as \"ipex\" when framework was pytorch_ipex, mxnet is currently unsupported;\n  inputs=\"image_tensor\",    # input: same as in the conf.yaml;\n  outputs=\"num_detections,detection_boxes,detection_scores,detection_classes\" # output: same as in the conf.yaml;\n  device=\"cpu\",             # device: same as in the conf.yaml;\n  tuning_criterion=tuning_criterion,\n  accuracy_criterion=accuracy_criterion,\n  ## tuning.random_seed and tuning.tensorboard: these parameters do not need to specially be defined;\n)\n\naccuracy_criterion=AccuracyCriterion(\n  tolerable_loss=0.01,      # relative: same as in the conf.yaml;\n)\ntuning_criterion=TuningCriterion(\n  timeout=0,                # timeout: same as in the conf.yaml;\n  max_trials=100,           # max_trials: same as in the conf.yaml;\n)          \n```\nThe update demo is shown as follows,\n```python\nfrom neural_compressor import mix_precision\nfrom neural_compressor.config import MixedPrecisionConfig\n\nconf = MixedPrecisionConfig()\n\nconverted_model = mix_precision.fit(model, config=conf)\nconverted_model.save('./path/to/save/')\n```", "doc_id": 87},
{"doc": "# Code Migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X\n## Orchestration\nIntel Neural Compressor supports arbitrary meaningful combinations of supported optimization methods under one-shot or multi-shot, such as pruning during quantization-aware training, or pruning and then post-training quantization, pruning and then distillation and then quantization.\n\n**Orchestration with Intel Neural Compressor 1.X**\n\nIntel Neural Compressor 1.X mainly relies on a `Scheduler` class to automatically pipeline execute model optimization with one shot or multiple shots way.\n\nFollowing is an example how to set the `Scheduler` for Orchestration process. If the user wants to execute the pruning and quantization-aware training with one-shot way,\n```python\nfrom neural_compressor.experimental import Quantization, Pruning, Scheduler\nprune = Pruning(prune_conf.yaml)\nquantizer = Quantization(quantization_aware_training_conf.yaml)\nscheduler = Scheduler()\nscheduler.model = model\ncombination = scheduler.combine(prune, quantizer)\nscheduler.append(combination)\nopt_model = scheduler.fit()\n```\nThe user needs to write the `prune_conf.yaml` and the `quantization_aware_training_conf.yaml` as aforementioned to configure the experimental settings.\n\n**Orchestration with Intel Neural Compressor 2.X**\n\nIntel Neural Compressor 2.X introduces `compression manager` to schedule the training process. The `conf.yaml` should follows in the previous sessions to set the configuration information. It also requires to inset the hooks from `compression manager` to control the training flow. Therefore, the code should be updated as,\n\n```python\nfrom neural_compressor.training import prepare_compression\nfrom neural_compressor.config import DistillationConfig, KnowledgeDistillationLossConfig, WeightPruningConfig\ndistillation_criterion = KnowledgeDistillationLossConfig()\nd_conf = DistillationConfig(model, distillation_criterion)\np_conf = WeightPruningConfig()\ncompression_manager = prepare_compression(model=model, confs=[d_conf, p_conf])\n\ncompression_manager.callbacks.on_train_begin()\ntrain_loop:\n    compression_manager.on_train_begin()\n    for epoch in range(epochs):\n        compression_manager.on_epoch_begin(epoch)\n        for i, batch in enumerate(dataloader):\n            compression_manager.on_step_begin(i)\n            ......\n            output = model(batch)\n            loss = ......\n            loss = compression_manager.on_after_compute_loss(batch, output, loss)\n            loss.backward()\n            compression_manager.on_before_optimizer_step()\n            optimizer.step()\n            compression_manager.on_step_end()\n        compression_manager.on_epoch_end()\n    compression_manager.on_train_end()\n    \nmodel.save('./path/to/save')\n```\n\n# Code Migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X\n## Benchmark\n\nThe benchmarking feature of Neural Compressor is used to measure the model performance with the objective settings. The user can get the performance of the models between the float32 model and the quantized low precision model in a same scenario.\n\n**Benchmark with Intel Neural Compressor 1.X**\n\nIn Intel Neural Compressor 1.X requires the user to define the experimental settings for low precision model and FP32 model with a `conf.yaml`.\n```yaml\nversion: 1.0\n\nmodel:                                               # mandatory. used to specify model specific information.\n  name: ssd_mobilenet_v1                             # mandatory. the model name.\n  framework: tensorflow                              # mandatory. supported values are tensorflow, pytorch, pytorch_fx, pytorch_ipex, onnxrt_integer, onnxrt_qlinear or mxnet; allow new framework backend extension.\n  inputs: image_tensor                               # optional. inputs and outputs fields are only required in tensorflow.\n  outputs: num_detections,detection_boxes,detection_scores,detection_classes\n\ndevice: cpu                                          # optional. default value is cpu. other value is gpu.\n\nevaluation:                                          # optional. used to config evaluation process.\n  performance:                                       # optional. used to benchmark performance of passing model.\n    warmup: 10\n    iteration: 100\n    configs:\n      cores_per_instance: 4\n      num_of_instance: 7\n      inter_num_of_threads: 1\n      intra_num_of_threads: 4\n    dataloader:\n      dataset:\n        dummy:\n          shape: [[128, 3, 224, 224], [128, 1, 1, 1]]\n```\n\nAnd then, the user can get the accuracy with,\n```python\ndataset = Dataset() #  dataset class that implement __getitem__ method or __iter__ method\nfrom neural_compressor.experimental import Benchmark, common\nfrom neural_compressor.conf.config import BenchmarkConf\nconf = BenchmarkConf(config.yaml)\nevaluator = Benchmark(conf)\nevaluator.dataloader = common.DataLoader(dataset, batch_size=batch_size)\n# user can also register postprocess and metric, this is optional\nevaluator.postprocess = common.Postprocess(postprocess_cls)\nevaluator.metric = common.Metric(metric_cls)\nresults = evaluator()\n```\n\n**Benchmark with Intel Neural Compressor 2.X**\n\nIn Intel Neural Compressor 2.X, we optimize the code to make it simple and clear for the user. We replace `conf.yaml` with `BenchmarkConfig`. The corresponding information should be defined as (Note: the corresponding names of the parameters in 1.X yaml file are attached in the comment.):\n```python\nfrom neural_compressor.config import BenchmarkConfig\n\nBenchmarkConfig(\n  ## model: this parameter does not need to specially be defined;\n  backend=\"default\",        # framework: set as \"default\" when framework was tensorflow, pytorch, pytorch_fx, onnxrt_integer and onnxrt_qlinear. Set as \"ipex\" when framework was pytorch_ipex, mxnet is currently unsupported;\n  inputs=\"image_tensor\",    # input: same as in the conf.yaml;\n  outputs=\"num_detections,detection_boxes,detection_scores,detection_classes\" # output: same as in the conf.yaml;\n  device=\"cpu\",             # device: same as in the conf.yaml;\n  warmup=10,                # warmup: same as in the conf.yaml;\n  iteration=100,            # iteration: same as in the conf.yaml;\n  cores_per_instance=4,     # cores_per_instance: same as in the conf.yaml;\n  num_of_instance=7,        # num_of_instance: same as in the conf.yaml;\n  inter_num_of_threads=1,   # inter_num_of_threads: same as in the conf.yaml;\n  intra_num_of_threads=4,   # intra_num_of_threads: same as in the conf.yaml;\n  ## dataloader: this parameter does not need to specially be defined;\n)     \n```\n\nThe new example in Intel Neural Compressor 2.X should be updated as,\n\n```python\nfrom neural_compressor.config import BenchmarkConfig\nfrom neural_compressor.benchmark import fit\nconf = BenchmarkConfig(warmup=10, iteration=100, cores_per_instance=4, num_of_instance=7)\nfit(model='./int8.pb', config=conf, b_dataloader=eval_dataloader)\n```", "doc_id": 88},
{"doc": "# Code Migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X\n## Examples\n\nUser could refer to [examples](https://github.com/intel/neural-compressor/blob/master/examples/README.md) for more details about the migration from Intel Neural Compressor 1.X to Intel Neural Compressor 2.X.\n\n# Intel Neural Compressor Mixed Precision\n## Introduction\n\nThe recent growth of Deep Learning has driven the development of more complex models that require significantly more compute and memory capabilities. Several low precision numeric formats have been proposed to address the problem. Google's [bfloat16](https://cloud.google.com/tpu/docs/bfloat16) and the [FP16: IEEE](https://en.wikipedia.org/wiki/Half-precision_floating-point_format) half-precision format are two of the most widely used sixteen bit formats. [Mixed precision](https://arxiv.org/abs/1710.03740) training and inference using low precision formats have been developed to reduce compute and bandwidth requirements.\n\nThe recently launched 3rd Gen Intel\u00ae Xeon\u00ae Scalable processor (codenamed Cooper Lake), featuring Intel\u00ae Deep Learning Boost, is the first general-purpose x86 CPU to support the bfloat16 format. Specifically, three new bfloat16 instructions are added as a part of the AVX512_BF16 extension within Intel Deep Learning Boost: VCVTNE2PS2BF16, VCVTNEPS2BF16, and VDPBF16PS. The first two instructions allow converting to and from bfloat16 data type, while the last one performs a dot product of bfloat16 pairs. Further details can be found in the [hardware numerics document](https://software.intel.com/content/www/us/en/develop/download/bfloat16-hardware-numerics-definition.html) published by Intel.", "doc_id": 89},
{"doc": "# Intel Neural Compressor Mixed Precision\n## Mixed Precision Support Matrix\n\n|Framework     |BF16         |FP16         |\n|--------------|:-----------:|:-----------:|\n|TensorFlow    |&#10004;     |:x:     |\n|PyTorch       |&#10004;     |:x:     |\n|ONNX Runtime  |&#10004;     |&#10004;     |\n|MXNet         |&#10004;     |:x:     |\n\n> **During quantization, BF16 conversion is default enabled, FP16 can be executed if 'device' of config is 'gpu'. Please refer to this [document](./quantization_mixed_precision.md) for its workflow.**\n\n# Intel Neural Compressor Mixed Precision\n## Get Started with Mixed Precision API\nTo get a bf16/fp16 model, users can use the Mixed Precision API as follows.\n\nSupported precisions for mix precision include bf16 and fp16. If users want to get a pure fp16 or bf16 model, they should add another precision into excluded_precisions.\n\n- BF16:\n\n```python\nfrom neural_compressor import mix_precision\nfrom neural_compressor.config import MixedPrecisionConfig\n\nconf = MixedPrecisionConfig(precision='bf16')\nconverted_model = mix_precision.fit(model, config=conf)\nconverted_model.save('./path/to/save/')\n```\n- FP16:\n```python\nfrom neural_compressor import mix_precision\nfrom neural_compressor.config import MixedPrecisionConfig\n\nconf = MixedPrecisionConfig(\n        backend='onnxrt_cuda_ep',\n        device='gpu',\n        precision='fp16')\nconverted_model = mix_precision.fit(model, config=conf)\nconverted_model.save('./path/to/save/')\n```\n> **BF16/FP16 conversion may lead to accuracy drop. Intel\u00ae Neural Compressor provides an accuracy-aware tuning function to reduce accuracy loss, which will fallback converted ops to FP32 automatically to get better accuracy. To enable this function, users only need to provide an evaluation function (or dataloader + metric).**", "doc_id": 90},
{"doc": "# Intel Neural Compressor Mixed Precision\n## Examples\n\nThere are some pre-requirements to run mixed precision examples for each framework. If the hardware requirements cannot be met, the program would exit consequently.\n\n- BF16:\n    ### TensorFlow\n    1. Hardware: CPU supports `avx512_bf16` instruction set.\n    2. Software: intel-tensorflow >= [2.3.0](https://pypi.org/project/intel-tensorflow/2.3.0/).\n    ### PyTorch\n    1. Hardware: CPU supports `avx512_bf16` instruction set.\n    2. Software: torch >= [1.11.0](https://download.pytorch.org/whl/torch_stable.html).\n    ### ONNX Runtime\n    1. Hardware: GPU, set 'device' of config to 'gpu' and 'backend' to 'onnxrt_cuda_ep'.\n    2. Software: onnxruntime-gpu.\n\n- FP16:\n    ### ONNX Runtime\n    1. Hardware: GPU, set 'device' of config to 'gpu' and 'backend' to 'onnxrt_cuda_ep'.\n    2. Software: onnxruntime-gpu.", "doc_id": 91},
{"doc": "# Intel Neural Compressor Model\n## Introduction\nThe Neural Compressor Model feature is used to encapsulate the behavior of model building and saving. By simply providing information such as different model formats and framework_specific_info, Neural Compressor performs optimizations and quantization on this model object and returns a Neural Compressor Model object for further model persistence or benchmarking. A Neural Compressor Model helps users to maintain necessary model information which is required during optimization and quantization such as the input/output names, workspace path, and other model format knowledge. This helps unify the features gap brought by different model formats and frameworks.\n\n# Intel Neural Compressor Model\n## Examples\nUsers can create, use, and save models in the following manners:\n```python\nfrom neural_compressor.model import Model\ninc_model = Model(input_model)\n```\nor\n```python\nfrom neural_compressor import quantization\nfrom neural_compressor.config import PostTrainingQuantConfig\n\nconf = PostTrainingQuantConfig()\nq_model = quantization.fit(model = inc_model, conf=conf)\nq_model.save(\"saved_result\")\n```", "doc_id": 92},
{"doc": "# Intel Neural Compressor Objective\n## Introduction\n\nIn terms of evaluating the status of a specific model during tuning, we should have general objectives. Intel\u00ae Neural Compressor Objective supports code-free configuration through a yaml file. With built-in objectives, users can compress models with different objectives easily. In special cases, users can also register their own objective classes.", "doc_id": 93},
{"doc": "# Intel Neural Compressor Objective\n### Single Objective\n\nThe objective supported by Intel\u00ae Neural Compressor is driven by accuracy. If users want to evaluate a specific model with other objectives, they can realize it with `objective` in a yaml file. Default value for `objective` is `performance`, and the other values are `modelsize` and `footprint`.", "doc_id": 94},
{"doc": "# Intel Neural Compressor Objective\n### Multiple Objectives\nIn some cases, users want to use more than one objective to evaluate the status of a specific model and they can realize it with `multi_objectives` in a yaml file. Currently `multi_objectives` supports built-in objectives.\nIf users use `multi_objectives` to evaluate the status of the model during tuning, Neural Compressor will return a model with the best score of `multi_objectives` and meeting `accuracy_criterion` after tuning ending.\nIntel\u00ae Neural Compressor will normalize the results of each objective to calculate the final weighted multi_objective score.", "doc_id": 95},
{"doc": "# Intel Neural Compressor Objective\n## Objective Support Matrix\n\nBuilt-in objectives support list:\n\n| Objective    | Usage                                                    |\n| :------      | :------                                                  |\n| accuracy     | Evaluate the accuracy                                    |\n| performance  | Evaluate the inference time                              |\n| footprint    | Evaluate the peak size of memory blocks during inference |\n| modelsize    | Evaluate the model size                                  |", "doc_id": 96},
{"doc": "# Intel Neural Compressor Objective\n## Get Started with Objective API\n### Config Single Objective\nUsers can specify a built-in objective in `neural_compressor.config.TuningCriterion` as shown below:\n\n```python\nfrom neural_compressor.config import TuningCriterion\ntuning_criterion = TuningCriterion(objective='accuracy')\n\n```", "doc_id": 97},
{"doc": "# Intel Neural Compressor Objective\n## Get Started with Objective API\n### Config Multiple Objectives\nUsers can specify built-in multiple objectives in `neural_compressor.config.TuningCriterion` as shown below:\n```python\nfrom neural_compressor.config import TuningCriterion\ntuning_criterion = TuningCriterion(objective=['performance', 'accuracy'])\n```", "doc_id": 98},
{"doc": "# Intel Neural Compressor Objective\n## Example\nRefer to [example](../neural_compressor/template/ptq.yaml) as an example.", "doc_id": 99},
{"doc": "# Intel Neural Compressor Optimization Orchestration\n## Introduction\n\nOrchestration is the combination of multiple optimization techniques, either applied simultaneously (one-shot). Intel Neural Compressor supports arbitrary meaningful combinations of supported optimization methods under one-shot, such as pruning during quantization-aware training.", "doc_id": 100},
{"doc": "# Intel Neural Compressor Optimization Orchestration\n### One-shot\nSince quantization-aware training, pruning and distillation all leverage training process for optimization, we can achieve the goal of optimization through one shot training with arbitrary meaningful combinations of these methods, which often gain more benefits in terms of performance and accuracy than just one compression technique applied, and usually are as efficient as applying just one compression technique. The three possible combinations are shown below.\n- Pruning during quantization-aware training\n- Distillation with pattern lock pruning\n- Distillation with pattern lock pruning and quantization-aware training", "doc_id": 101},
{"doc": "# Intel Neural Compressor Optimization Orchestration\n## Orchestration Support Matrix\n<table>\n    <thead>\n        <tr>\n            <th>Orchestration</th>\n            <th>Combinations</th>\n            <th>Supported</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td rowspan=4>One-shot</td>\n            <td>Pruning + Quantization Aware Training</td>\n            <td>&#10004;</td>\n        </tr>\n        <tr>\n            <td>Distillation + Quantization Aware Training</td>\n            <td>&#10004;</td>\n        </tr>\n        <tr>\n            <td>Distillation + Pruning</td>\n            <td>&#10004;</td>\n        </tr>\n        <tr>\n            <td>Distillation + Pruning + Quantization Aware Training</td>\n            <td>&#10004;</td>\n        </tr>\n    </tbody>\n</table>", "doc_id": 102},
{"doc": "# Intel Neural Compressor Optimization Orchestration\n## Get Started with Orchestration API \nNeural Compressor defines `Scheduler` class to automatically pipeline execute model optimization with one shot way. \nUser instantiates model optimization components, such as quantization, pruning, distillation, separately. After that, user could append\nthose separate optimization objects into scheduler's pipeline, the scheduler API executes them one by one.\nIn following example it execute the distillation and pruning with one-shot way, the code is like below.\n```python\nfrom neural_compressor.training import prepare_compression\nfrom neural_compressor.config import DistillationConfig, KnowledgeDistillationLossConfig, WeightPruningConfig\ndistillation_criterion = KnowledgeDistillationLossConfig()\nd_conf = DistillationConfig(model, distillation_criterion)\np_conf = WeightPruningConfig()\ncompression_manager = prepare_compression(model=model, confs=[d_conf, p_conf])\n\ncompression_manager.callbacks.on_train_begin()\ntrain_loop:\n    compression_manager.on_train_begin()\n    for epoch in range(epochs):\n        compression_manager.on_epoch_begin(epoch)\n        for i, batch in enumerate(dataloader):\n            compression_manager.on_step_begin(i)\n            ......\n            output = model(batch)\n            loss = ......\n            loss = compression_manager.on_after_compute_loss(batch, output, loss)\n            loss.backward()\n            compression_manager.on_before_optimizer_step()\n            optimizer.step()\n            compression_manager.on_step_end()\n        compression_manager.on_epoch_end()\n    compression_manager.on_train_end()\n    \nmodel.save('./path/to/save')\n```\n\n# Intel Neural Compressor Optimization Orchestration\n## Examples\n[Orchestration Examples](../../examples/README.md#orchestration)", "doc_id": 103},
{"doc": "# Intel Neural Compressor Pruning\n## Introduction\n### Neural Network Pruning\nNeural network pruning (briefly known as pruning or sparsity) is one of the most promising model compression techniques. It removes the least important parameters in the network and achieves compact architectures with minimal accuracy drop and maximal inference acceleration. As current state-of-the-art models have increasingly more parameters, pruning plays a crucial role in enabling them to run on devices whose memory footprints and computing resources are limited.", "doc_id": 104},
{"doc": "# Intel Neural Compressor Pruning\n### Pruning Patterns\nPruning patterns defines the rules of pruned weights' arrangements in space.\n- Unstructured Pruning\n  Unstructured pruning means finding and removing the less salient connection in the model where the nonzero patterns are irregular and could be anywhere in the matrix.\n- 2in4 Pruning\n  NVIDIA proposed [2:4 sparsity](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/) (or known as \"2in4 sparsity\") in Ampere architecture, for every 4 continuous elements in a matrix, two of them are zero and others are non-zero.\n- Structured Pruning\n  Structured pruning means finding parameters in groups, deleting entire blocks, filters, or channels according to some pruning criterions. In general, structured pruning leads to lower accuracy due to restrictive structure than unstructured pruning; However, it can accelerate the model execution significantly because it can fit hardware design better.\n  Different from 2:4 sparsity above, we propose the block-wise structured sparsity patterns that we are able to demonstrate the performance benefits on existing Intel hardwares even without the support of hardware sparsity. A block-wise sparsity pattern with block size ```S``` means the contiguous ```S``` elements in this block are all zero values.\n  For a typical GEMM, the weight dimension is ```IC``` x ```OC```, where ```IC``` is the number of input channels and ```OC``` is the number of output channels. Note that sometimes ```IC``` is also called dimension ```K```, and ```OC``` is called dimension ```N```. The sparsity dimension is on ```OC``` (or ```N```).\n  For a typical Convolution, the weight dimension is ```OC x IC x KH x KW```, where ```OC``` is the number of output channels, ```IC``` is the number of input channels, and ```KH``` and ```KW``` is the kernel height and weight. The sparsity dimension is also on ```OC```.\n  Here is a figure showing a matrix with ```IC``` = 32 and ```OC``` = 16 dimension, and a block-wise sparsity pattern with block size 4 on ```OC``` dimension.\n- Channel-wise Pruning\n  Channel-wise pruning means removing less salient channels on feature maps and it could directly shrink feature map widths. Users could set a channelx1 (or 1xchannel) pruning pattern to use this method.\n  An advantage of channel pruning is that in some particular structure(feed forward parts in Transformers etc.), pruned channels can be removed permanently from original weights without influencing other dense channels. Via this process, we can decrease these weights' size and obtain direct improvements of inference speed, without using hardware related optimization tools like [Intel Extension for Transformers](https://github.com/intel/intel-extension-for-transformers). \n  We name this process as <span id=\"click\">**Model Auto Slim**</span> and currently we have validated that this process can significantly improve some popular transformer model's inference speed. Please refer more details of such method in this [model slim example](../../examples/pytorch/nlp/huggingface_models/question-answering/model_slim/eager/).", "doc_id": 105},
{"doc": "# Intel Neural Compressor Pruning\n### Pruning Criteria\nPruning criteria defines the rules of which weights are least important to be pruned, in order to maintain the model's original accuracy. Most popular criteria examine weights' absolute value and their corresponding gradients. \n- Magnitude\n  The algorithm prunes the weight by the lowest absolute value at each layer with given sparsity target.\n- Gradient sensitivity\n  The algorithm prunes the head, intermediate layers, and hidden states in NLP model according to importance score calculated by following the paper [FastFormers](https://arxiv.org/abs/2010.13382). \n- Group Lasso\n  The algorithm uses Group lasso regularization to prune entire rows, columns or blocks of parameters that result in a smaller dense network.\n- SNIP\n  The algorithm prunes the dense model at its initialization, by analyzing the weights' effect to the loss function when they are masked. Please refer to the original [paper](https://arxiv.org/abs/1810.02340) for details\n- SNIP with momentum\n  The algorithm improves original SNIP algorithms and introduces weights' score maps which updates in a momentum way.\\\n  In the following formula, $n$ is the pruning step and $W$ and $G$ are model's weights and gradients respectively.\n  $$Score_{n} = 1.0 \\times Score_{n-1} + 0.9 \\times |W_{n} \\times G_{n}|$$", "doc_id": 106},
{"doc": "# Intel Neural Compressor Pruning\n### Pruning Schedule\nPruning schedule defines the way the model reach the target sparsity (the ratio of pruned weights).\n- Iterative Pruning\n  Iterative pruning means the model is gradually pruned to its target sparsity during a training process. The pruning process contains several pruning steps, and each step raises model's sparsity to a higher value. In the final pruning step, the model reaches target sparsity and the pruning process ends. \n- One-shot Pruning\n  One-shot pruning means the model is pruned to its target sparsity with one single step. This pruning method often works at model's initialization step. It can easily cause accuracy drop, but save much training time.", "doc_id": 107},
{"doc": "# Intel Neural Compressor Pruning\n### Pruning Types\nPruning type defines how the masks are generated and applied to a neural network. In Intel Neural Compressor, both pruning criterion and pruning type are defined in pruning_type. Currently supported pruning types include **snip_momentum(default)**, **snip_momentum_progressive**, **magnitude**, **magnitude_progressive**, **gradient**, **gradient_progressive**, **snip**, **snip_progressive** and **pattern_lock**. progressive pruning is preferred when large patterns like 1xchannel and channelx1 are selected.\n- Progressive Pruning\n  Progressive pruning aims at smoothing the structured pruning by automatically interpolating a group of intervals masks during the pruning process. In this method, a sequence of masks is generated to enable a more flexible pruning process and those masks would gradually change into ones to fit the target pruning structure.\n  Progressive pruning is used mainly for channel-wise pruning and currently only supports NxM pruning pattern.\n  (a) refers to the traditional structured iterative pruning;  <Br/>\n  (b) inserts unstructured masks which prune some weights by referring to pre-defined score maps.\n  (b) is adopted in progressive pruning implementation. after a new structure pruning step, newly generated masks with full-zero blocks are not used to prune the model immediately. Instead, only a part of weights in these blocks is selected to be pruned by referring to these weights\u2019 score maps. these partial-zero unstructured masks are added to the previous structured masks and  pruning continues. After training the model with these interpolating masks and masking more elements in these blocks, the mask interpolation process is returned. After several steps of mask interpolation, All weights in the blocks are finally masked and the model is trained as pure block-wise sparsity.\n- Pattern_lock Pruning\n  Pattern_lock pruning type uses masks of a fixed pattern during the pruning process. It locks the sparsity pattern in fine-tuning phase by freezing those zero values of weight tensor during weight update of training. It can be applied for the following scenario: after the model is pruned under a large dataset, pattern lock can be used to retrain the sparse model on a downstream task (a smaller dataset). Please refer to [Prune once for all](https://arxiv.org/pdf/2111.05754.pdf) for more information.\n\n# Intel Neural Compressor Pruning\n### Pruning Scope\nRange of sparse score calculation in iterative pruning, default scope is global.\n- Global\n  The score map is computed out of entire parameters, Some layers are higher than the target sparsity and some of them are lower, the total sparsity of the model reaches the target. You can also set the \"min sparsity ratio\"/\"max sparsity ratio\" to be the same as the target to achieve same sparsity for each layer in a global way.\n- Local\n  The score map is computed from the corresponding layer's weight, The sparsity of each layer is equal to the target.", "doc_id": 108},
{"doc": "# Intel Neural Compressor Pruning\n### Regularization\nRegularization is a technique that discourages learning a more complex model and therefore performs variable-selection. In the image below, some weights are pushed to be as small as possible and the connections are thus pruned. **Group-lasso** method is used in Intel Neural Compressor.\n- Group Lasso\n  The main ideas of Group Lasso are to construct an objective function that penalizes the L2 parameterization of the grouped variables, determines the coefficients of some groups of variables to be zero, and obtains a refined model by feature filtering.", "doc_id": 109},
{"doc": "# Intel Neural Compressor Pruning\n## Get Started with Pruning API\nNeural Compressor `Pruning` API is defined under `neural_compressor.training`, which takes a user defined yaml file as input. Below is the launcher code of applying the API to execute a pruning process.\nUsers can pass the customized training/evaluation functions to `Pruning` for flexible scenarios. In this case, pruning process can be done by pre-defined hooks in Neural Compressor. Users need to put those hooks inside the training function.\nThe following section exemplifies how to use hooks in user pass-in training function to perform model pruning. Through the pruning API, multiple pruner objects are supported in one single Pruning object to enable layer-specific configurations and a default setting is used as a complement.\n- Step 1: Define a dict-like configuration in your training codes. We provide you a template of configuration below.\n  ```python\n      configs = [\n              { ## pruner1\n                  'target_sparsity': 0.9,   # Target sparsity ratio of modules.\n                  'pruning_type': \"snip_momentum\", # Default pruning type.\n                  'pattern': \"4x1\", # Default pruning pattern.\n                  'op_names': ['layer1.*'],  # A list of modules that would be pruned.\n                  'excluded_op_names': ['layer3.*'],  # A list of modules that would not be pruned.\n                  'start_step': 0,  # Step at which to begin pruning.\n                  'end_step': 10,   # Step at which to end pruning.\n                  'pruning_scope': \"global\", # Default pruning scope.\n                  'pruning_frequency': 1, # Frequency of applying pruning.\n                  'min_sparsity_ratio_per_op': 0.0,  # Minimum sparsity ratio of each module.\n                  'max_sparsity_ratio_per_op': 0.98, # Maximum sparsity ratio of each module.\n                  'sparsity_decay_type': \"exp\", # Function applied to control pruning rate.\n                  'pruning_op_types': ['Conv', 'Linear'], # Types of op that would be pruned.\n              },\n              { ## pruner2\n                  \"op_names\": ['layer3.*'], # A list of modules that would be pruned.\n                  \"pruning_type\": \"snip_momentum_progressive\",   # Pruning type for the listed ops.\n                  # 'target_sparsity'\n              } # For layer3, the missing target_sparsity would be complemented by default setting (i.e. 0.8)\n          ]\n  ```\n- Step 2: Insert API functions in your codes. Only 5 lines of codes are required.\n  ```python\n      \"\"\" All you need is to insert following API functions to your codes:\n      on_train_begin() # Setup pruner\n      on_step_begin() # Prune weights\n      on_before_optimizer_step() # Do weight regularization\n      on_after_optimizer_step() # Update weights' criteria, mask weights\n      on_train_end() # end of pruner, Print sparse information\n      \"\"\"\n      from neural_compressor.training import prepare_compression, WeightPruningConfig\n      ##setting configs\n      pruning_configs=[\n      {\"op_names\": ['layer1.*']\uff0c\"pattern\":'4x1'},\n      {\"op_names\": ['layer2.*']\uff0c\"pattern\":'1x1', 'target_sparsity':0.5}\n      ]\n      config = WeightPruningConfig(pruning_configs,\n                                   target_sparsity=0.8,\n                                   excluded_op_names=['classifier'])  ##default setting\n      config = WeightPruningConfig(configs)\n      compression_manager = prepare_compression(model, config)\n      compression_manager.callbacks.on_train_begin()  ## insert hook\n      for epoch in range(num_train_epochs):\n          model.train()\n          for step, batch in enumerate(train_dataloader):\n              compression_manager.callbacks.on_step_begin(step) ## insert hook\n              outputs = model(**batch)\n              loss = outputs.loss\n              loss.backward()\n              compression_manager.callbacks.on_before_optimizer_step()  ## insert hook\n              optimizer.step()\n              compression_manager.callbacks.on_after_optimizer_step() ## insert hook\n              lr_scheduler.step()\n              model.zero_grad()\n      ...\n      compression_manager.callbacks.on_train_end()\n      ...\n  ```\n In the case mentioned above, pruning process can be done by pre-defined hooks in Neural Compressor. Users need to place those hooks inside the training function. The pre-defined Neural Compressor hooks are listed below.\n```python\n    on_train_begin() : Execute at the beginning of training phase.\n    on_epoch_begin(epoch) : Execute at the beginning of each epoch.\n    on_step_begin(batch) : Execute at the beginning of each batch.\n    on_step_end() : Execute at the end of each batch.\n    on_epoch_end() : Execute at the end of each epoch.\n    on_before_optimizer_step() : Execute before optimization step.\n    on_after_optimizer_step() : Execute after optimization step.\n    on_train_end() : Execute at the ending of training phase.\n```", "doc_id": 110},
{"doc": "# Intel Neural Compressor Pruning\n## Examples\nWe validate the sparsity on typical models across different domains (including CV, NLP, and Recommendation System). [Validated pruning examples](./validated_model_list.md#validated-pruning-examples) shows the sparsity pattern, sparsity ratio, and accuracy of sparse and dense (Reference) model for each model. \n- Text Classification\n  Sparsity is implemented in different pruning patterns of MRPC and SST-2 tasks [Text-classification examples](../../examples/pytorch/nlp/huggingface_models/text-classification/pruning/eager).\n- Question Answering\n  Multiple examples of sparse models were obtained on the SQuAD-v1.1 dataset [Question-answering examples](../../examples/pytorch/nlp/huggingface_models/question-answering/pruning/eager).\n- Language Translation (Experimental)\n  Pruning Flan-T5-small model on English-Romanian translation task [Translation examples](../../examples/pytorch/nlp/huggingface_models/translation/pruning/eager).\n- Object Detection (Experimental)\n  Pruning on YOLOv5 model using coco dataset [Object-detection examples](../../examples/pytorch/object_detection/yolo_v5/pruning/eager).\n- Image Recognition (Experimental)\n  Pruning on ResNet50 model using ImageNet dataset [Image-recognition examples](../../examples/pytorch/image_recognition/ResNet50/pruning/eager/).\nPlease refer to [pruning examples](../../examples/README.md#Pruning-1) for more information.", "doc_id": 111},
{"doc": "# Intel Neural Compressor Pruning\n## Sparse Model Deployment\nParticular hardware/software like [Intel Extension for Transformer](https://github.com/intel/intel-extension-for-transformers) are required to obtain inference speed and footprints' optimization for most sparse models. However, using [model slim](#click) for some special structures can obtain significant inference speed improvements and footprint reduction without the post-pruning deployment. In other words, you can achieve model acceleration directly under your training framework (PyTorch, etc.)", "doc_id": 112},
{"doc": "# Intel Neural Compressor Pruning\n## Reference\n[1] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr. SNIP: Single-shot network pruning based on connection sensitivity. In International Conference on Learning Representations, 2019.\n[2] Zafrir, Ofir, Ariel Larey, Guy Boudoukh, Haihao Shen, and Moshe Wasserblat. \"Prune once for all: Sparse pre-trained language models.\" arXiv preprint arXiv:2111.05754 (2021).", "doc_id": 113},
{"doc": "# Intel Neural Compressor Pythonic Style Access for Configurations\n## Introduction\nTo meet the variety of needs arising from various circumstances, INC now provides a\npythonic style access - Pythonic API - for same purpose of either user or framework configurations.\n\nThe Pythonic API for Configuration allows users to specify configurations\ndirectly in their python codes without referring to \na separate YAML file. While we support both simultaneously, \nthe Pythonic API for Configurations has several advantages over YAML files, \nwhich one can tell from usages in the context below. Hence, we recommend \nusers to use the Pythonic API for Configurations moving forward.", "doc_id": 114},
{"doc": "# Intel Neural Compressor Pythonic Style Access for Configurations\n## Supported Feature Matrix\n\n### Pythonic API for User Configurations\n| Optimization Techniques | Pythonic API |\n|-------------------------|:------------:|\n| Quantization            |   &#10004;   |\n| Pruning                 |   &#10004;   |\n| Distillation            |   &#10004;   |\n| NAS                     |   &#10004;   |\n### Pythonic API for Framework Configurations\n\n| Framework  | Pythonic API |\n|------------|:------------:|\n| TensorFlow |   &#10004;   |\n| PyTorch    |   &#10004;   |\n| ONNX       |   &#10004;   |\n| MXNet      |   &#10004;   |", "doc_id": 115},
{"doc": "# Intel Neural Compressor Pythonic Style Access for Configurations\n## Get Started with Pythonic API for Configurations\n\n### Pythonic API for User Configurations\nNow, let's go through the Pythonic API for Configurations in the order of\nsections similar as in user YAML files.", "doc_id": 116},
{"doc": "# Intel Neural Compressor Pythonic Style Access for Configurations\n## Get Started with Pythonic API for Configurations\n### Pythonic API for User Configurations\n#### Quantization\nTo specify quantization configurations, users can use the following \nPythonic API step by step. \n\n* First, load the ***config*** module\n```python\nfrom neural_compressor import config\n```\n* Next, assign values to the attributes of *config.quantization* to use specific configurations, and pass the config to *Quantization* API.\n```python\nconfig.quantization.inputs = ['image'] # list of str\nconfig.quantization.outputs = ['out'] # list of str\nconfig.quantization.backend = 'onnxrt_integerops' # support tensorflow, tensorflow_itex, pytorch, pytorch_ipex, pytorch_fx, onnxrt_qlinearops, onnxrt_integerops, onnxrt_qdq, onnxrt_qoperator, mxnet\nconfig.quantization.approach = 'post_training_dynamic_quant' # support post_training_static_quant, post_training_dynamic_quant, quant_aware_training\nconfig.quantization.device = 'cpu' # support cpu, gpu\nconfig.quantization.op_type_dict = {'Conv': {'weight': {'dtype': ['fp32']}, 'activation': {'dtype': ['fp32']}}} # dict \nconfig.quantization.strategy = 'mse' # support basic, mse, bayesian, random, exhaustive\nconfig.quantization.objective = 'accuracy' # support performance, accuracy, modelsize, footprint\nconfig.quantization.timeout = 100 # int, default is 0\nconfig.quantization.accuracy_criterion.relative = 0.5 # float, default is 0.01\nconfig.quantization.reduce_range = False # bool. default value depends on hardware, True if cpu supports VNNI instruction, otherwise is False\nconfig.quantization.use_bf16 = False # bool\nfrom neural_compressor.experimental import Quantization\nquantizer = Quantization(config)\n```", "doc_id": 117},
{"doc": "# Intel Neural Compressor Pythonic Style Access for Configurations\n## Get Started with Pythonic API for Configurations\n### Pythonic API for User Configurations\n#### Distillation\nTo specify distillation configurations, users can assign values to \nthe corresponding attributes.\n```python\nfrom neural_compressor import config\nconfig.distillation.optimizer = {'SGD': {'learning_rate': 0.0001}}\n\nfrom neural_compressor.experimental import Distillation\ndistiller = Distillation(config)\n```", "doc_id": 118},
{"doc": "# Intel Neural Compressor Pythonic Style Access for Configurations\n## Get Started with Pythonic API for Configurations\n### Pythonic API for User Configurations\n#### Pruning\nTo specify pruning configurations, users can assign values to the corresponding attributes. \n```python\nfrom neural_compressor import config\nconfig.pruning.weight_compression.initial_sparsity = 0.0\nconfig.pruning.weight_compression.target_sparsity = 0.9\nconfig.pruning.weight_compression.max_sparsity_ratio_per_layer = 0.98\nconfig.pruning.weight_compression.prune_type = \"basic_magnitude\"\nconfig.pruning.weight_compression.start_epoch = 0\nconfig.pruning.weight_compression.end_epoch = 3\nconfig.pruning.weight_compression.start_step = 0\nconfig.pruning.weight_compression.end_step = 0\nconfig.pruning.weight_compression.update_frequency = 1.0\nconfig.pruning.weight_compression.update_frequency_on_step = 1\nconfig.pruning.weight_compression.prune_domain = \"global\"\nconfig.pruning.weight_compression.pattern = \"tile_pattern_1x1\"\n\nfrom neural_compressor.experimental import Pruning\nprune = Pruning(config)\n```", "doc_id": 119},
{"doc": "# Intel Neural Compressor Pythonic Style Access for Configurations\n## Get Started with Pythonic API for Configurations\n### Pythonic API for User Configurations\n#### NAS\nTo specify nas configurations, users can assign values to the\ncorresponding attributes.\n\n```python\nfrom neural_compressor import config\nconfig.nas.approach = 'dynas'\nfrom neural_compressor.experimental import NAS\nnas = NAS(config)\n```", "doc_id": 120},
{"doc": "# Intel Neural Compressor Pythonic Style Access for Configurations\n## Get Started with Pythonic API for Configurations\n### Pythonic API for User Configurations\n#### Benchmark\nTo specify benchmark configurations, users can assign values to the\ncorresponding attributes.\n```python\nfrom neural_compressor import config\nconfig.benchmark.warmup = 10\nconfig.benchmark.iteration = 10\nconfig.benchmark.cores_per_instance = 10\nconfig.benchmark.num_of_instance = 10\nconfig.benchmark.inter_num_of_threads = 10\nconfig.benchmark.intra_num_of_threads = 10\n\nfrom neural_compressor.experimental import Benchmark\nbenchmark = Benchmark(config)\n```", "doc_id": 121},
{"doc": "# Intel Neural Compressor Pythonic Style Access for Configurations\n## Get Started with Pythonic API for Configurations\n### Pythonic API for Framework Configurations\nNow, let's go through the Pythonic API for Configurations in setting up similar framework\ncapabilities as in YAML files. Users can specify a framework's (eg. ONNX Runtime) capability by\nassigning values to corresponding attributes. \n\n```python\nconfig.onnxruntime.precisions = ['int8', 'uint8']\nconfig.onnxruntime.graph_optimization_level = 'DISABLE_ALL' # only onnxruntime has graph_optimization_level attribute\n```", "doc_id": 122},
{"doc": "# Intel Neural Compressor Quantization\n## Quantization Introduction\n\nQuantization is a very popular deep learning model optimization technique invented for improving the speed of inference. It minimizes the number of bits required by converting a set of real-valued numbers into the lower bit data representation, such as int8 and int4, mainly on inference phase with minimal to no loss in accuracy. This way reduces the memory requirement, cache miss rate, and computational cost of using neural networks and finally achieve the goal of higher inference performance. On Intel 3rd generation Xeon Scalable processor, user could expect up to 4x theoretical performance speedup. On Nvidia GPU, it could also bring significant inference performance speedup.", "doc_id": 123},
{"doc": "# Intel Neural Compressor Quantization\n## Quantization Fundamentals\n`Affine quantization` and `Scale quantization` are two common range mapping techniques used in tensor conversion between different data types.\nThe math equation is like: $$X_{int8} = round(Scale \\times X_{fp32} + ZeroPoint)$$.\n**Affine Quantization**\nThis is so-called `asymmetric quantization`, in which we map the min/max range in the float tensor to the integer range. Here int8 range is [-128, 127], uint8 range is [0, 255]. \nhere:\nIf INT8 is specified, $Scale = (|X_{f_{max}} - X_{f_{min}}|) / 127$ and $ZeroPoint = -128 - X_{f_{min}} / Scale$.\nor\nIf UINT8 is specified, $Scale = (|X_{f_{max}} - X_{f_{min}}|) / 255$ and $ZeroPoint = - X_{f_{min}} / Scale$.\n**Scale Quantization**\nThis is so-called `Symmetric quantization`, in which we use the maximum absolute value in the float tensor as float range and map to the corresponding integer range. \nThe math equation is like:\nhere:\nIf INT8 is specified, $Scale = max(abs(X_{f_{max}}), abs(X_{f_{min}})) / 127$ and $ZeroPoint = 0$. \nor\nIf UINT8 is specified, $Scale = max(abs(X_{f_{max}}), abs(X_{f_{min}})) / 255$ and $ZeroPoint = 128$.\n*NOTE*\nSometimes the reduce_range feature, that's using 7 bit width (1 sign bit + 6 data bits) to represent int8 range, may be needed on some early Xeon platforms, it's because those platforms may have overflow issues due to fp16 intermediate calculation result when executing int8 dot product operation. After AVX512_VNNI instruction is introduced, this issue gets solved by supporting fp32 intermediate data.", "doc_id": 124},
{"doc": "# Intel Neural Compressor Quantization\n### Quantization Support Matrix\n\n| Framework | Backend Library |  Symmetric Quantization | Asymmetric Quantization |\n| :-------------- |:---------------:| ---------------:|---------------:|\n| TensorFlow    | [oneDNN](https://github.com/oneapi-src/oneDNN) | Activation (int8/uint8), Weight (int8) | - |\n| PyTorch         | [FBGEMM](https://github.com/pytorch/FBGEMM) | Activation (uint8), Weight (int8) | Activation (uint8) |\n| PyTorch(IPEX) | [oneDNN](https://github.com/oneapi-src/oneDNN)  | Activation (int8/uint8), Weight (int8) | - |\n| MXNet           | [oneDNN](https://github.com/oneapi-src/oneDNN)  | Activation (int8/uint8), Weight (int8) | - |\n| ONNX Runtime | [MLAS](https://github.com/microsoft/onnxruntime/tree/master/onnxruntime/core/mlas) | Weight (int8) | Activation (uint8) |", "doc_id": 125},
{"doc": "# Intel Neural Compressor Quantization\n#### Quantization Scheme in TensorFlow\n+ Symmetric Quantization\n    + int8: scale = 2 * max(abs(rmin), abs(rmax)) / (max(int8) - min(int8) - 1)\n    + uint8: scale = max(rmin, rmax) / (max(uint8) - min(uint8))", "doc_id": 126},
{"doc": "# Intel Neural Compressor Quantization\n#### Quantization Scheme in PyTorch\n+ Symmetric Quantization\n    + int8: scale = max(abs(rmin), abs(rmax)) / (float(max(int8) - min(int8)) / 2)\n    + uint8: scale = max(abs(rmin), abs(rmax)) / (float(max(int8) - min(int8)) / 2)\n+ Asymmetric Quantization\n    + uint8: scale = (rmax - rmin) / (max(uint8) - min(uint8)); zero_point = min(uint8)  - round(rmin / scale)", "doc_id": 127},
{"doc": "# Intel Neural Compressor Quantization\n#### Quantization Scheme in IPEX\n+ Symmetric Quantization\n    + int8: scale = 2 * max(abs(rmin), abs(rmax)) / (max(int8) - min(int8) - 1)\n    + uint8: scale = max(rmin, rmax) / (max(uint8) - min(uint8))", "doc_id": 128},
{"doc": "# Intel Neural Compressor Quantization\n#### Quantization Scheme in MXNet\n+ Symmetric Quantization\n    + int8: scale = 2 * max(abs(rmin), abs(rmax)) / (max(int8) - min(int8) - 1)\n    + uint8: scale = max(rmin, rmax) / (max(uint8) - min(uint8))", "doc_id": 129},
{"doc": "# Intel Neural Compressor Quantization\n#### Quantization Scheme in ONNX Runtime\n+ Symmetric Quantization\n    + int8: scale = 2 * max(abs(rmin), abs(rmax)) / (max(int8) - min(int8) - 1)\n+ Asymmetric Quantization\n    + uint8: scale = (rmax - rmin) / (max(uint8) - min(uint8)); zero_point = min(uint8)  - round(rmin / scale) \n\n# Intel Neural Compressor Quantization\n#### Reference\n+ oneDNN: [Lower Numerical Precision Deep Learning Inference and Training](https://software.intel.com/content/www/us/en/develop/articles/lower-numerical-precision-deep-learning-inference-and-training.html)\n+ FBGEMM: [FBGEMM Quantization](https://github.com/pytorch/pytorch/blob/master/torch/quantization/observer.py)\n+ MLAS:  [MLAS Quantization](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/quantization/onnx_quantizer.py)", "doc_id": 130},
{"doc": "# Intel Neural Compressor Quantization\n### Quantization Approaches\nQuantization has three different approaches:\n1) post training dynamic quantization\n2) post training static  quantization\n3) quantization aware training.\nThe first two approaches belong to optimization on inference. The last belongs to optimization during training.", "doc_id": 131},
{"doc": "# Intel Neural Compressor Quantization\n### Quantization Approaches\n#### Post Training Dynamic Quantization\nThe weights of the neural network get quantized into int8 format from float32 format offline. The activations of the neural network is quantized as well with the min/max range collected during inference runtime.\nThis approach is widely used in dynamic length neural networks, like NLP model.", "doc_id": 132},
{"doc": "# Intel Neural Compressor Quantization\n### Quantization Approaches\n#### Post Training Static Quantization\nCompared with `post training dynamic quantization`, the min/max range in weights and activations are collected offline on a so-called `calibration` dataset. This dataset should be able to represent the data distribution of those unseen inference dataset. The `calibration` process runs on the original fp32 model and dumps out all the tensor distributions for `Scale` and `ZeroPoint` calculations. Usually preparing 100 samples are enough for calibration.\nThis approach is major quantization approach people should try because it could provide the better performance comparing with `post training dynamic quantization`.", "doc_id": 133},
{"doc": "# Intel Neural Compressor Quantization\n### Quantization Approaches\n#### Quantization Aware Training\nQuantization aware training emulates inference-time quantization in the forward pass of the training process by inserting `fake quant` ops before those quantizable ops. With `quantization aware training`, all weights and activations are `fake quantized` during both the forward and backward passes of training: that is, float values are rounded to mimic int8 values, but all computations are still done with floating point numbers. Thus, all the weight adjustments during training are made while aware of the fact that the model will ultimately be quantized; after quantizing, therefore, this method will usually yield higher accuracy than either dynamic quantization or post-training static quantization.", "doc_id": 134},
{"doc": "# Intel Neural Compressor Quantization\n## With or Without Accuracy Aware Tuning\nAccuracy aware tuning is one of unique features provided by Intel(R) Neural Compressor, compared with other 3rd party model compression tools. This feature can be used to solve accuracy loss pain points brought by applying low precision quantization and other lossy optimization methods. \nThis tuning algorithm creates a tuning space by querying framework quantization capability and model structure, selects the ops to be quantized by the tuning strategy, generates quantized graph, and evaluates the accuracy of this quantized graph. The optimal model will be yielded if the pre-defined accuracy goal is met.\nNeural compressor also support to quantize all quantizable ops without accuracy tuning, user can decide whether to tune the model accuracy or not. Please refer to \"Get Start\" below.", "doc_id": 135},
{"doc": "# Intel Neural Compressor Quantization\n## With or Without Accuracy Aware Tuning\n### Working Flow\nCurrently `accuracy aware tuning` supports `post training quantization`, `quantization aware training`.\nUser could refer to below chart to understand the whole tuning flow.", "doc_id": 136},
{"doc": "# Intel Neural Compressor Quantization\n## Get Started\nThe design philosophy of the quantization interface of Intel(R) Neural Compressor is easy-of-use. It requests user to provide `model`, `calibration dataloader`, and `evaluation function`. Those parameters would be used to quantize and tune the model. \n\n`model` is the framework model location or the framework model object.\n\n`calibration dataloader` is used to load the data samples for calibration phase. In most cases, it could be the partial samples of the evaluation dataset.\n\nIf a user needs to tune the model accuracy, the user should provide either `evaluation function` or `evaluation dataloader` `evaluation metric`. If the user won't to tune the model accuracy, then the user should provide neither `evaluation function` nor `evaluation dataloader` `evaluation metric`.\n\n`evaluation function` is a function used to evaluate model accuracy. It is a optional. This function should be same with how user makes evaluation on fp32 model, just taking `model` as input and returning a scalar value represented the evaluation accuracy.\n\n`evaluation dataloader` is a data loader for evaluation. It is iterable and should yield a tuple of (input, label). The input could be a object, list, tuple or dict, depending on user implementation, as well as it can be taken as model input. The label should be able to take as input of supported metrics. If this parameter is not None, user needs to specify pre-defined evaluation metrics through configuration file and should set \"eval_func\" parameter as None. Tuner will combine model, eval_dataloader and pre-defined metrics to run evaluation process.\n\n`evaluation metric` is an object to compute the metric to evaluating the performance of the model or a dict of built-in metric configures, neural_compressor will initialize this class when evaluation. `evaluation metric` must be supported by neural compressor. Please refer to [metric.md](metric.md).\n\n# Intel Neural Compressor Quantization\n## Get Started\n### Post Training Quantization\n1. Without Accuracy Aware Tuning\nThis means user could leverage Intel(R) Neural Compressor to directly generate a fully quantized model without accuracy aware tuning. It's user responsibility to ensure the accuracy of the quantized model meets expectation. Intel(R) Neural Compressor support `Post Training Static Quantization` and `Post Training Dynamic Quantization`.\n``` python\n# main.py\n# Original code\nmodel = ResNet50()\nval_dataset = ...\nval_dataloader = torch.utils.data.Dataloader(\n                     val_dataset,\n                     batch_size=args.batch_size, shuffle=False,\n                     num_workers=args.workers, ping_memory=True)\n# Quantization code\nfrom neural_compressor import quantization\nfrom neural_compressor.config import PostTrainingQuantConfig\n\nconf = PostTrainingQuantConfig() # default approach is \"auto\", you can set \"dynamic\":PostTrainingQuantConfig(approach=\"dynamic\")\nq_model = quantization.fit(model=model,\n                           conf=conf,\n                           calib_dataloader=val_dataloader)\nq_model.save('./output')\n```\n2. With Accuracy Aware Tuning\nThis means user could leverage the advance feature of Intel(R) Neural Compressor to tune out a best quantized model which has best accuracy and good performance. User should provide either `eval_func` or `eval_dataloader` `eval_metric`.\n``` python\n# main.py\n# Original code\ndef validate(val_loader, model, criterion, args):\n    ...\n    return top1.avg\n\nmodel = ResNet50()\nval_dataset = ...\nval_dataloader = torch.utils.data.Dataloader(\n                     val_dataset,\n                     batch_size=args.batch_size, shuffle=False,\n                     num_workers=args.workers, ping_memory=True)\n# Quantization code\nfrom neural_compressor import quantization\nfrom neural_compressor.config import PostTrainingQuantConfig\n\nconf = PostTrainingQuantConfig()\nq_model = quantization.fit(model=model,\n                           conf=conf,\n                           calib_dataloader=val_dataloader,\n                           eval_func=validate)\nq_model.save('./output')\n```\nor\n```python\nfrom neural_compressor.metric import METRICS\nmetrics = METRICS('pytorch')\ntop1 = metrics['topk']()\nq_model = quantization.fit(model=model,\n                           conf=conf,\n                           calib_dataloader=val_dataloader,\n                           eval_dataloader=val_dataloader,\n                           eval_metric=top1)\n```", "doc_id": 137},
{"doc": "# Intel Neural Compressor Quantization\n## Get Started\n### Quantization Aware Training\n1. Without Accuracy Aware Tuning\nThis method only requires the user to call the callback function during the training process. After the training is completed, after the training is completed, Neural Compressor will convert to quantized model. \n```python\n# main.py\n# Original code\nmodel = ResNet50()\ntrain_dataset = ...\ntrain_dataloader = torch.utils.data.Dataloader(\n                     train_dataset,\n                     batch_size=args.batch_size, shuffle=True,\n                     num_workers=args.workers, ping_memory=True)\ncriterion = ...\n# Quantization code\ndef train_func(model):\n    ...\n\nfrom neural_compressor import QuantizationAwareTrainingConfig\nfrom neural_compressor.training import prepare_compression\nconf = QuantizationAwareTrainingConfig()\ncompression_manager = prepare_compression(model, conf)\ncompression_manager.callbacks.on_train_begin()\nmodel = compression_manager.model\ntrain_func(model)\ncompression_manager.callbacks.on_train_end()\ncompression_manager.save('./output')\n```\n2. With Accuracy Aware Tuning\nThis method requires the user to provide training function and evaluation function to Neural Compressor, and in training function, the user should call the callback function.\n```python\n# main.py\n# Original code\nmodel = ResNet50()\nval_dataset = ...\nval_dataloader = torch.utils.data.Dataloader(\n                     val_dataset,\n                     batch_size=args.batch_size, shuffle=False,\n                     num_workers=args.workers, ping_memory=True)\ncriterion = ...\ndef validate(val_loader, model, criterion, args):\n    ...\n    return top1.avg\n# Quantization code\ndef train_func(model):\n    ...\n    return model  # user should return a best performance model here\nfrom neural_compressor import QuantizationAwareTrainingConfig\nfrom neural_compressor.training import prepare_compression, fit\nconf = QuantizationAwareTrainingConfig()\ncompression_manager = prepare_compression(model, conf)\nq_model = fit(compression_manager=compression_manager, train_func=train_func, eval_func=validate)\ncompression_manager.save('./output')\n```", "doc_id": 138},
{"doc": "# Intel Neural Compressor Quantization\n## Get Started\n### Specify Quantization Rules\nIntel(R) Neural Compressor support specify quantization rules by operator name or operator type. Users can set `op_name_dict` and `op_type_dict` in config class to achieve the above purpose.\n1. Example of `op_name_dict`\n```python\nop_name_dict = {\n    \"layer1.0.conv1\": {\n        \"activation\": {\n            \"dtype\": [\"fp32\"]\n        },\n        \"weight\": {\n            \"dtype\": [\"fp32\"]\n        }\n    },\n    \"layer2.0.conv1\": {\n        \"activation\": {\n            \"dtype\": [\"uint8\"],\n            \"algorithm\": [\"minmax\"],\n            \"granularity\": [\"per_tensor\"],\n            \"scheme\": [\"sym\"]\n        },\n        \"weight\": {\n            \"dtype\": [\"int8\"],\n            \"algorithm\": [\"minmax\"],\n            \"granularity\": [\"per_channel\"],\n            \"scheme\": [\"sym\"]\n        }\n    },\n}\nconf = PostTrainingQuantConfig(op_name_dict=op_name_dict)\n```\n2. Example of `op_type_dict`\n```python\nop_type_dict = {\n    'Conv': {\n        'weight': {\n            'dtype': ['fp32']\n        },\n        'activation': {\n            'dtype': ['fp32']\n        }\n    }\n}\nconf = PostTrainingQuantConfig(op_type_dict=op_type_dict)\n```", "doc_id": 139},
{"doc": "# Intel Neural Compressor Quantization\n## Get Started\n### Specify Quantization Recipes\nIntel(R) Neural Compressor support some quantization recipes. Users can set `recipes` in config class to achieve the above purpose. (`fast_bias_correction` and `weight_correction` is working in progress.)\n\n| Recipes | PyTorch |  Tensorflow | ONNX Runtime |\n| :---------------- |:---------------:| ---------------:|---------------:|\n| smooth_quant      | \u2705 | N/A | \u2705 |\n| smooth_quant_args | \u2705 | N/A | \u2705 |\n| fast_bias_correction | N/A | N/A | N/A |\n| weight_correction | N/A | N/A | N/A |\n| first_conv_or_matmul_quantization | N/A | \u2705 | \u2705 |\n| last_conv_or_matmul_quantization | N/A | \u2705 | \u2705 |\n| pre_post_process_quantization | N/A | N/A | \u2705 |\n| gemm_to_matmul | N/A | N/A | \u2705 |\n| graph_optimization_level | N/A | N/A | \u2705 |\n| add_qdq_pair_to_weight | N/A | N/A | \u2705 |\n| optypes_to_exclude_output_quant | N/A | N/A | \u2705 |\n| dedicated_qdq_pair | N/A | N/A | \u2705 |\n\nExample of recipe:\n```python\nrecipes = {\n    \"smooth_quant\": True,\n    \"smooth_quant_args\": {\n        \"alpha\": 0.5 # default value is 0.5\n    },\n    \"fast_bias_correction\": False,\n}\nconf = PostTrainingQuantConfig(recipes=recipes)\n```", "doc_id": 140},
{"doc": "# Intel Neural Compressor Quantization\n## Examples\nUser could refer to [examples](https://github.com/intel/neural-compressor/blob/master/examples/README.md) on how to quantize a new model.", "doc_id": 141},
{"doc": "# Intel Neural Compressor Mixed Precision during Quantization\n### Turn OFF Auto Mixed Precision during Quantization\n\nBF16 conversion during quantization is default ON. To force disable it, users need to exclude \"bf16\" in PostTrainingQuantConfig and QuantizationAwareTrainingConfig:\n\n```python\nfrom neural_compressor.config import PostPostTrainingQuantConfig\nfrom neural_compressor import quantization\n\nconf = PostTrainingQuantConfig(excluded_precisions=['bf16'])\nq_model = quantization.fit(model_origin,\n                           conf,\n                           calib_dataloader=dataloader,\n                           calib_func=eval_func)\n```", "doc_id": 142},
{"doc": "# Intel Neural Compressor Mixed Precision during Quantization\n### Tensorflow\nIntel has worked with the TensorFlow development team to enhance TensorFlow to include bfloat16 data support for CPUs. For more information about BF16 in TensorFlow, please read [Accelerating AI performance on 3rd Gen Intel\u00ae Xeon\u00ae Scalable processors with TensorFlow and Bfloat16](https://blog.tensorflow.org/2020/06/accelerating-ai-performance-on-3rd-gen-processors-with-tensorflow-bfloat16.html).\n- BF16 conversion during quantization in TensorFlow\n- Three steps\n1. Convert to a `FP32 + INT8` mixed precision Graph\n   In this steps, TF adaptor will regard all fallback datatype as `FP32`. According to the per op datatype in tuning config passed by strategy, TF adaptor will generate a `FP32 + INT8` mixed precision graph.\n2. Convert to a `BF16 + FP32 + INT8` mixed precision Graph\n   In this phase, adaptor will convert some `FP32` ops to `BF16` according to `bf16_ops` list in tuning config.\n3. Optimize the `BF16 + FP32 + INT8` mixed precision Graph\n   After the mixed precision graph generated, there are still some optimization need to be applied to improved the performance, for example `Cast + Cast` and so on. The `BF16Convert` transformer also apply a depth-first method to make it possible to take the ops use `BF16` which can support `BF16` datatype to reduce the insertion of `Cast` op.", "doc_id": 143},
{"doc": "# Intel Neural Compressor Mixed Precision during Quantization\n### PyTorch\nIntel has also worked with the PyTorch development team to enhance PyTorch to include bfloat16 data support for CPUs.\n- BF16 conversion during quantization in PyTorch\n- Two steps\n1. Convert to a `FP32 + INT8` mixed precision Graph or Module\n   In this steps, PT adaptor will combine the `INT8` ops and all fallback ops to `FP32 + INT8` mixed precision Graph or Module no matter in Eager mode or Fx Graph mode.\n2. Convert to a `BF16 + FP32 + INT8` mixed precision Graph or Module\n   In this phase, adaptor will according to `BF16` op list from strategy tune config to wrapper the `FP32` module with `BF16Wrapper` to realize the `BF16 + FP32 + INT8` mixed precision Graph or Module. adaptor will do retrace the `GraphModule` again if using Fx Graph mode.", "doc_id": 144},
{"doc": "# Intel Neural Compressor Release\n## Release Notes\n\nView new feature information and release downloads for the latest and previous releases on GitHub. Validated configurations and distribution sites located here as well:\n\n> <https://github.com/intel/neural-compressor/releases>\n\nContact inc.maintainers@intel.com if you need additional assistance.", "doc_id": 145},
{"doc": "# Intel Neural Compressor Release\n## Known Issues\n\nThe MSE tuning strategy does not work with the PyTorch adaptor layer. This strategy requires a comparison between the FP32 and INT8 tensors to decide which op impacts the final quantization accuracy. The PyTorch adaptor layer does not implement this inspect tensor interface. Therefore, do not choose the MSE tuning strategy for PyTorch models.", "doc_id": 146},
{"doc": "# Intel Neural Compressor Release\n## Incompatible Changes\n\n[Neural Compressor v1.2](https://github.com/intel/neural-compressor/tree/v1.2) introduces incompatible changes in user facing APIs. Please refer to [incompatible changes](incompatible_changes.md) to know which incompatible changes are made in v1.2.\n\n[Neural Compressor v1.2.1](https://github.com/intel/neural-compressor/tree/v1.2.1) solves this backward compatible issues introduced in v1.2 by moving new user facing APIs to neural_compressor.experimental package and keep old one as is. Please refer to [API documentation](./api-documentation/apis.rst) to know the details of user-facing APIs.\n\n[Neural Compressor v1.7](https://github.com/intel/neural-compressor/tree/v1.7) renames the pip/conda package name from lpot to neural_compressor. To run old examples on latest software, please replace package name for compatibility with `sed -i \"s|lpot|neural_compressor|g\" your_script.py` .\n\n[Neural Compressor v2.0](https://github.com/intel/neural-compressor/tree/v2.0) renames the `DATASETS` class as `Datasets`, please notice use cases like `from neural_compressor.data import Datasets`. Details please check the [PR](https://github.com/intel/neural-compressor/pull/244/files).", "doc_id": 147},
{"doc": "# Intel Neural Compressor SigOpt Strategy\n## Introduction\n\n[SigOpt](https://app.sigopt.com/) is an online model development platform that makes it easy to track runs, visualize training, and scale hyperparameter optimization for any type of model. [Optimization Loop](https://app.sigopt.com/docs/overview/optimization) is the backbone of using SigOpt. We can set metrics and realize the interaction between the online platform and tuning configurations based on this mechanism.", "doc_id": 148},
{"doc": "# Intel Neural Compressor SigOpt Strategy\n### Preparation\n\nBefore using the `SigOpt` strategy, a SigOpt account is necessary.\n- Each account has its own API token. Find your API token and then fill it in the `sigopt_api_token` field. \n- Create a new project and fill the corresponding name into the `sigopt_project_id` field.\n- Set the name of this experiment in `sigopt_experiment_id` field optionally. The default name is \"nc-tune\".", "doc_id": 149},
{"doc": "# Intel Neural Compressor SigOpt Strategy\n### SigOpt Platform \n\nIf you are using the SigOpt products for the first time, please [sign-up](https://app.sigopt.com/signup), if not, please [login](https://app.sigopt.com/login). It is free to apply for an account. Although there are certain restrictions on the model parameters and the number of experiments created, it is sufficient for ordinary customers. If you want higher capacity, please contact support@sigopt.com.\n\nAfter logging in, you can use `the token api` to connect the local code to the online platform, corresponding to `sigopt_api_token`. It can be obtained [here](https://app.sigopt.com/tokens/info).\n\nSigOpt has two concepts: [project](https://app.sigopt.com/projects) and [experiment](https://app.sigopt.com/experiments). Create a project before experimenting, corresponding to `sigopt_project_id` and `sigopt_experiment_name`. Multiple experiments can be created on each project. After creating the experiment, SigOpt will execute three simple steps below in a loop:\n\n- Receive a Suggestion from SigOpt;\n- Evaluate your metrics;\n- Report an Observation to SigOpt;\n\nIn our built-in sigopt strategy, the metrics add accuracy as a constraint and optimize for latency.", "doc_id": 150},
{"doc": "# Intel Neural Compressor SigOpt Strategy\n### Neural Compressor Configuration\nCompare to `Basic` strategy, `sigopt_api_token` and `sigopt_project_id` is necessary for `SigOpt` strategy. Before using the strategy, it is required to create the project corresponding to `sigopt_project_id` in your account.\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    tuning_criterion=TuningCriterion(\n        strategy=\"sigopt\",\n        strategy_kwargs={\n            \"sigopt_api_token\": \"YOUR-ACCOUNT-API-TOKEN\",\n            \"sigopt_project_id\": \"PROJECT-ID\",\n            \"sigopt_experiment_name\": \"nc-tune\",\n        },\n    ),\n)\n```", "doc_id": 151},
{"doc": "# Intel Neural Compressor SigOpt Strategy\n## Performance\n\n### Benefit of SigOpt Strategy\n\n- Metric based SigOpt is better than self-defining and easy to use. You can read the details [here](https://app.sigopt.com/docs/overview/metric_constraints). \n- With the token api, results of each experiment are recorded in your account. You can use the SigOpt data analysis function to analyze the results, such as drawing a chart, calculating the F1 score, etc.", "doc_id": 152},
{"doc": "# Intel Neural Compressor SigOpt Strategy\n## Performance\n### Performance Comparison of Different Strategies\n- MobileNet_v1(tensorflow)\n\n    |strategy|FP32 baseline|int8 accuracy|int8 duration(s)|\n    |--------|-------------|-------------|----------------|\n    |  basic |  0.8266     | 0.8372      |  88.2132       |\n    | sigopt |  0.8266     | 0.8372      |  83.7495       |\n\n- ResNet50_v1(tensorflow)\n\n    |strategy|FP32 baseline|int8 accuracy|int8 duration(s)|\n    |--------|-------------|-------------|----------------|\n    |  basic |  0.8299     | 0.8294      |  85.0837       |\n    | sigopt |  0.8299     | 0.8291      |  83.4469       |", "doc_id": 153},
{"doc": "# Intel Neural Compressor Smooth Quant\n## Introduction\n\nQuantization is a common compression operation to reduce memory and accelerate inference by converting the floating point matrix to an integer matrix. For large language models (LLMs) with gigantic parameters, the systematic outliers make quantification of activations difficult.  [SmoothQuant](https://arxiv.org/abs/2211.10438), a training free post-training quantization (PTQ) solution, offline migrates this difficulty from activations to weights with a mathematically equivalent transformation.", "doc_id": 154},
{"doc": "# Intel Neural Compressor Smooth Quant\n## Quantization Fundamentals\nQuantization is a common compression operation to reduce memory and accelerate inference; therefore, the difficulty of LLM deployment can be alleviated. Quantization converts the floating point matrix to an integer matrix.\nThe equation of quantization is as follows:\n\n$$\nX_{int8} = round(X_{fp32}/S) + Z \\tag{1}\n$$\n\nwhere $X_{fp32}$ is the input matrix, $S$ is the scale factor,  $Z$ is the integer zero point.", "doc_id": 155},
{"doc": "# Intel Neural Compressor Smooth Quant\n### Per-tensor & Per-channel\n\nThere are several choices of sharing quantization parameters among tensor elements, also called quantization granularity. The coarsest level, per-tensor granularity, is that all elements in the tensor share the same quantization parameters. Finer granularity means sharing quantization parameters per row or per column for 2D matrices and per channel for 3D matrices. Similarly, the finest granularity is that each element has an individual parameter.\n\nHowever, due to the model accuracy and computational consumption, per-tensor or per-channel are usually adopted. **In the following part, We will show per-channel could bring lower quantization loss but with some limitations, that is why normally we use per-channel for weight quantization and per-tensor for activation/input quantization**", "doc_id": 156},
{"doc": "# Intel Neural Compressor Smooth Quant\n#### Per-tensor example\nSuppose the weight tensor is:\n```python\nimport torch\nW = torch.Tensor(\n    [[0.6839, 0.4741, 0.7451],\n    [0.9301, 0.1742, 0.6835]]\n    )\n```\nAccording to the formula (1), we need to scale $S$ and zero point $Z$ to calculate the integer matrix.\n\n$$\nS = \\frac{X_{max} - X{min}}{2^b -1} \\tag{2}\n$$\n\n$$\nZ = -round(X_{min/}/S) \\tag{3}\n$$\n\nThe per-tensor quantization function is:\n\n```python\ndef quantize(x, num_bits=8):\n    q_min, q_max = 0, 2. ** num_bits - 1.\n    scale = (torch.max(x) - torch.min(x)) / (2 ** num_bits - 1)\n    scale = torch.clip(scale, min=1e-5)\n    zp = torch.round(0 - (torch.min(x)) / scale)\n    q_x = x / scale + zp\n    q_x.clamp_(q_min, q_max).round_()\n    print(f'scale = {scale}, zp = {zp}')\n    return q_x, scale, zp\n```\n\nThen we can get the quantized $W_{q}$\n\n```bash\n>>> W_q, scale, zp = quantize(W)\nscale = 0.00296431384049356, zp = -59.0\n>>> W_q\ntensor([[172., 101., 192.],\n        [255.,   0., 172.]])\n```\n\nWith the value of scale and zp, we can dequantize the tensor.\n\n```python\ndef dequantize(q_x, scale, zp):\n    return scale * (q_x - zp)\n```\n\n```bash\n>>> W_dq = dequantize(W_dq, 0.001, -50)\n>>> W_dq\ntensor([[0.1220, 0.0500, 0.1430],\n        [0.2570, 0.0500, 0.1890]])\n>>> loss = torch.nn.MSELoss()(W_dq, W)\n>>> loss.item()\n0.1983354538679123\n\n>>> W_dq = dequantize(W_q, scale, zp)\n>>> W_dq\ntensor([[0.6848, 0.4743, 0.7440],\n        [0.9308, 0.1749, 0.6848]])\n>>> loss = torch.nn.MSELoss()(W_dq, W)\n>>> loss.item()\n7.385297635664756e-07\n```\nThe difference between $W$ and $W_{dq}$ shows that quantization affects precision and appropriate values of scale and zero point will reduce the loss of precision.", "doc_id": 157},
{"doc": "# Intel Neural Compressor Smooth Quant\n#### Per-channel example\nSimilarly, the example of per-channel quantization is as follows:\n\n```python\ndef quantize_per_channel(x, num_bits=8):\n    q_min, q_max = 0, 2. ** num_bits - 1.\n    x_tmp = x.detach().reshape(x.shape[0], -1)\n    scales = x_tmp.max(dim=-1, keepdim=True)[0] / (2 ** num_bits - 1)\n    zp =  torch.round(0 - x_tmp.min(dim=-1, keepdim=True)[0].divide(scales))\n    q_x = x_tmp.divide(scales) + zp\n    q_x.clamp_(q_min, q_max).round_()\n    print(f'scale = {scales}, \\n zp = {zp}')\n    return q_x, scale, zp\n\ndef dequantize_per_channel(q_x, scales, zp):\n    print(q_x, scales, zp)\n    print(scales * (q_x - zp))\n    return scales * (q_x - zp)\n```\n\n```bash\n>>>W_q, scale, zp = quantize_per_channel(W)\nscale = tensor([[0.0029],\n        [0.0036]]), \nzp = tensor([[-162.],\n        [ -48.]])\n>>>W_q\ntensor([[ 72.,   0.,  93.],\n        [207.,   0., 139.]])\n\n>>>W_dq = dequantize_per_channel(W_q, scales, zp)\n>>>W_dq\ntensor([[0.6837, 0.4734, 0.7451],\n        [0.9301, 0.1751, 0.6821]])\n```\nAnd the loss is\n```bash\n>>> loss = torch.nn.MSELoss()(W_dq, W)\n>>> loss.item()\n5.637690492221736e-07\n```\nThrough this example, we can see that per-channel quantization has finer granularity and has lower loss (loss 5.6376e-07 for per-channel quantization and 7.3852e-07 for per-tensor quantization).", "doc_id": 158},
{"doc": "# Intel Neural Compressor Smooth Quant\n#### Matmul quantization example\n\nFor a linear layer in most model, $Y=X \\cdot W$, we can quantize both the weights and activations in order to reduce the storage and accelerate inference.\nUsing per-tensor scale quantization to show the process.\n\n```python\ndef quantize_per_tensor_absmax(x, n_bits=8):\n    scales = x.abs().max()\n    q_max = 2**(n_bits-1)-1\n    scales.clamp_(min=1e-5).div_(q_max)\n    q_x = x / scales\n    q_x = q_x.clamp_(-q_max, q_max).round_()\n    return q_x, scales\n\ndef dequantize(q_x, scale):\n    return scale * q_x\n```\nRandomly initialize the $W$ and $Y$, then calculate the result of $Y=X \\cdot W$\n```bash\n>>>W = torch.rand(2, 3, dtype=torch.float32)\n>>>X = torch.rand(3, 4, dtype=torch.float32)\n>>>W\ntensor([[0.0806, 0.7589, 0.6038],\n        [0.3815, 0.5040, 0.7174]])\n>>>X\ntensor([[0.5444, 0.5826, 0.7772, 0.5555],\n        [0.3740, 0.3253, 0.0698, 0.1381],\n        [0.5972, 0.0086, 0.0737, 0.8298]])\n>>>Y = torch.matmul(W, X)\n>>>Y\ntensor([[0.6883, 0.2991, 0.1601, 0.6506],\n        [0.8246, 0.3924, 0.3845, 0.8768]])\n```\n\nQuantize weight and activation, matmul(quantize(X), quantize(Y))\n\n```bash\n>>>W_q, W_scale = quantize_per_tensor_absmax(W)\n>>>X_q, X_scale = quantize_per_tensor_absmax(X)\n>>>print(f'{W_q}\\n{W_scale.item()}')\n>>>print(f'{X_q}\\n{X_scale.item()}')\ntensor([[ 13., 127., 101.],\n        [ 64.,  84., 120.]])\n0.0059755356051027775\ntensor([[ 83.,  89., 119.,  85.],\n        [ 57.,  50.,  11.,  21.],\n        [ 91.,   1.,  11., 127.]])\n0.006533813662827015\n\n>>>Y_q = torch.matmul(W_q, X_q)\n>>>Y_q\ntensor([[17509.,  7608.,  4055., 16599.],\n        [21020., 10016.,  9860., 22444.]])\n>>>Y_dq = dequantize(Y, W_scale * X_scale)\n>>>Y_dq\ntensor([[0.6836, 0.2970, 0.1583, 0.6481],\n        [0.8207, 0.3911, 0.3850, 0.8763]])", "doc_id": 159},
{"doc": "# Intel Neural Compressor Smooth Quant\n#### Per-channel limitation\n\nThough per-channel quantization could bring lower quantization error, we could not apply it for activations due to the difficulty of the dequantization. We would prove it in the following image and the zero point of quantization would be ignored for simplicity.\n\nThe left side of the image presents a normal linear forward  with 1x2 input $x$ and 2x2 weight $w$. The results $y$ could be easily obtained by simple mathematics. In the middle sub-image, we apply per-tensor quantization for activations and per-channel quantization for weights; the results after quantization that are denoted by $y_1$ and $y_2$, could be easily dequantized to the float results $y_{fp1}$ and $y_{fp2}$ by per channel scale $1.0/s_1s_x$ and $1.0/s_2s_x$. However, after applying per-channel quantization for activation on the right sub-image, we could not dequantize the  $y_1$ and  $y_2$ to float results.", "doc_id": 160},
{"doc": "# Intel Neural Compressor Smooth Quant\n# SmoothQuant and Our Enhancement\n\n### SmoothQuant\n\nIn the previous subsection, we have explained why per-channel quantization could not be applied for activation, even though it could lead to lower quantization loss. However, the quantization error loss of activation plays an important role in the accuracy loss of model quantization[^2][^3][^4]. \n\nTo reduce the quantization loss of activations, lots of methods have been proposed. In the following, we briefly introduce SPIQ[^2], Outlier Suppression[^3] and Smoothquant[^4]. All these three methods share a similar idea to migrate the difficulty from activation quantization to weight quantization but differ in how much difficulty to be transferred.\n\nSo **the first question is how to migrate the difficulty from activation to weights?** The solution is straightforward, that is to convert the network to an output equivalent network that is presented in the image below and apply quantization to this equivalent network. The intuition is that each channel of activation could be scaled to make it more quantization-friendly, similar to a fake per-channel activation quantization.\n\nPlease note that this conversion will make the quantization of weights more difficult, because the scales attached to weights showed above are per-input-channel, while quantization of weights is per-output-channel or per-tensor.\n\nSo **the second question is how much difficulty to be migrated**, that is how to choose the **convention per-channel scale** $s_{x1}$ and $s_{x2}$ on the above image. Different works adopt different ways.\n\n*SPIQ* just adopts the quantization scale of activations as the convention per-channel scale.\n\n*Outlier suppression* adopts the scale of the preceding layernorm as the convention per-channel scale.\n\n*Smoothquant* introduces a hyperparameter $\\alpha$ as a smooth factor to calculate the convention per-channel scale and balance the quantization difficulty of activation and weight.\n\n$$\ns_j = max(|X_j|)^\\alpha/max(|W_j|)^{1-\\alpha} \\tag{4}\n$$\n\nj is the index of the input channels.\n\nFor most of the models such as OPT and BLOOM, $\\alpha = 0.5$ is a well-balanced value to split the difficulty of weight and activation quantization. A larger $\\alpha$ value could be used on models with more significant activation outliers to migrate more quantization difficulty to weights.", "doc_id": 161},
{"doc": "# Intel Neural Compressor Smooth Quant\n### Our enhancement: \n\n#### Algorithm: Layer-wise Auto-tuning of $\\alpha$.\n\nSmoothQuant method aims to split the quantization difficulty of weight and activation by using a fixed-value $\\alpha$ for an entire model. However, as the distributions of activation outliers vary not only across different models but also across different layers within a model, we hereby propose a method to obtain layer-wise optimal $\\alpha$ values with the ability to tune automatically.\n\nOur proposed method consists of 5 major steps:\n\n-    Hook input and output values of all layers using register_forward_hook.\n-    Generate a list of $\\alpha$ values given user-defined $\\alpha$ range and step_sizes.\n-    Recalculate smoothing factor given an $\\alpha$ value and adjust parameters(weights and activations).\n-    Perform per-channel quantization_dequantization of weights and per-tensor quantization_dequantization of inputs to predict the layer-wise output corresponding to the given $\\alpha$ value.\n-    Calculate the mean-squared loss with respect to the actual output value, recover the adjusted parameters and save the layer-wise optimal $\\alpha$ values.\n\nMultiple criteria (e.g min, max and mean) are supported to determine the $\\alpha$ value of an input LayerNorm op of a transformer block.\n\nIn our experiments, an $\\alpha$ range of [0.3, 0.7] with a step_size of 0.05 is found to be well-balanced one for the majority of models.", "doc_id": 162},
{"doc": "# Intel Neural Compressor Smooth Quant\n### Our enhancement:\n#### Engineering \n\n*fully automated*: the user only needs to pass a model and dataloader\n```python\nfrom neural_compressor.adaptor.torch_utils.smooth_quant import TorchSmoothQuant\nsq = TorchSmoothQuant(model, dataloader)\nsq.transform(alpha) ##alpha could be a float or a string 'auto'\n```\nplease note that we rely on torch jit to analyze the model. If you are using huggingface model, you could set torchscript to True when loading the model or set the return_dict to False\"\n\n*support lots of fusing patterns*: when applying the convention per-channel scales, a mul layer needs to be inserted, which will introduce some overhead. The official code fuses this op to the previous layernorm, while we support more fusing patterns, like linear_1->relu->linear_2, which means the scales of linear_1 will be fused to linear_2. All the supported patterns are shown below. Currently we only handle the layer whose scale could be fused, we are trying to support other layers, please stay tuned.\n\n```bash\nconv2d/linear->relu/leakyrelu/hardtanh->conv2d/linear/layernorm/batchnorm/instancenorm/t5norm/llamanorm/groupnorm/\n\nconv2d/linear->conv2d/linear/layernorm/batchnorm/instancenorm/t5norm/llamanorm/groupnorm\n```", "doc_id": 163},
{"doc": "# Intel Neural Compressor Smooth Quant\n## Validated Models\nDataset: lambada, task: text-generation, alpha [0.4, 0.6] is sweet spot region in SmoothQuant paper\n| Model\\Last token accuracy |  FP32  | INT8 (w/o SmoothQuant) | INT8 (w/ SmoothQuant) | INT8 (w/ SmoothQuant auto tuning) |\n|---------------------|:------:|:----------------------:|-----------------------|-----------------------------------|\n| bigscience/bloom-560m | 65.20% |         63.44%         | 66.48% (alpha=0.5)    | 64.76% (alpha: 95.9% over 0.6, 4.1% in [0.4, 0.6])                           |\n| bigscience/bloom-1b7 | 71.43% |         67.78%         | 72.56% (alpha=0.5)    | 72.58% (alpha: 55.1% over 0.6, 30.6% in [0.4, 0.6], 14.3% under 0.4)                            |\n| bigscience/bloom-3b | 73.97% |         69.99%         | 74.02% (alpha=0.5)    | 74.16% (alpha: 100% over 0.6)                            |\n| bigscience/bloom-7b1 | 77.44% |         75.46%         | 77.02%(alpha=0.5)     | 77.45% (alpha: 91.8% over 0.6, 4.9% in [0.4, 0.6], 3.3% under 0.4)                           |\n| bigscience/bloom-176b | 84.17% |         82.13%         | 83.52% (alpha=0.6)    | -                                 |\n| facebook/opt-125m   | 63.89% |         63.48%         | 63.44% (alpha=0.5)    | 64.14% (alpha: 59.4% over 0.6, 8.1% in [0.4, 0.6], 32.4% under 0.4)                           |\n| facebook/opt-1.3b   | 75.41% |         73.59%         | 70.94% (alpha=0.5)    | 74.80% (alpha: 69.9% over 0.6, 24.7% in [0.4, 0.6], 5.5% under 0.4)                            |\n| facebook/opt-2.7b   | 77.79% |         78.57%         | 78.60%(alpha=0.5)     | 78.25% (alpha: 73.2% over 0.6, 21.6% in [0.4, 0.6], 5.2% under 0.4)                           |\n| facebook/opt-6.7b   | 81.26% |         76.65%         | 81.58%(alpha=0.5)     | 81.39% (alpha: 68.0% over 0.6, 26.8% in [0.4, 0.6], 5.2% under 0.4)                           |\n| EleutherAI/gpt-j-6B | 79.17% |         78.82%         | 78.84%(alpha=0.6)     | 79.29% (alpha: 96.4% over 0.6, 3.6% in [0.4, 0.6])                                            |", "doc_id": 164},
{"doc": "# Intel Neural Compressor Smooth Quant\n## Examples\n\nUser could refer to [examples](https://github.com/intel/neural-compressor/blob/master/examples/pytorch/nlp/huggingface_models/language-modeling/quantization/ptq_static/ipex/smooth_quant/README.md) on how to use smooth quant.", "doc_id": 165},
{"doc": "# Intel Neural Compressor Smooth Quant\n## Reference\n\n[^1]: Jason, Wei, et al. \"Emergent Abilities of Large Language Models\". Published in Transactions on Machine Learning Research (2022)\n\n[^2]: Yvinec, Edouard, et al. \"SPIQ: Data-Free Per-Channel Static Input Quantization.\" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2023.\n\n[^3]: Wei, Xiuying, et al. \"Outlier suppression: Pushing the limit of low-bit transformer language models.\" arXiv preprint arXiv:2209.13325 (2022).\n\n[^4]: Xiao, Guangxuan, et al. \"Smoothquant: Accurate and efficient post-training quantization for large language models.\" arXiv preprint arXiv:2211.10438 (2022)..", "doc_id": 166},
{"doc": "# Intel Neural Compressor TensorBoard\n## Introduction\n\nTensorBoard is a suite of web applications that provide measurements and visualizations used to inspect and understand your machine learning workflow for [TensorFlow TensorBoard](https://github.com/tensorflow/tensorboard) and [PyTorch TensorBoard](https://github.com/pytorch/pytorch/tree/master/torch/utils/tensorboard). Intel\u00ae Neural Compressor performs accuracy-driven quantization; the tuning process quantizes the tensor and performs graph transformation and optimization to achieve optimal performance under accuracy requirement. If you want to observe the behaviors of the optimizations, or if you want to discover why an accuracy target cannot be met, TensorBoard can provide you with some valuable information. You can inspect the graph and tensor after each tuning run. If a model cannot meet accuracy requirements, you can analyze the comparison of FP32 and the INT8 tensor histogram.    \n\nWe collect the TensorBoard event summary during evaluation. The first time is on the baseline FP32 model and later on at the end of each tuning runs are based on the quantized model. The TensorBoard log directory is named baseline_acc_<accuracy> and tune_<runs>_acc_<accuracy>, to indicate the stage and accuracy of the data that is generated. Users can select their data of interest to observe with TensorBoard.", "doc_id": 167},
{"doc": "# Intel Neural Compressor TensorBoard\n## Supported Feature Matrix\n| Optimized Framework | Tensorboard Support |\n|---------------------|:-------------------:|\n| PyTorch             |       &#10004;      |\n| TensorFlow          |       &#10004;      |", "doc_id": 168},
{"doc": "# Intel Neural Compressor TensorBoard\n## Get Started with TensorBoard\n### PyTorch TensorBoard\n\nPyTorch TensorBoard implementation includes three steps:\n\n* Before evaluation in the _pre_eval_hook() where instruments observers are placed in the model.\n* During evaluation where observers collect tensor information in a dict data structure.\n* After evaluation where the graph and tensor information is dumped with the TensorBoard summary writer in _post_eval_hook().", "doc_id": 169},
{"doc": "The detailed algorithm can be described by the Pseudo code:\n```python\n\ndef evaluate(self, model, dataloader, postprocess=None, \\\n                 metric=None, measurer=None, iteration=-1, tensorboard=False):\n# The tensorboard summary is collected in the evaluation function of adaptor\n\n    if tensorboard:\n         model = self._pre_eval_hook(model) \n    #evaluation code\n    ....\n    acc = metric.result()     \n    if tensorboard: \n         self._post_eval_hook(model, accuracy=acc, input=input) \n\ndef _pre_eval_hook(self, model):\n# Insert observer submodule into each module in whitelist in order to collect tensor information\n\n   class _RecordingObserver(ABC, torch.nn.Module):\n   # Define the Observer class \n\n        def forward(self, x):\n        # Record the tensor information in a dict structure\n            self.output_tensors_dict[self.current_iter] = x.to(\"cpu\") \n\n        @torch.jit.export\n        def get_tensor_value(self):\n            return self.output_tensors_dict\n\n   def _observer_forward_hook(module, input, output):\n        #Forward hook that calls observer on the output\n        return module.activation_post_process(output)\n\n   def _add_observer_(module, op_list=None, prefix=\"\"): \n\n        #Add observer for each child module\n        for name, child in module.named_children():\n            _add_observer_(child, op_list, op_name)\n\n        if module is a leaf:\n           module.add_module(\n                    'activation_post_process',\n                    module.qconfig.activation())\n                module.register_forward_hook(_observer_forward_hook)\n\ndef _post_eval_hook(self, model, **args):\n   # Dump tensor and graph information with TensorBoard summary writer\n    if self.dump_times == 0:\n       writer = SummaryWriter('runs/eval/baseline' +\n                             '_acc' + str(accuracy), model)\n    else:\n       writer = SummaryWriter('runs/eval/tune_' +\n                                  str(self.dump_times) +\n                                  '_acc' + str(accuracy), model)\n\n    if args is not None and 'input' in args and self.dump_times == 0:\n       writer.add_graph(model, args['input'])\n\n    from torch.quantization import get_observer_dict\n    get_observer_dict(model, observer_dict)\n    for key in observer_dict:\n        ......\n        op_name = key.strip(\".activation_post_process\")\n        summary[op_name + \".output\"] = observer_dict[key].get_tensor_value()\n        \n        for iter in summary[op_name + \".output\"]:\n            #Record output tensor, for fused op only record the parent op output \n            ......\n            if summary[op_name + \".output\"][iter].is_quantized:\n                  writer.add_histogram(\n                        op + \"/Output/int8\",\n                        torch.dequantize(summary[op_name +\n                                                 \".output\"][iter]))\n            else:\n                  writer.add_histogram(\n                        op + \"/Output/fp32\",\n                        summary[op_name + \".output\"][iter])\n\n        state_dict = model.state_dict()\n        for key in state_dict:\n            # Record weight tensor, fused child tensorBoard tag will be merge \n            if state_dict[key].is_quantized:\n                writer.add_histogram(op + \"/int8\",\n                                     torch.dequantize(state_dict[key]))\n            else:\n                writer.add_histogram(op + \"/fp32\", state_dict[key])\n      \n```", "doc_id": 170},
{"doc": "# Intel Neural Compressor TensorBoard\n#### Usage\n\n1. Add \"tensorboard: true\" in the yaml file.\n2. Run quantization tuning; a \"./runs\" folder is generated in the working folder.\n3. Start tensorboard:\n\n   ``shell\n     tensorboard --bind_all --logdir_spec baseline:./runs/eval/tune_0_acc0.80,tune_1:././runs/eval/tune_1_acc0.79  \n   ``", "doc_id": 171},
{"doc": "# Intel Neural Compressor TensorBoard\n### TensorFlow Tensorboard\n\nTensorFlow TensorBoard implementation includes four steps:\n\n1. Before evaluation where we create the TensorBoard summary write and write graph, collect FP32 and node names for inspection, and dump the histogram of weights and bias tensor directly from graph_def.\n2. Run get_tensor_by_name_with_import() where we get data output tensors.\n3. Run session.run() to predict and get the inference result of the output tensor list collected in the previous step.\n4. Enumerate the output tensor and write the histogram.   \n\nSee the [tensorflow.py](https://github.com/intel/neural-compressor/tree/master/neural_compressor/adaptor/tensorflow.py) evaluate() function for details.", "doc_id": 172},
{"doc": "# Intel Neural Compressor TensorBoard\n#### Usage\n\n1. Add \"tensorboard: true\" in the yaml file.\n\n2. Run quantization tuning; a \"./runs\" folder is generated in the working folder. For example: \n\n   ```shell\n   ls ./runs/eval  \n   baseline_acc_0.776  tune_1_acc_0.095 \n   ```\n   The baseline_acc_0.776 folder contains the FP32 event log and 0.776 is the FP32 accuracy. tune_1_acc_0.095 contains the evaluation event log of the first run of tuning.  \n\n3. Start tensorboard:\n\n   ```shell\n   tensorboard --bind_all --logdir_spec baseline:./runs_v3/eval/baseline_acc_0.776/,tune_1:./runs_v3/eval/tune_1_acc_0.095/", "doc_id": 173},
{"doc": "# Intel Neural Compressor TensorBoard\n## Examples\n### PyTorch Examples\n\n```shell\n  examples/pytorch/eager/image_recognition/imagenet/cpu/ptq/run_tuning_dump_tensor.sh \n```", "doc_id": 174},
{"doc": "# Intel Neural Compressor TensorBoard\n## Examples\n### TensorFlow Examples\n1. Add \"tensorboard: true\" into examples/tensorflow/image_recognition/inceptionv3.yaml. In order to demonstrate the usage of TensorBoard, remove the following lines which are added to skip the quantization of 'v0/cg/conv0/conv2d/Conv2D' to avoid a known limitation.\n   ```yaml\n      op_wise: {\n               'v0/cg/conv0/conv2d/Conv2D': {\n                 'activation':  {'dtype': ['fp32']},\n               }\n             }\n   ```\n2. Run tuning:\n   ```shell\n   bash run_tuning.sh --topology=inception_v3 --dataset_location=<imagenet> \\\n            --input_model=./inceptionv3_fp32_pretrained_model.pb --output_model=./nc_inceptionv3.pb  --config=./inceptionv3_dump_tensor.yaml \n   ```\n3. Start TensorBoard\n   ```shell\n   tensorboard --bind_all --logdir_spec baseline:./runs_v3/eval/baseline_acc_0.776/,tune_1:./runs_v3/eval/tune_1_acc_0.095/\n   ```\n4. In order to find the reason why tune_1 got such poor accuracy, we can observe the TensorBoard.", "doc_id": 175},
{"doc": "# Intel Neural Compressor Tuning Strategies\n\n1. [Introduction](#introduction)\n\n2. [Strategy Design](#strategy-design)\n\n    2.1. [Tuning Space](#tuning-space)\n\n\t2.2. [Exit Policy](#exit-policy)\n\n\t2.3. [Accuracy Criteria](#accuracy-criteria)\n\n    2.4. [Tuning Process](#tuning-process)\n\n3. [Tuning Algorithms](#tuning-algorithms)\n\n    3.1. [Auto](#auto)\n\n    3.2. [Conservative Tuning](#conservative-tuning)\n\n    3.3. [Basic](#basic)\n\n    3.4. [MSE](#mse)\n\n    3.5. [MSE_V2](#mse_v2)\n\n    3.6. [HAWQ_V2](#hawq_v2)\n\n    3.7. [Bayesian](#bayesian)\n\n    3.8. [Exhaustive](#exhaustive)\n\n    3.9. [Random](#random)\n\n    3.10. [SigOpt](#sigopt)\n\n    3.11. [TPE](#tpe)\n\n 4. [Distributed Tuning](#distributed-tuning)\n\n 5. [Customize a New Tuning Strategy](#customize-a-new-tuning-strategy)", "doc_id": 176},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Introduction\n\nIntel\u00ae Neural Compressor aims to help users quickly deploy\nthe low-precision inference solution on popular Deep Learning frameworks such as TensorFlow, PyTorch, ONNX, and MXNet. With built-in strategies, it automatically optimizes low-precision recipes for deep learning models to achieve optimal product objectives, such as inference performance and memory usage, with expected accuracy criteria. Currently, several strategies, including `O0`, `Basic`, `MSE`, `MSE_V2`, `HAWQ_V2`, `Bayesian`, `Exhaustive`, `Random`, `SigOpt`, `TPE`, etc are supported. By default, the `Basic` strategy is used for tuning.", "doc_id": 177},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Strategy Design\nBefore tuning, the `tuning space` was constructed according to the framework capability and user configuration. Then the selected strategy generates the next quantization configuration according to its traverse process and the previous tuning record. The tuning process stops when meeting the exit policy. The function of strategies is shown\nbelow:\n\n![Tuning Strategy](./imgs/strategy.png \"Strategy Framework\")", "doc_id": 178},
{"doc": "# Intel Neural Compressor Tuning Strategies\n### Tuning Space\n\nIntel\u00ae Neural Compressor supports multiple quantization modes such as Post Training Static Quantization (PTQ static), Post Training Dynamic Quantization (PTQ dynamic), Quantization Aware Training, etc. One operator (OP) with a specific quantization mode has multiple ways to quantize, for example it may have multiple quantization scheme(symmetric/asymmetric), calibration algorithm(Min-Max/KL Divergence), etc. We use the `framework capability` to represent the methods that we have already supported. The `tuning space` includes all tuning items and their options. For example, the tuning items and options of the `Conv2D` (PyTorch) supported by Intel\u00ae Neural Compressor are as follows:\n![Conv2D_PyTorch_Cap](./imgs/Conv2D_PyTorch_Cap.png \"Conv2D PyTorch Capability\")\n\nTo incorporate the human experience and reduce the tuning time, user can reduce the tuning space by specifying the `op_name_dict` and `op_type_dict` in `PostTrainingQuantConfig` (`QuantizationAwareTrainingConfig`). Before tuning, the strategy will merge these configurations with framework capability to create the final tuning space.", "doc_id": 179},
{"doc": "# Intel Neural Compressor Tuning Strategies\n### Exit Policy\nUser can control the tuning process by setting the exit policy by specifying the `timeout`, and `max_trials` fields in the `TuningCriterion`.\n\n```python\nfrom neural_compressor.config import TuningCriterion\n\ntuning_criterion=TuningCriterion(\n    timeout=0, # optional. tuning timeout (seconds). When set to 0, early stopping is enabled.\n    max_trials=100, # optional. max tuning times. combined with the `timeout` field to decide when to exit tuning.\n    strategy=\"basic\", # optional. name of the tuning strategy. \n    strategy_kwargs=None, # optional. see concrete tuning strategy for available settings.\n)\n```", "doc_id": 180},
{"doc": "# Intel Neural Compressor Tuning Strategies\n### Accuracy Criteria\nUser can set the accuracy criteria by specifying the `higher_is_better`, `criterion`, and `tolerable_loss` fields in the `AccuracyCriterion`.\n\n``` python\nfrom neural_compressor.config import AccuracyCriterion\n\naccuracy_criterion = AccuracyCriterion(\n    higher_is_better=True,  # optional. \n    criterion='relative',  # optional. Available values are 'relative' and 'absolute'.\n    tolerable_loss=0.01,  # optional.\n)\n```", "doc_id": 181},
{"doc": "# Intel Neural Compressor Tuning Strategies\n### Tuning Process \n\nIntel\u00ae Neural Compressor allows users to choose different tuning processes by specifying the quantization level (`quant_level`). Currently, the recognized `quant_level`s are `0`, `1`, and `\"auto\"`. For `quant_level` is `1`, the tuning process can be finer-grained controlled by setting the `strategy` field.\n\n- `0`: \"Conservative\" tuning. `0` starts with an `fp32` model and tries to quantize OPs into lower precision by **op-type-wise**. `0` can be useful to give users insights about the accuracy degradation after quantizing some OPs.\n\n- `1`: \"Aggressive\" tuning. `1` starts with the default quantization configuration and selects different quantization parameters. `1` can be used to achieve the performance. \n\n- `\"auto\"` (default) Auto tuning. `\"auto\"` combines the advantages of `quant_level=0` and `quant_level=1`. Currently, it tries default quantization configuration, `0`, and [`basic`](./tuning_strategies.md#basic) strategy sequentially.", "doc_id": 182},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n### Auto\n#### Design\nThe auto tuning (`quant_level`=`\"auto\"`) is the default tuning process. Classical settings are shown below:\n#### Usage\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    quant_level=\"auto\",  # optional, the quantization level.\n    tuning_criterion=TuningCriterion(\n        timeout=0,  # optional. tuning timeout (seconds). When set to 0, early stopping is enabled.\n        max_trials=100,  # optional. max tuning times. combined with the `timeout` field to decide when to exit tuning.\n    ),\n)\n```", "doc_id": 183},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n### Conservative Tuning\n\n#### Design\nThe conservative tuning (`quant_level`=`0`) starts with an `fp32` model and tries to convert key OPs like `conv`, `matmul`, or `linear` into lower precision **op-type-wise**.\n#### Usage\n\nTo use conservative tuning, the `quant_level` field should be set to `0` in `PostTrainingQuantConfig`.\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    quant_level=0,  # the quantization level.\n    tuning_criterion=TuningCriterion(\n        timeout=0,  # optional. tuning timeout (seconds). When set to 0, early stopping is enabled.\n        max_trials=100,  # optional. max tuning times. combined with the `timeout` field to decide when to exit tuning.\n    ),\n)\n```", "doc_id": 184},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n## Basic\n### Design\n\nThe `Basic` strategy is designed for quantizing most models. There are six stages executed by `Basic` strategy sequentially, and the tuning process ends once the condition meets the exit policy. \n- **Stage I**. Quantize with default quantization configuration\n    At this stage, it tries to quantize OPs with the default quantization configuration which is consistent with the framework behavior.\n- **Stage II**. Apply all recipes\n    At this stage, it tries to apply all recipes.\n- **Stage III**. OP-Type-Wise Tuning\n    At this stage, it tries to quantize OPs as many as possible and traverse all OP type wise tuning configs. Note that, the OP is initialized with different quantization modes according to the quantization approach.\n    a. `post_training_static_quant`: Quantize all OPs support PTQ static.\n    b. `post_training_dynamic_quant`: Quantize all OPs support PTQ dynamic.\n    c. `post_training_auto_quant`: Quantize all OPs support PTQ static or PTQ dynamic. For OPs supporting both PTQ static and PTQ dynamic, PTQ static will be tried first, and PTQ dynamic will be tried when none of the OP type wise tuning configs meet the accuracy loss criteria.\n- **Stage IV**. Try recipe One by One\n    At this stage, it tries recipe one by one based on the tuning config with the best result in the previous stage.\n- **Stage V**. Fallback OP One by One\n    At this stage, it performs high-precision OP (FP32, BF16 ...) fallbacks one by one based on the tuning config with the best result in the previous stage, and records the impact of each OP. \n- **Stage VI**. Fallback Multiple OPs Accumulated\n    At the final stage, it first sorted the OPs list according to the impact score in stage V, and tries to incrementally fallback multiple OPs to high precision according to the sorted OP list.\n### Usage\n`Basic` is the default strategy for `quant_level`=`1`, it can be used by default with nothing changed in the `strategy` field of `TuningCriterion` after set the `quant_level`=`1` in `PostTrainingQuantConfig`. Classical settings are shown below:\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    quant_level=1,\n    tuning_criterion=TuningCriterion(\n        strategy=\"basic\"  # optional. name of tuning strategy. \n    ),\n)\n\n```", "doc_id": 185},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n### MSE\n\n#### Design\n\n`MSE` and `Basic` strategies share similar ideas. The primary difference\nbetween the two strategies is the way sorted op lists generated in stage II. The `MSE` strategy needs to get the tensors for each OP of raw FP32\nmodels and the quantized model based on the best model-wise tuning\nconfiguration. It then calculates the MSE (Mean Squared Error) for each\nOP, sorts those OPs according to the MSE value, and performs\nthe op-wise fallback in this order.\n\n#### Usage\n\nThe usage of `MSE` is similar to `Basic`. To use `MSE` strategy, the `strategy` field of the `TuningCriterion` should be specified with `mse`.\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    quant_level=1,\n    tuning_criterion=TuningCriterion(\n        strategy=\"mse\" \n    ),\n)\n\n```", "doc_id": 186},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n### MSE_V2\n\n#### Design\n\n`MSE_v2` is a strategy with a two stages fallback and revert fallback. In the fallback stage, it uses multi-batch data to score the op impact and then fallback the op with the highest score util found the quantized model meets accuracy criteria. In the revert fallback stage, it also scores the impact of fallback OPs in the previous stage and selects the op with the lowest score to revert the fallback until the quantized model not meets accuracy criteria.\n\n#### Usage\nTo use the `MSE_V2` tuning strategy, the `strategy` field in the `TuningCriterion` should be specified with `mse_v2`. Also, the `confidence_batches` can be specified optionally inside the `strategy_kwargs` for the number of batches to score the op impact. Increasing `confidence_batches` will generally improve the accuracy of the scoring with more time spent in tuning process.\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    quant_level=1,\n    tuning_criterion=TuningCriterion(\n        strategy=\"mse_v2\",\n        strategy_kwargs={\"confidence_batches\": 2}  # optional. the number of batches to score the op impact.\n    ),\n)\n```", "doc_id": 187},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n### HAWQ_V2\n\n#### Design\n`HAWQ_V2` implements the [Hessian Aware trace-Weighted Quantization of Neural Networks](https://arxiv.org/abs/1911.03852). We made a small change to it by using the hessian trace to score the op impact and then fallback the OPs according to the scoring result.\n\n#### Usage\nTo use the `HAWQ_V2` tuning strategy, the `strategy` field in the `TuningCriterion` should be specified with `hawq_v2`, and the loss function for calculating the hessian trace of model should be provided. The loss function should be set in the field of `hawq_v2_loss` in the `strategy_kwargs`.\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion", "doc_id": 188},
{"doc": "def model_loss(output, target, criterion):\n    return criterion(output, target)", "doc_id": 189},
{"doc": "conf = PostTrainingQuantConfig(\n    quant_level=1,\n    tuning_criterion=TuningCriterion(\n        strategy=\"hawq_v2\",\n        strategy_kwargs={\"hawq_v2_loss\": model_loss}  # required. the loss function for calculating the hessian trace.\n    ),\n)\n```", "doc_id": 190},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n### Bayesian\n\n#### Design\n\n`Bayesian` optimization is a sequential design strategy for the global\noptimization of black-box functions. This strategy comes from the [Bayesian\noptimization](https://github.com/fmfn/BayesianOptimization) package and\nchanged it to a discrete version that complied with the strategy standard of\nIntel\u00ae Neural Compressor. It uses [Gaussian processes](https://en.wikipedia.org/wiki/Neural_network_Gaussian_process) to define\nthe prior/posterior distribution over the black-box function with the tuning\nhistory and then finds the tuning configuration that maximizes the expected\nimprovement. For now, `Bayesian` just focus on op-wise quantization configs tuning \nwithout the fallback phase. In order to obtain a quantized model with good accuracy \nand better performance in a short time. We don't add datatype as a tuning \nparameter into `Bayesian`.\n\n#### Usage\n\nFor the `Bayesian` strategy, it is recommended to set `timeout` or `max_trials` to a non-zero\nvalue as shown in below example, because the param space for `bayesian` can be very small and the accuracy goal might not be reached, which can make the tuning end never. Additionally, if the log level is set to `debug` by `LOGLEVEL=DEBUG` in the environment variable, the message `[DEBUG] Tuning config was evaluated, skip!` will print endlessly. If the `timeout` is changed from 0 to an integer, `Bayesian` ends after the timeout is reached.\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    quant_level=1,\n    tuning_criterion=TuningCriterion(\n        timeout=0,  # optional. tuning timeout (seconds). When set to 0, early stopping is enabled.\n        max_trials=100,  # optional. max tuning times. combined with the `timeout` field to decide when to exit tuning.\n        strategy=\"bayesian\"\n    ),\n)\n\n```", "doc_id": 191},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n### Exhaustive\n\n#### Design\n\nThe `Exhaustive` strategy is used to sequentially traverse all possible tuning\nconfigurations in a tuning space. From the perspective of the impact on\nperformance, we currently only traverse all possible quantization tuning\nconfigs. Same reason as `Bayesian`, fallback datatypes are not included for now.\n\n#### Usage\n\n`Exhaustive` usage is similar to `basic`, with `exhaustive` specified to `strategy` field in the `TuningCriterion`.", "doc_id": 192},
{"doc": "```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    quant_level=1,\n    tuning_criterion=TuningCriterion(\n        strategy=\"exhaustive\",\n    ),\n)\n```", "doc_id": 193},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n### Random\n\n#### Design\n\n`Random` strategy is used to randomly choose tuning configurations from the\ntuning space. As with the `Exhaustive` strategy, it also only considers quantization\ntuning configs to generate a better-performance quantized model.\n\n#### Usage\n\n`Random` usage is similar to `basic`, with `random` specified to `strategy` field in the `TuningCriterion`.\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    quant_level=1,\n    tuning_criterion=TuningCriterion(\n        strategy=\"random\",\n    ),\n)\n```", "doc_id": 194},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n### SigOpt\n\n#### Design\n\n`SigOpt` strategy is to use [SigOpt Optimization Loop](https://app.sigopt.com/docs/overview/optimization) method to accelerate and visualize the traversal of the tuning configurations from the tuning space. The metrics add accuracy as a constraint and optimize for latency to improve performance. [SigOpt Projects](https://app.sigopt.com/) can show the result of each tuning experiment.\n\n#### Usage\n\nCompared to `Basic`, `sigopt_api_token` and `sigopt_project_id` are necessary for `SigOpt`.\nFor details, [how to use sigopt strategy in neural_compressor](./sigopt_strategy.md) is available.\n\nNote that the `sigopt_api_token`, `sigopt_project_id`, and `sigopt_experiment_name` should be set inside the `strategy_kwargs`.\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    quant_level=1,\n    tuning_criterion=TuningCriterion(\n        strategy=\"sigopt\",\n        strategy_kwargs={\n            \"sigopt_api_token\": \"YOUR-ACCOUNT-API-TOKEN\",\n            \"sigopt_project_id\": \"PROJECT-ID\",\n            \"sigopt_experiment_name\": \"nc-tune\",\n        },\n    ),\n)\n```", "doc_id": 195},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Tuning Algorithms\n### TPE\n\n#### Design\n\n`TPE` uses sequential model-based optimization methods (SMBOs). **Sequential** refers to running trials one after another and selecting a better\n**hyperparameter** to evaluate based on previous trials. A hyperparameter is\na parameter whose value is set before the learning process begins; it\ncontrols the learning process. SMBO apples Bayesian reasoning in that it\nupdates a **surrogate** model that represents an **objective** function\n(objective functions are more expensive to compute). Specifically, it finds\nhyperparameters that perform best on the surrogate and then applies them to\nthe objective function. The process is repeated and the surrogate is updated\nwith incorporated new results until the timeout or max trials is reached.\n\nA surrogate model and selection criteria can be built in a variety of ways.\n`TPE` builds a surrogate model by applying Bayesian reasoning. The TPE\nalgorithm consists of the following steps:\n\n- Define a domain of hyperparameter search space.\n- Create an objective function that takes in hyperparameters and outputs a\nscore (e.g., loss, RMSE, cross-entropy) that we want to minimize.\n- Collect a few observations (score) using a randomly selected set of\nhyperparameters.\n- Sort the collected observations by score and divide them into two groups\nbased on some quantile. The first group (x1) contains observations that\ngive the best scores and the second one (x2) contains all other\nobservations.\n- Model the two densities l(x1) and g(x2) using Parzen Estimators (also known as kernel density estimators), which are a simple average of kernels centered on existing data points.\n- Draw sample hyperparameters from l(x1). Evaluate them in terms of l(x1)/g(x2), and return the set that yields the minimum value under l(x1)/g(x1) that\ncorresponds to the greatest expected improvement. Evaluate these\nhyperparameters on the objective function.\n- Update the observation list in step 3.\n8. Repeat steps 4-7 with a fixed number of trials.\n\n>Note: TPE requires many iterations in order to reach an optimal solution.\nIt is recommended to run at least 200 iterations, because every iteration\nrequires evaluation of a generated model, which means accuracy measurements\non a dataset and latency measurements using a benchmark. This process may\ntake from 24 hours to a few days to complete, depending on the model.\n\n#### Usage\n\n`TPE` usage is similar to `basic` with `tpe` specified to `strategy` field in the `TuningCriterion`.\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig, TuningCriterion\n\nconf = PostTrainingQuantConfig(\n    quant_level=1,\n    tuning_criterion=TuningCriterion(\n        strategy=\"tpe\"\n    )\n)\n```\n\nThe `next_tune_cfg` function is used to yield the next tune configuration according to some algorithm or strategy. `TuneStrategy` base class will traverse all the tuning space till a quantization configuration meets the pre-defined accuracy criterion.\n\nThe `traverse` function can be overridden optionally if the traverse process required by the new strategy is different from the one `TuneStrategy` base class implemented.\n\nAn example of customizing a new tuning strategy can be reached at [TPE Strategy](../../neural_compressor/contrib/strategy/tpe.py).", "doc_id": 196},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Distributed Tuning\n\n### Design\n\nIntel\u00ae Neural Compressor provides distributed tuning to speed up the tuning process by leveraging the multi-node cluster. It seamlessly parallelizes the tuning process across multi nodes by using the MPI. In distributed tuning, the `fp32` model is replicated on every node, and each original model replica is fed with a different quantization configuration. The master handler coordinates the tuning process and synchronizes the tuning result of each stage to every slave handler. The distributed tuning allows the tuning process to scale up significantly to the number of nodes, which translates into faster results and more efficient utilization of computing resources.", "doc_id": 197},
{"doc": "### Usage\n\nTo use Distributed Tuning, the `use_distributed_tuning` field in the `PostTrainingQuantConfig` should be specified with `True`.\n\n```python\nfrom neural_compressor.config import PostTrainingQuantConfig\n\nconf = PostTrainingQuantConfig(use_distributed_tuning=True)\n```\nAn example of distributed tuning can be reached at [ptq_static_mrpc](../../examples/pytorch/nlp/huggingface_models/text-classification/quantization/ptq_static/fx).", "doc_id": 198},
{"doc": "# Intel Neural Compressor Tuning Strategies\n## Customize a New Tuning Strategy\n\nIntel\u00ae Neural Compressor supports new strategy extension by implementing a sub-class of the `TuneStrategy` class in neural_compressor.strategy package and registering it by the `strategy_registry` decorator.\n\nFor example, user can implement an `Abc` strategy like below:\n\n```python\n@strategy_registry\nclass AbcTuneStrategy(TuneStrategy):\n    def __init__(self, model, conf, q_dataloader, q_func=None,\n                 eval_dataloader=None, eval_func=None, dicts=None):\n        ...\n\n    def next_tune_cfg(self):\n        # generate the next tuning config\n        ...\n    \n    def traverse(self):\n        for tune_cfg in self.next_tune_cfg():\n            # do quantization\n            ...\n\n```", "doc_id": 199},
{"doc": "# Intel Neural Compressor User YAML Configuration Files\n## Introduction\n\nIntel\u00ae Neural Compressor uses YAML files for quick \nand user-friendly configurations. There are two types of YAML files - \nuser YAML files and framework YAML files, which are used in \nrunning user cases and setting up framework capabilities, respectively.\n\nFirst, let's take a look at a user YAML file, It defines the model, tuning\nstrategies, tuning calibrations and evaluations, and performance benchmarking\nof the passing model vs. original model.", "doc_id": 200},
{"doc": "# Intel Neural Compressor User YAML Configuration Files\n## Supported Feature Matrix\n\n| Optimization Techniques | YAML Configuration Files |\n|-------------------------|:------------------------:|\n| Quantization            |         &#10004;         |\n| Pruning                 |         &#10004;         |\n| Distillation            |         &#10004;         |", "doc_id": 201},
{"doc": "# Intel Neural Compressor User YAML Configuration Files\n## Get started with User YAML Files\nA complete user YAML file is organized logically into several sections: \n* ***model***: The model specifications define a user model's name, inputs, outputs and framework.\n\n```yaml\nmodel:                                               # mandatory. used to specify model specific information.\n  name: mobilenet_v1 \n  framework: tensorflow                              # mandatory. supported values are tensorflow, pytorch, pytorch_ipex, onnxrt_integer, onnxrt_qlinear or mxnet; allow new framework backend extension.\n  inputs: image_tensor                               # optional. inputs field is only required in tensorflow.\n  outputs: num_detections,detection_boxes,detection_scores,detection_classes # optional. outputs field is only required in tensorflow.\n```\n* ***quantization***: The quantization specifications define quantization tuning space and related calibrations. To calibrate, users can \nspecify *sampling_size* (optional) and use the subsection *dataloader* to specify\nthe dataset location using *root* and transformation using *transform*. To \nimplement tuning space constraints, users can use the subsection *model_wise* and *op_wise* for specific configurations.\n \n```yaml\nquantization:                                        # optional. tuning constraints on model-wise for advance user to reduce tuning space.\n  calibration:\n    sampling_size: 20                                # optional. default value is 100. used to set how many samples should be used in calibration.\n    dataloader:\n      dataset:\n        ImageRecord:\n          root: /path/to/imagenet/                   # NOTE: modify to calibration dataset location if needed\n      transform:\n        BilinearImagenet: \n          height: 224\n          width: 224\n  model_wise:                                        # optional. tuning constraints on model-wise for advance user to reduce tuning space.\n    weight:\n      granularity: per_channel\n      scheme: asym\n      dtype: int8\n      algorithm: minmax\n    activation:\n      granularity: per_tensor\n      scheme: asym\n      dtype: int8, fp32\n      algorithm: minmax, kl\n  op_wise: {                                         # optional. tuning constraints on op-wise for advance user to reduce tuning space. \n         'conv1': {\n           'activation':  {'dtype': ['uint8', 'fp32'], \n                           'algorithm': ['minmax', 'kl'], \n                           'scheme':['sym']},\n           'weight': {'dtype': ['int8', 'fp32'], \n                      'algorithm': ['minmax']}\n         }\n       }\n```\n\n* ***pruning***: The pruning specifications define pruning tuning space. To define the training behavior, uses can \nuse the subsection *train* to specify the training hyper-parameters and the training dataloader. \nTo define the pruning approach, users can use the subsection *approach* to specify \npruning target, choose the type of pruning algorithm, and the way to apply it \nduring training process. \n\n```yaml\npruning:\n  train:\n    dataloader:\n      ... \n    epoch: 40\n    optimizer:\n      Adam:\n        learning_rate: 1e-06\n        beta_1: 0.9\n        beta_2: 0.999\n        epsilon: 1e-07\n    criterion:\n      SparseCategoricalCrossentropy:\n        reduction: sum_over_batch_size\n        from_logits: False\n  approach:\n    weight_compression:\n      initial_sparsity: 0.0\n      target_sparsity: 0.54\n      start_epoch: 0\n      end_epoch: 19\n      pruners:\n        - !Pruner\n            start_epoch: 0\n            end_epoch: 19\n            prune_type: basic_magnitude\n```\n* ***distillation***: The distillation specifications define distillation's tuning\nspace. Similar to pruning, to define the training behavior, users can use the \nsubsection *train* to specify the training hyper-parameters and the training \ndataloader and it is optional if users implement *train_func* and set the attribute\nof distillation instance to *train_func*. For criterion, Intel\u00ae Neural Compressor provides a built-in \nknowledge distillation loss class to calculate distillation loss.\n```yaml\ndistillation:\n  train:\n    start_epoch: 0\n    end_epoch: 90\n    iteration: 1000\n    frequency: 1\n    dataloader:\n      ...\n    optimizer:\n      SGD:\n        learning_rate: 0.001  \n        momentum: 0.1\n        nesterov: True\n        weight_decay: 0.001\n    criterion:\n      KnowledgeDistillationLoss:\n        temperature: 1.0\n        loss_types: ['CE', 'CE']\n        loss_weights: [0.5, 0.5]\n```\n* ***evaluation***: The evaluation specifications define the dataloader and metric for accuracy evaluation as well as dataloader \nand configurations for performance benchmarking. \n```yaml\nevaluation:                                          # optional. required if user doesn't provide eval_func in neural_compressor.Quantization.\n  accuracy:                                          \n    metric:\n      ...\n    dataloader:\n      ...\n```\n* ***tuning***: The tuning specifications define overall tuning targets. Users can\nuse *accuracy_criterion* to specify the target of accuracy loss percentage and use\n*exit_policy* to specify the tuning timeout in seconds. The random\nseed can be specified using *random_seed*. \n\n```yaml\ntuning:\n  accuracy_criterion:\n    relative: 0.01                                  # the tuning target of accuracy loss percentage: 1%\n    higher_is_better: True\n  exit_policy:\n    timeout: 0                                      # tuning timeout (seconds), 0 means early stop\n  random_seed: 9527                                 # random seed\n```", "doc_id": 202}]
