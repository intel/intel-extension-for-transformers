[run]
branch = True

[report]
omit =
 */**/fake*yaml
 */**/fake.py
 */intel_extension_for_transformers/transformers/llm/amp/**
 */intel_extension_for_transformers/transformers/llm/finetuning/**
 */intel_extension_for_transformers/transformers/llm/operator/**
 */intel_extension_for_transformers/transformers/llm/inference/**
 */intel_extension_for_transformers/transformers/llm/library/**
 */intel_extension_for_transformers/transformers/modeling/**
 */intel_extension_for_transformers/transformers/runtime/**
 */intel_extension_for_transformers/transformers/setfit/**
 */intel_extension_for_transformers/neural_chat/**
 */intel_extension_for_transformers/haystack/**
 */intel_extension_for_transformers/langchain/**
 */intel_extension_for_transformers/llama_index/**
 */intel_extension_for_transformers/transformers/utils/get_throughput.py
 */intel_extension_for_transformers/transformers/kv_cache_compression/models/**
exclude_lines =
 pragma: no cover
 raise NotImplementedError
 raise TypeError
 if self.device == "gpu":
 if device == "gpu":
 except ImportError:
 except Exception as e:
 onnx_version < ONNX18_VERSION
 onnx_version >= ONNX18_VERSION
