name: Chatbot finetune on mosaicml/mpt-7b-chat with hpu

on:
  workflow_call:

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}-ft-mpt-7b-hpu
  cancel-in-progress: true

jobs:
  finetuning:
    name: finetuning test
    runs-on: guadi2-4
    steps:
      - name: Clean Up Working Directory
        run: sudo rm -rf ~/itrex-actions-runner/_work/intel-extension-for-transformers/intel-extension-for-transformers/*
        
      - uses: actions/checkout@v3
        with:
          submodules: "recursive"

      - name: Load environment variables
        run:
          cat ~/itrex-actions-runner/.env >> $GITHUB_ENV

      - name: Build Docker Image
        run:
          docker build --no-cache ./ --target hpu --build-arg ITREX_VER=${{ github.event.pull_request.head.ref }} --build-arg REPO=${{ github.server_url }}/${{ github.event.pull_request.head.repo.full_name }}.git --build-arg REPO_PATH="." --build-arg http_proxy="${{ env.HTTP_PROXY_IMAGE_BUILD }}" --build-arg https_proxy="${{ env.HTTPS_PROXY_IMAGE_BUILD }}" -f intel_extension_for_transformers/neural_chat/docker/Dockerfile -t chatbotfinetune-hpu:latest && yes | docker container prune && yes | docker image prune

      - name: Start Docker Container
        id: master_container
        run: |
          cid=$(docker ps -q --filter "name=chatbotfinetune-hpu-s0")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi
          docker run -tid --runtime=habana -v /mnt/DP_disk1/huggingface/cache/:/root/.cache/huggingface/hub -e http_proxy="${{ env.HTTP_PROXY_CONTAINER_RUN }}" -e https_proxy="${{ env.HTTPS_PROXY_CONTAINER_RUN }}" --name="chatbotfinetune-hpu-s0" --hostname="chatbotfinetune-container-mpi-s0" chatbotfinetune-hpu:latest

      - name: Run Finetuning
        run: |
          cmd="python3 /intel-extension-for-transformers/workflows/chatbot/fine_tuning/instruction_tuning_pipeline/finetune_clm.py \
            --model_name_or_path mosaicml/mpt-7b-chat \
            --train_file /intel-extension-for-transformers/.github/workflows/sample_data/alpaca_data_sample_45.json \
            --bf16 True \
            --output_dir ./mpt_peft_finetuned_model \
            --num_train_epochs 3 \
            --per_device_train_batch_size 4 \
            --per_device_eval_batch_size 4 \
            --gradient_accumulation_steps 1 \
            --save_strategy \"epoch\" \
            --save_steps 2000 \
            --save_total_limit 1 \
            --learning_rate 1e-4  \
            --logging_steps 10 \
            --peft lora \
            --dataset_concatenation \
            --do_train \
            --tokenizer_name \"EleutherAI/gpt-neox-20b\" \
            --use_fast_tokenizer True \
            --max_eval_samples 64 \
            --device hpu \
            --use_habana \
            --use_lazy_mode "
          
          docker exec "chatbotfinetune-hpu-s0" bash -c "$cmd"       

      - name: Stop Container
        if: success() || failure()
        run: |
          cid=$(docker ps -q --filter "name=chatbotfinetune-hpu-s0")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi

      - name: Test Summary
        run: echo "Finetuning completed successfully"
