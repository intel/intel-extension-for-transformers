name: Chatbot inference on mosaicml/mpt-7b-chat

on:
  workflow_call:

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}-inf-mpt-7b
  cancel-in-progress: true

jobs:
  inference:
    name: inference test
    runs-on: neural-chat-inference
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Load environment variables
        run: 
          cat ~/actions-runner/.env >> $GITHUB_ENV

      - name: Build Docker Image
        run: 
          if [ $(docker images | grep chatbotinfer-gha | wc -l) == 0 ]; then
            docker build  --no-cache ./ --target cpu --build-arg REPO=${{ github.server_url }}/${{ github.repository }}.git --build-arg ITREX_VER=${{ github.head_ref }} --build-arg REPO_PATH="." --build-arg http_proxy="${{ env.HTTP_PROXY_IMAGE_BUILD }}" --build-arg https_proxy="${{ env.HTTPS_PROXY_IMAGE_BUILD }}" -f intel_extension_for_transformers/neural_chat/docker/Dockerfile -t chatbotinfer-gha:latest && yes | docker container prune && yes | docker image prune;
          fi

      - name: Start Docker Container
        run: |
          cid=$(docker ps -q --filter "name=chatbotinfer-gha")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi
          docker run -tid -v /home/sdp/.cache/huggingface/hub:/root/.cache/huggingface/hub -e http_proxy="${{ env.HTTP_PROXY_CONTAINER_RUN }}" -e https_proxy="${{ env.HTTPS_PROXY_CONTAINER_RUN }}" --name="chatbotinfer-gha" --hostname="chatbotinfer-gha-container" chatbotinfer-gha:latest

      - name: Run Inference Test
        run: |
          docker exec "chatbotinfer-gha" bash -c "cd /intel-extension-for-transformers && source activate && conda activate neuralchat; \
                 git config --global --add safe.directory '*' && \
                 git submodule update --init --recursive && \
                 pip uninstall intel-extension-for-transformers -y; \
                 pip install -r requirements.txt; \
                 python setup.py install; \
                 pip install -r intel_extension_for_transformers/neural_chat/requirements.txt; \
                 python workflows/chatbot/inference/generate.py --base_model_path \"mosaicml/mpt-7b-chat\" --instructions \"Transform the following sentence into one that shows contrast. The tree is rotten.\" "

      - name: Stop Container
        if: always()
        run: |
          cid=$(docker ps -q --filter "name=chatbotinfer-gha")
          if [[ ! -z "$cid" ]]; then docker stop $cid && docker rm $cid; fi

      - name: Test Summary
        run: echo "Inference completed successfully"
