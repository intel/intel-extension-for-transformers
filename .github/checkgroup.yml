custom_service_name: "CI checker"
subprojects:
  - id: "Format Scan Tests workflow"
    paths:
      - intel_extension_for_transformers/**
      - neural_chat/**
      - workflows/**
      - setup.py
      - .github/workflows/format_scan.yml
      - .github/workflows/script/formatScan/**
    checks:
      - "format-scan (pylint)"
      - "format-scan (bandit)"
      - "format-scan (cloc)"
      - "format-scan (cpplint)"

  - id: "Optimize Unit Test workflow"
    paths:
      - intel_extension_for_transformers/transformers/**
      - requirements.txt
      - setup.py
      - intel_extension_for_transformers/transformers/llm/evaluation/**
      - intel_extension_for_transformers/transformers/llm/quantization/**
      - intel_extension_for_transformers/transformers/llm/operator/**
      - tests/**
      - .github/workflows/unit-test-optimize.yml
      - ".github/workflows/script/unitTest/run_unit_test_optimize.sh"
    checks:
      - "optimize-unit-test-baseline"
      - "optimize-unit-test-PR-test"
      - "Genreate-OptimizeUT-Report"


  # - id: "Windows Binary Test"
  #   paths:
  #     - ".github/workflows/windows-test.yml"
  #     - "requirements.txt"
  #     - "setup.py"
  #     - "intel_extension_for_transformers/transformers/runtime/**"
  #     - "intel_extension_for_transformers/transformers/llm/operator/**"
  #     - "!intel_extension_for_transformers/transformers/runtime/third_party/**"
  #     - "!intel_extension_for_transformers/transformers/runtime/docs/**"
  #     - "!intel_extension_for_transformers/transformers/runtime/test/**"
  #   checks:
  #     - "Windows-Binary-Test"

  # - id: "LLM Model Test workflow"
  #   paths:
  #     - ".github/workflows/llm-test.yml"
  #     - ".github/workflows/script/models/run_llm.sh"
  #     - "intel_extension_for_transformers/transformers/runtime/**"
  #     - "!intel_extension_for_transformers/transformers/runtime/kernels/**"
  #     - "!intel_extension_for_transformers/transformers/runtime/test/**"
  #     - "!intel_extension_for_transformers/transformers/runtime/third_party/**"
  #     - "!intel_extension_for_transformers/transformers/runtime/docs/**"
  #   checks:
  #     - "LLM-Workflow (gpt-j-6b, engine, latency, bf16,int8,fp8)"
  #     - "Generate-LLM-Report"

  # - id: "Chat Bot Test workflow"
  #   paths:
  #     - ".github/workflows/chatbot-test.yml"
  #     - ".github/workflows/chatbot-inference-llama-2-7b-chat-hf.yml"
  #     - ".github/workflows/chatbot-inference-mpt-7b-chat.yml"
  #     - ".github/workflows/chatbot-finetune-mpt-7b-chat.yml"
  #     - ".github/workflows/chatbot-inference-llama-2-7b-chat-hf-hpu.yml"
  #     - ".github/workflows/chatbot-inference-mpt-7b-chat-hpu.yml"
  #     - ".github/workflows/chatbot-finetune-mpt-7b-chat-hpu.yml"
  #     - ".github/workflows/script/chatbot/**"
  #     - ".github/workflows/sample_data/**"
  #     - "intel_extension_for_transformers/neural_chat/**"
  #     - "intel_extension_for_transformers/transformers/llm/finetuning/**"
  #     - "intel_extension_for_transformers/transformers/llm/quantization/**"
  #     - "intel_extension_for_transformers/transformers/**"
  #     - "workflows/chatbot/inference/**"
  #     - "workflows/chatbot/fine_tuning/**"
  #     - "!intel_extension_for_transformers/neural_chat/docs/**"
  #     - "!intel_extension_for_transformers/neural_chat/tests/ci/**"
  #     - "!intel_extension_for_transformers/neural_chat/examples/**"
  #     - "!intel_extension_for_transformers/neural_chat/assets/**"
  #     - "!intel_extension_for_transformers/neural_chat/README.md"
  #   checks:
  #     - "call-inference-llama-2-7b-chat-hf / inference test"
  #     - "call-inference-mpt-7b-chat / inference test"
