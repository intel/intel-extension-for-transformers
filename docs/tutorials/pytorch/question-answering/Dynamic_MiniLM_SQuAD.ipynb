{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ec3098",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e00856",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how to quantize a Dynamic-MiniLM model with both static and dynamic post training quantization based on [IntelÂ® Neural Compressor](https://github.com/intel/neural-compressor) and benchmark the quantized models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbd8bd4",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b20e2b",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1816be1",
   "metadata": {},
   "source": [
    "* Follow [installation](https://github.com/intel/intel-extension-for-transformers#installation) to install **intel-extension-for-transformers**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef103e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install model dependency\n",
    "!pip install accelerate datasets sentencepiece torch wandb torchprofile transformers neural_compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35d9563",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b424903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/scipy/__init__.py:138: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.1)\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion} is required for this version of \"\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "import transformers\n",
    "import torch\n",
    "import torchprofile\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_dataset, load_metric\n",
    "from itertools import chain\n",
    "from intel_extension_for_transformers.optimization import metrics, OptimizedModel, QuantizationConfig, DynamicLengthConfig\n",
    "from intel_extension_for_transformers.optimization.trainer import NLPTrainer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils.versions import require_version\n",
    "from typing import Optional\n",
    "from intel_extension_for_transformers.optimization.utils.models.modeling_roberta_dynamic import RobertaForQuestionAnswering\n",
    "# to use modeling roberta with LAT:\n",
    "transformers.models.roberta.modeling_roberta.RobertaForQuestionAnswering = RobertaForQuestionAnswering\n",
    "import collections\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    "    default_data_collator,\n",
    "    EvalPrediction,\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn import KLDivLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"True\"\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048665d4",
   "metadata": {},
   "source": [
    "## Define arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aead749f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"The model checkpoint for weights initialization.\"\n",
    "            \"Don't set if you want to train a model from scratch.\"\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=384,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch (which can \"\n",
    "            \"be faster on GPU but will be slower on TPU).\"\n",
    "        },\n",
    "    )\n",
    "    doc_stride: int = field(\n",
    "        default=128,\n",
    "        metadata={\"help\": \"When splitting up a long document into chunks, how much stride to take between chunks.\"},\n",
    "    )\n",
    "    n_best_size: int = field(\n",
    "        default=20,\n",
    "        metadata={\"help\": \"The total number of n-best predictions to generate when looking for an answer.\"},\n",
    "    )\n",
    "    max_answer_length: int = field(\n",
    "        default=30,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum length of an answer that can be generated. This is needed because the start \"\n",
    "            \"and end predictions are not conditioned on one another.\"\n",
    "        },\n",
    "    )\n",
    "    version_2_with_negative: bool = field(\n",
    "        default=False, metadata={\"help\": \"If true, some of the examples do not have an answer.\"}\n",
    "    )\n",
    "    null_score_diff_threshold: float = field(\n",
    "        default=0.0,\n",
    "        metadata={\n",
    "            \"help\": \"The threshold used to select the null answer: if the best answer has a score that is less than \"\n",
    "            \"the score of the null answer minus this threshold, the null answer is selected for this example. \"\n",
    "            \"Only useful when `version_2_with_negative=True`.\"\n",
    "        },\n",
    "    )\n",
    "                 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522f4153",
   "metadata": {},
   "source": [
    "## Download dataset from the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22bcb0fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 03:30:05 [WARNING] Reusing dataset squad (/nfs/site/home/sguskin/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305b29af83df4d3786a46ef1358217c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "data_args = DataTrainingArguments(\n",
    "    dataset_name=\"squad\"\n",
    ")\n",
    "\n",
    "raw_datasets = load_dataset(\n",
    "    data_args.dataset_name, data_args.dataset_config_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634a186",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a873a324",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = raw_datasets[\"train\"] \n",
    "eval_examples = raw_datasets[\"validation\"]\n",
    "column_names = raw_datasets[\"train\"].column_names\n",
    "column_names = raw_datasets[\"validation\"].column_names\n",
    "\n",
    "question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    " \n",
    "def run_data_processing(tokenizer):\n",
    "    \n",
    "    \n",
    "    # Training preprocessing\n",
    "    def prepare_train_features(examples):\n",
    "        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_length,\n",
    "            stride=data_args.doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
    "        )\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "        tokenized_examples[\"start_positions\"] = []\n",
    "        tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "        for i, offsets in enumerate(offset_mapping):\n",
    "            input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "            cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            sample_index = sample_mapping[i]\n",
    "            answers = examples[answer_column_name][sample_index]\n",
    "            if len(answers[\"answer_start\"]) == 0:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                start_char = answers[\"answer_start\"][0]\n",
    "                end_char = start_char + len(answers[\"text\"][0])\n",
    "                token_start_index = 0\n",
    "                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                    token_start_index += 1\n",
    "                token_end_index = len(input_ids) - 1\n",
    "                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                    token_end_index -= 1\n",
    "\n",
    "                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                    tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                    tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                else:\n",
    "                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                        token_start_index += 1\n",
    "                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                    while offsets[token_end_index][1] >= end_char:\n",
    "                        token_end_index -= 1\n",
    "                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "        return tokenized_examples\n",
    "\n",
    "\n",
    "    # Validation preprocessing\n",
    "    def prepare_validation_features(examples):\n",
    "        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "        tokenized_examples = tokenizer(\n",
    "            examples[question_column_name if pad_on_right else context_column_name],\n",
    "            examples[context_column_name if pad_on_right else question_column_name],\n",
    "            truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "            max_length=max_seq_length,\n",
    "            stride=data_args.doc_stride,\n",
    "            return_overflowing_tokens=True,\n",
    "            return_offsets_mapping=True,\n",
    "            padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
    "        )\n",
    "\n",
    "        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "        tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "        for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "            sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "            context_index = 1 if pad_on_right else 0\n",
    "            sample_index = sample_mapping[i]\n",
    "            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "            tokenized_examples[\"offset_mapping\"][i] = [\n",
    "                (o if sequence_ids[k] == context_index else None)\n",
    "                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "            ]\n",
    "\n",
    "        return tokenized_examples\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "    \n",
    "    # Create train feature from dataset\n",
    "    train_dataset = raw_datasets[\"train\"] \n",
    "    train_dataset = train_dataset.map(\n",
    "        prepare_train_features,\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on train dataset\")\n",
    "\n",
    "    # Validation Feature Creation\n",
    "    eval_examples = raw_datasets[\"validation\"]\n",
    "    eval_dataset = eval_examples.map(\n",
    "        prepare_validation_features,\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on validation dataset\")\n",
    "\n",
    "    # Data collator\n",
    "    # We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n",
    "    # collator.\n",
    "    data_collator = (\n",
    "        default_data_collator\n",
    "        if data_args.pad_to_max_length\n",
    "        else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n",
    "    )\n",
    "    \n",
    "    return train_dataset, eval_dataset, eval_examples, data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7450ae9b",
   "metadata": {},
   "source": [
    "### Preprocess data for BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3852f6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 03:30:15 [WARNING] Parameter 'function'=<function run_data_processing.<locals>.prepare_train_features at 0x7fb0f1294f70> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f87b7f2109f4c3c9f472d78975f923d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d686683dd1434d5fbf63e25e2909d07a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd408c238eb46eaa0e82ecf615fd737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on train dataset:   0%|          | 0/88 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e828f5213464435b5dd5fbe94c52e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on validation dataset:   0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LENGTH=384\n",
    "\n",
    "data_args = DataTrainingArguments(\n",
    "    dataset_name=\"squad\",\n",
    "    max_seq_length=MAX_LENGTH,\n",
    "    overwrite_cache=True\n",
    ")\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "roberta_train_dataset, roberta_eval_dataset, roberta_eval_examples, roberta_data_collator = run_data_processing(roberta_tokenizer)\n",
    "bert_train_dataset, bert_eval_dataset, bert_eval_examples, bert_data_collator = run_data_processing(bert_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c50ad4d",
   "metadata": {},
   "source": [
    "## Prepare post-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d1073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=data_args.version_2_with_negative,\n",
    "        n_best_size=data_args.n_best_size,\n",
    "        max_answer_length=data_args.max_answer_length,\n",
    "        null_score_diff_threshold=data_args.null_score_diff_threshold,\n",
    "        output_dir=training_args.output_dir,\n",
    "        log_level=logging.WARNING,\n",
    "        prefix=stage,\n",
    "    )\n",
    "    # Format the result to the format the metric expects.\n",
    "    if data_args.version_2_with_negative:\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
    "\n",
    "\n",
    "def postprocess_qa_predictions(\n",
    "    examples,\n",
    "    features,\n",
    "    predictions: Tuple[np.ndarray, np.ndarray],\n",
    "    version_2_with_negative: bool = False,\n",
    "    n_best_size: int = 20,\n",
    "    max_answer_length: int = 30,\n",
    "    null_score_diff_threshold: float = 0.0,\n",
    "    output_dir: Optional[str] = None,\n",
    "    prefix: Optional[str] = None,\n",
    "    log_level: Optional[int] = logging.WARNING,\n",
    "):\n",
    "    \"\"\"\n",
    "    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\n",
    "    original contexts. This is the base postprocessing functions for models that only return start and end logits.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if len(predictions) != 2:\n",
    "        raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")\n",
    "    all_start_logits, all_end_logits = predictions\n",
    "\n",
    "    if len(predictions[0]) != len(features):\n",
    "        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n",
    "\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    all_predictions = collections.OrderedDict()\n",
    "    all_nbest_json = collections.OrderedDict()\n",
    "    if version_2_with_negative:\n",
    "        scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_prediction = None\n",
    "        prelim_predictions = []\n",
    "\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n",
    "            # available in the current feature.\n",
    "            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n",
    "\n",
    "            # Update minimum null prediction.\n",
    "            feature_null_score = start_logits[0] + end_logits[0]\n",
    "            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n",
    "                min_null_prediction = {\n",
    "                    \"offsets\": (0, 0),\n",
    "                    \"score\": feature_null_score,\n",
    "                    \"start_logit\": start_logits[0],\n",
    "                    \"end_logit\": end_logits[0],\n",
    "                }\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or len(offset_mapping[start_index]) < 2\n",
    "                        or offset_mapping[end_index] is None\n",
    "                        or len(offset_mapping[end_index]) < 2\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    # Don't consider answer that don't have the maximum context available (if such information is\n",
    "                    # provided).\n",
    "                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
    "                        continue\n",
    "\n",
    "                    prelim_predictions.append(\n",
    "                        {\n",
    "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"start_logit\": start_logits[start_index],\n",
    "                            \"end_logit\": end_logits[end_index],\n",
    "                        }\n",
    "                    )\n",
    "        if version_2_with_negative:\n",
    "            # Add the minimum null prediction\n",
    "            prelim_predictions.append(min_null_prediction)\n",
    "            null_score = min_null_prediction[\"score\"]\n",
    "\n",
    "        # Only keep the best `n_best_size` predictions.\n",
    "        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "\n",
    "        # Add back the minimum null prediction if it was removed because of its low score.\n",
    "        if version_2_with_negative and not any(p[\"offsets\"] == (0, 0) for p in predictions):\n",
    "            predictions.append(min_null_prediction)\n",
    "\n",
    "        # Use the offsets to gather the answer text in the original context.\n",
    "        context = example[\"context\"]\n",
    "        for pred in predictions:\n",
    "            offsets = pred.pop(\"offsets\")\n",
    "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
    "\n",
    "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "        # failure.\n",
    "        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n",
    "            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
    "\n",
    "        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n",
    "        # the LogSumExp trick).\n",
    "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
    "        exp_scores = np.exp(scores - np.max(scores))\n",
    "        probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "        # Include the probabilities in our predictions.\n",
    "        for prob, pred in zip(probs, predictions):\n",
    "            pred[\"probability\"] = prob\n",
    "\n",
    "        # Pick the best prediction. If the null answer is not possible, this is easy.\n",
    "        if not version_2_with_negative:\n",
    "            all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n",
    "        else:\n",
    "            # Otherwise we first need to find the best non-empty prediction.\n",
    "            i = 0\n",
    "            while predictions[i][\"text\"] == \"\":\n",
    "                i += 1\n",
    "            best_non_null_pred = predictions[i]\n",
    "\n",
    "            # Then we compare to the null prediction using the threshold.\n",
    "            score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n",
    "            scores_diff_json[example[\"id\"]] = float(score_diff)  # To be JSON-serializable.\n",
    "            if score_diff > null_score_diff_threshold:\n",
    "                all_predictions[example[\"id\"]] = \"\"\n",
    "            else:\n",
    "                all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
    "\n",
    "        # Make `predictions` JSON-serializable by casting np.float back to float.\n",
    "        all_nbest_json[example[\"id\"]] = [\n",
    "            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n",
    "            for pred in predictions\n",
    "        ]\n",
    "\n",
    "    # If we have an output_dir, let's save all those dicts.\n",
    "    if output_dir is not None:\n",
    "        if not os.path.isdir(output_dir):\n",
    "            raise EnvironmentError(f\"{output_dir} is not a directory.\")\n",
    "\n",
    "        prediction_file = os.path.join(\n",
    "            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n",
    "        )\n",
    "        nbest_file = os.path.join(\n",
    "            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n",
    "        )\n",
    "        if version_2_with_negative:\n",
    "            null_odds_file = os.path.join(\n",
    "                output_dir, \"null_odds.json\" if prefix is None else f\"{prefix}_null_odds.json\"\n",
    "            )\n",
    "\n",
    "        logger.info(f\"Saving predictions to {prediction_file}.\")\n",
    "        with open(prediction_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "        logger.info(f\"Saving nbest_preds to {nbest_file}.\")\n",
    "        with open(nbest_file, \"w\") as writer:\n",
    "            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "        if version_2_with_negative:\n",
    "            logger.info(f\"Saving null_odds to {null_odds_file}.\")\n",
    "            with open(null_odds_file, \"w\") as writer:\n",
    "                writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if data_args.version_2_with_negative else \"squad\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064669e",
   "metadata": {},
   "source": [
    "## Define the QA Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8503e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionAnsweringTrainer(NLPTrainer):\n",
    "    def __init__(self, *args, eval_examples, post_process_function, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.eval_examples = eval_examples\n",
    "        self.post_process_function = post_process_function\n",
    "\n",
    "    def evaluate(self, eval_dataset=None, eval_examples=None, ignore_keys=None, metric_key_prefix: str = \"eval\"):\n",
    "        eval_dataset = self.eval_dataset if eval_dataset is None else eval_dataset\n",
    "        eval_dataloader = self.get_eval_dataloader(eval_dataset)\n",
    "        eval_examples = self.eval_examples if eval_examples is None else eval_examples\n",
    "        compute_metrics = self.compute_metrics\n",
    "        self.compute_metrics = None\n",
    "        eval_loop = self.prediction_loop if self.args.use_legacy_prediction_loop else self.evaluation_loop\n",
    "        try:\n",
    "            output = eval_loop(\n",
    "                eval_dataloader,\n",
    "                description=\"Evaluation\",\n",
    "                prediction_loss_only=True if compute_metrics is None else None,\n",
    "                ignore_keys=ignore_keys,\n",
    "            )\n",
    "        finally:\n",
    "            self.compute_metrics = compute_metrics\n",
    "\n",
    "        eval_preds = self.post_process_function(eval_examples, eval_dataset, output.predictions)\n",
    "        metrics = self.compute_metrics(eval_preds)\n",
    "\n",
    "        # Prefix all keys with metric_key_prefix + '_'\n",
    "        for key in list(metrics.keys()):\n",
    "            if not key.startswith(f\"{metric_key_prefix}_\"):\n",
    "                metrics[f\"{metric_key_prefix}_{key}\"] = metrics.pop(key)\n",
    "\n",
    "        self.log(metrics)\n",
    "\n",
    "        if self.args.tpu_metrics_debug or self.args.debug:\n",
    "            # tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\n",
    "            xm.master_print(met.metrics_report())\n",
    "\n",
    "        self.control = self.callback_handler.on_evaluate(self.args, self.state, self.control, metrics)\n",
    "        return metrics\n",
    "    \n",
    "    \n",
    "    def compute_latency(self, dummy_inputs):\n",
    "        repetitions = 300\n",
    "        timings=np.zeros((repetitions,1))\n",
    "        with torch.no_grad():\n",
    "            for rep in range(repetitions):\n",
    "                start_time = timeit.default_timer()\n",
    "                _ = self.model(dummy_inputs)\n",
    "                end_time = timeit.default_timer() - start_time\n",
    "                timings[rep] = end_time\n",
    "        return (np.sum(timings) / repetitions) * 1000\n",
    "    \n",
    "    \n",
    "    def compute_flops(self, dummy_inputs):\n",
    "        flops = torchprofile.profile_macs(model, dummy_inputs)\n",
    "        return flops\n",
    "    \n",
    "    \n",
    "    def run_benchmark(self):\n",
    "        set_seed(training_args.seed)\n",
    "\n",
    "        start_time = timeit.default_timer()\n",
    "        results = self.evaluate()\n",
    "        evalTime = timeit.default_timer() - start_time\n",
    "        eval_samples = len(self.eval_dataset)\n",
    "        samples = eval_samples - (eval_samples % self.args.per_device_eval_batch_size) \\\n",
    "            if self.args.dataloader_drop_last else eval_samples\n",
    "\n",
    "        size = (1, data_args.max_seq_length)\n",
    "        dummy_inputs = torch.ones(size, dtype=torch.long).to(self.args.device)\n",
    "        latency = self.compute_latency(dummy_inputs)\n",
    "        flops = self.compute_flops(dummy_inputs)\n",
    "\n",
    "\n",
    "        print(\"Batch size = {}\".format(self.args.per_device_eval_batch_size))\n",
    "        print(\"Finally Eval {} Accuracy: {}\".format('eval_f1', results['eval_f1']))\n",
    "        print(\"Latency: {} ms\".format(evalTime / samples * 1000))\n",
    "        print(\"Throughput: {} samples/sec\".format(samples/evalTime))\n",
    "        print(\"Duration: {} sec\".format(evalTime))\n",
    "        print(\"Latency: {} ms\".format(latency))\n",
    "        print(\"FLOPs: {} \".format(flops))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd3980e",
   "metadata": {},
   "source": [
    "# Dynamic MiniLM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2156ea01",
   "metadata": {},
   "source": [
    "## Finetune General MiniLM [skip to next cell to use a pre-trained model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11b035ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "loading configuration file https://huggingface.co/nreimers/MiniLMv2-L6-H384-distilled-from-RoBERTa-Large/resolve/main/config.json from cache at /nfs/site/home/sguskin/.cache/huggingface/transformers/75b95a76c6ebbfa5159ad312e1db4c1496c1457f80b3efc920e4875859915fbf.63e63567d8e7af602c1f3fc233723a51baffb13ac83b69c81d922ccee39694b6\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"nreimers/MiniLMv2-L6-H384-distilled-from-RoBERTa-Large\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/nreimers/MiniLMv2-L6-H384-distilled-from-RoBERTa-Large/resolve/main/pytorch_model.bin from cache at /nfs/site/home/sguskin/.cache/huggingface/transformers/484ba5d86b62780ae4331ef6a6540d511df65f4273fa6068fbf6bf44473d94ac.91be27c626ad4aa50b0fb750a3802eac61aa0de3cfdd3cee715c5ac84f488321\n",
      "Some weights of the model checkpoint at nreimers/MiniLMv2-L6-H384-distilled-from-RoBERTa-Large were not used when initializing RobertaForQuestionAnswering: ['lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at nreimers/MiniLMv2-L6-H384-distilled-from-RoBERTa-Large and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "2022-07-31 04:08:58 [INFO] ***** Running training *****\n",
      "2022-07-31 04:08:58 [INFO]   Num examples = 88568\n",
      "2022-07-31 04:08:58 [INFO]   Num Epochs = 5\n",
      "2022-07-31 04:08:58 [INFO]   Instantaneous batch size per device = 8\n",
      "2022-07-31 04:08:58 [INFO]   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "2022-07-31 04:08:58 [INFO]   Gradient Accumulation steps = 1\n",
      "2022-07-31 04:08:58 [INFO]   Total optimization steps = 55355\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='55355' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [    2/55355 : < :, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-acb81c038a7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, component, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0;31m# pylint: disable=E0401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1025\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   1255\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_training\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"component\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict, layer_config, length_config, always_keep_cls_token)\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1584\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1585\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, layer_config, length_config, always_keep_cls_token)\u001b[0m\n\u001b[1;32m    905\u001b[0m         )\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    908\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, layer_config, length_config, always_keep_cls_token)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 )\n\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m                 layer_outputs, keep_indices = layer_module(\n\u001b[0m\u001b[1;32m    556\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, output_length, always_keep_cls_token)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 336\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mmixed_query_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "LEARNING_RATE = 3e-5\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "model_output_dir = \"./finetuned_minilm\"\n",
    "model_id = \"nreimers/MiniLMv2-L6-H384-distilled-from-RoBERTa-Large\"\n",
    "\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=model_id, \n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    no_cuda=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config\n",
    ")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=roberta_train_dataset,\n",
    "        eval_dataset=roberta_eval_dataset,\n",
    "        eval_examples=roberta_eval_examples,\n",
    "        tokenizer=roberta_tokenizer,\n",
    "        data_collator=roberta_data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d2967",
   "metadata": {},
   "source": [
    "## Length-Adaptive Training [skip to next cell to use a pre-trained model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df04531d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "loading configuration file https://huggingface.co/sguskin/minilmv2-L6-H384-squad1.1/resolve/main/config.json from cache at /nfs/site/home/sguskin/.cache/huggingface/transformers/2a0a3167cdf8c882e045b75110510a5556b91912782b70bb89479bc195e7b119.e71a9885c092b916425692f83d14fc42505bce78475d548b96142356756b8f78\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sguskin/minilmv2-L6-H384-squad1.1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sguskin/minilmv2-L6-H384-squad1.1/resolve/main/pytorch_model.bin from cache at /nfs/site/home/sguskin/.cache/huggingface/transformers/bb2c92543eaaaa575738bc9e75cdf8556fc6e9e1fecc4d65b671f6ce77e59812.26662ce9d6bd7953fb1fcbcb74eccc0eced80f958065644264e1928ce81ff497\n",
      "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at sguskin/minilmv2-L6-H384-squad1.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "2022-07-31 00:02:42 [INFO] ***** Running training *****\n",
      "2022-07-31 00:02:42 [INFO]   Num examples = 88568\n",
      "2022-07-31 00:02:42 [INFO]   Num Epochs = 10\n",
      "2022-07-31 00:02:42 [INFO]   Instantaneous batch size per device = 8\n",
      "2022-07-31 00:02:42 [INFO]   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "2022-07-31 00:02:42 [INFO]   Gradient Accumulation steps = 1\n",
      "2022-07-31 00:02:42 [INFO]   Total optimization steps = 110710\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='110710' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [     4/110710 00:03 < 58:35:05, 0.52 it/s, Epoch 0.00/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-b7ddcf3f3c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# train a length-adaptive model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, component, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m    804\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/trainer.py\u001b[0m in \u001b[0;36mtraining_step_length_adaptive\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1236\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1238\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "LEARNING_RATE = 3e-5\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "model_output_dir = \"./dynamic_minilm\"\n",
    "model_id = \"sguskin/minilmv2-L6-H384-squad1.1\"\n",
    "\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=model_id, \n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    no_cuda=True,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config\n",
    ")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=roberta_train_dataset,\n",
    "        eval_dataset=roberta_eval_dataset,\n",
    "        eval_examples=roberta_eval_examples,\n",
    "        tokenizer=roberta_tokenizer,\n",
    "        data_collator=roberta_data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "# set trainer to train with dynamic-lengths\n",
    "\n",
    "dynamic_length_config = DynamicLengthConfig(\n",
    "    dynamic_training=True,\n",
    "    num_sandwich=2,\n",
    "    length_drop_ratio_bound=0.2,\n",
    "    layer_dropout_prob=0.2,\n",
    "    layer_dropout_bound=0,\n",
    "    max_length=data_args.max_seq_length\n",
    ")\n",
    "\n",
    "trainer.set_dynamic_config(dynamic_length_config)\n",
    "\n",
    "# train a length-adaptive model\n",
    "\n",
    "trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d39641",
   "metadata": {},
   "source": [
    "## Run evolutionary search to find the optimal length configuration [skip to the benchmark section to see a pre-calculated result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c48fdc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "loading configuration file https://huggingface.co/sguskin/dynamic-minilmv2-L6-H384-squad1.1/resolve/main/config.json from cache at /nfs/site/home/sguskin/.cache/huggingface/transformers/a44461e990b9565b874f36efa632d10137ea10d6601ed9b20aac30d24f2a9f73.55d058ace8df1ff1081862f63663338c8e826cbc9617c9a914349e35c8de1d98\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sguskin/dynamic-minilmv2-L6-H384-squad1.1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sguskin/dynamic-minilmv2-L6-H384-squad1.1/resolve/main/pytorch_model.bin from cache at /nfs/site/home/sguskin/.cache/huggingface/transformers/5a9501e18debd14416f6e474a6727b67024243d5e27a043143713d735cee3a68.61d8609e27c6af89935061ff6c32a2d720c0b1230ddd5ef9f7e65fe8b4553872\n",
      "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at sguskin/dynamic-minilmv2-L6-H384-squad1.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::cumsum\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::type_as\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scalarimplicit\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::arange\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::repeat\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::topk\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::expand\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gather\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gelu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scatter\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "2022-07-31 00:02:58 [INFO] The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10790\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 26/675 00:05 < 02:13, 4.85 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-e62057320dd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# run search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0msearch_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_evolutionary_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/trainer.py\u001b[0m in \u001b[0;36mrun_evolutionary_search\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2042\u001b[0m         )\n\u001b[1;32m   2043\u001b[0m         \u001b[0mupper_gene\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamic_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m         \u001b[0mevolution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_gene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlower_gene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m         \u001b[0mevolution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_gene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupper_gene\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m         \u001b[0mevolution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower_constraint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevolution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlower_gene\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/dynamic/evolution.py\u001b[0m in \u001b[0;36madd_gene\u001b[0;34m(self, gene, macs, score, method, parents)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0meval_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mevalTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0meval_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_f1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-1f0d81291661>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, eval_examples, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             output = eval_loop(\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2798\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2799\u001b[0m             \u001b[0minputs_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_inputs_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3047\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3048\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3049\u001b[0;31m                         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3050\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3051\u001b[0m                         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict, layer_config, length_config, always_keep_cls_token)\u001b[0m\n\u001b[1;32m   1582\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1584\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1585\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, layer_config, length_config, always_keep_cls_token)\u001b[0m\n\u001b[1;32m    905\u001b[0m         )\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    908\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, layer_config, length_config, always_keep_cls_token)\u001b[0m\n\u001b[1;32m    553\u001b[0m                 )\n\u001b[1;32m    554\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m                 layer_outputs, keep_indices = layer_module(\n\u001b[0m\u001b[1;32m    556\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, output_length, always_keep_cls_token)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 336\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;31m# Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m             \u001b[0mattention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_scores\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Normalize the attention scores to probabilities.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "EVAL_BATCH_SIZE = 16\n",
    "model_output_dir = \"./evo_search\"\n",
    "model_id = \"sguskin/dynamic-minilmv2-L6-H384-squad1.1\"\n",
    "\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=model_id, \n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    no_cuda=True,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config\n",
    ")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "search_trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=roberta_train_dataset,\n",
    "        eval_dataset=roberta_eval_dataset,\n",
    "        eval_examples=roberta_eval_examples,\n",
    "        tokenizer=roberta_tokenizer,\n",
    "        data_collator=roberta_data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "# set trainer with evolutionary search parameters\n",
    "\n",
    "dynamic_length_config = DynamicLengthConfig(\n",
    "            evo_iter=30,\n",
    "            population_size=20,\n",
    "            mutation_size=30,\n",
    "            mutation_prob=0.5,\n",
    "            crossover_size=30,\n",
    "            max_length=data_args.max_seq_length,\n",
    "            model_name_or_path=model_args.model_name_or_path\n",
    "        )\n",
    "search_trainer.set_dynamic_config(dynamic_length_config)        \n",
    "\n",
    "# run search\n",
    "search_trainer.run_evolutionary_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5b0fd7",
   "metadata": {},
   "source": [
    "## Evolutionary search reuslts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b072209f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_lc: (269, 253, 252, 202, 104, 34)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot pareto-graph found by evolutionary search, and select\n",
    "# the best length-configuration (lc) within your computational budget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "cols=['lc', 'flops', 'f1','method', 'parents'] \n",
    "df=pd.read_csv('evo_search/parents-iter30.tsv',  names=cols, sep='\\t', header=None)\n",
    "df.plot(x ='flops', y='f1', kind = 'scatter', title='Dynamic-MiniLM')\n",
    "\n",
    "constraint = 87.5 # not below 1%-loss of bert-base\n",
    "best_lc = df[df[\"f1\"] >= constraint].min()['lc']\n",
    "print('best_lc: ' + str(best_lc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc02dd99",
   "metadata": {},
   "source": [
    "# Dynamic MiniLM Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ac9d8",
   "metadata": {},
   "source": [
    "## Set benchmark parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67b8ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WAND_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EVAL_BATCH_SIZE = 16\n",
    "model_output_dir = \"./benchmark_results\"\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_output_dir,\n",
    "    no_cuda=True,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    overwrite_output_dir=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5b041",
   "metadata": {},
   "source": [
    "## Dynamic-MiniLM performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "689e288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sguskin/dynamic-minilmv2-L6-H384-squad1.1/resolve/main/config.json from cache at /nfs/site/home/sguskin/.cache/huggingface/transformers/a44461e990b9565b874f36efa632d10137ea10d6601ed9b20aac30d24f2a9f73.55d058ace8df1ff1081862f63663338c8e826cbc9617c9a914349e35c8de1d98\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sguskin/dynamic-minilmv2-L6-H384-squad1.1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sguskin/dynamic-minilmv2-L6-H384-squad1.1/resolve/main/pytorch_model.bin from cache at /nfs/site/home/sguskin/.cache/huggingface/transformers/5a9501e18debd14416f6e474a6727b67024243d5e27a043143713d735cee3a68.61d8609e27c6af89935061ff6c32a2d720c0b1230ddd5ef9f7e65fe8b4553872\n",
      "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at sguskin/dynamic-minilmv2-L6-H384-squad1.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "2022-07-31 02:10:32 [INFO] The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10790\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1350' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [675/675 13:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03899e3dc781464886e58bc793c7972b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 02:18:04 [INFO] Saving predictions to ./benchmark_results/eval_predictions.json.\n",
      "2022-07-31 02:18:04 [INFO] Saving nbest_preds to ./benchmark_results/eval_nbest_predictions.json.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = 16\n",
      "Finally Eval eval_f1 Accuracy: 89.28118310884184\n",
      "Latency: 42.47066985688019 ms\n",
      "Throughput: 23.54566112024723 samples/sec\n",
      "Duration: 458.2585277557373 sec\n",
      "Latency: 16.216413071379066 ms\n",
      "FLOPs: 4758553344 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::ones\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::expand\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::cumsum\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::type_as\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gelu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    }
   ],
   "source": [
    "# load dynamic-minilm model\n",
    "\n",
    "model_id = \"sguskin/dynamic-minilmv2-L6-H384-squad1.1\"\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=model_id, \n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config\n",
    ")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "dynamic_trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=roberta_train_dataset,\n",
    "        eval_dataset=roberta_eval_dataset,\n",
    "        eval_examples=roberta_eval_examples,\n",
    "        tokenizer=roberta_tokenizer,\n",
    "        data_collator=roberta_data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "dynamic_trainer.run_benchmark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8172fdfe",
   "metadata": {},
   "source": [
    "## Dynamic-MiniLM performance with best length-configuration found by evolutionary-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5194fbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 02:21:26 [INFO] The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10790\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting length config to - (269, 253, 252, 202, 104, 34)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719256b67cc641a3bf4fd68ded9c7fdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 02:24:56 [INFO] Saving predictions to ./benchmark_results/eval_predictions.json.\n",
      "2022-07-31 02:24:56 [INFO] Saving nbest_preds to ./benchmark_results/eval_nbest_predictions.json.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = 16\n",
      "Finally Eval eval_f1 Accuracy: 87.76366630183897\n",
      "Latency: 20.010042966682672 ms\n",
      "Throughput: 49.97490518461306 samples/sec\n",
      "Duration: 215.90836361050606 sec\n",
      "Latency: 17.005792840694387 ms\n",
      "FLOPs: 2485456896 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::ones\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::expand\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::cumsum\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::type_as\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scalarimplicit\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::arange\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::repeat\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::topk\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gather\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gelu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scatter\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dynamic_length_config = DynamicLengthConfig(\n",
    "        length_config=best_lc\n",
    "    )\n",
    "\n",
    "# set sequence-lengths per each layer during evaluation\n",
    "dynamic_trainer.set_dynamic_config(dynamic_length_config)\n",
    "\n",
    "dynamic_trainer.run_benchmark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855c2eb",
   "metadata": {},
   "source": [
    "## Baseline - BERT-Base performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ba75c1c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 03:34:15 [INFO] Lock 140397938227472 acquired on /nfs/site/home/sguskin/.cache/huggingface/transformers/4f73d77ab928c1e85c649317a03fb1d8bb17c4271c9925f77242cc5c8e8a2b6f.44e7e7e8a71d2128b2d7743552f49bbaa35ba4056ca5b61fc2c5c12edb3e4f48.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c4750c7d48c446ebb14613c16b240bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 03:34:15 [INFO] Lock 140397938227472 released on /nfs/site/home/sguskin/.cache/huggingface/transformers/4f73d77ab928c1e85c649317a03fb1d8bb17c4271c9925f77242cc5c8e8a2b6f.44e7e7e8a71d2128b2d7743552f49bbaa35ba4056ca5b61fc2c5c12edb3e4f48.lock\n",
      "2022-07-31 03:34:16 [INFO] Lock 140397938227472 acquired on /nfs/site/home/sguskin/.cache/huggingface/transformers/1827bd29e826845f5f0fb3ad61e3a61daf3820ffb6561b4d8eca942a23290198.5008df80b7b872551ce8e818b04ba991d932907cbd1f15ca3470239e0c2f9245.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7393ed37cec148edb31bdc406a94817b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/415M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 03:34:26 [INFO] Lock 140397938227472 released on /nfs/site/home/sguskin/.cache/huggingface/transformers/1827bd29e826845f5f0fb3ad61e3a61daf3820ffb6561b4d8eca942a23290198.5008df80b7b872551ce8e818b04ba991d932907cbd1f15ca3470239e0c2f9245.lock\n",
      "2022-07-31 03:34:31 [INFO] The following columns in the evaluation set  don't have a corresponding argument in `BertForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10784\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='674' max='674' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [674/674 20:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e112eb164044080b1b7f29ab2e74281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 03:55:36 [INFO] Saving predictions to ./benchmark_results/eval_predictions.json.\n",
      "2022-07-31 03:55:36 [INFO] Saving nbest_preds to ./benchmark_results/eval_nbest_predictions.json.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = 16\n",
      "Finally Eval eval_f1 Accuracy: 88.94360518627263\n",
      "Latency: 117.84504024522278 ms\n",
      "Throughput: 8.485719873480532 samples/sec\n",
      "Duration: 1270.8409140044823 sec\n",
      "Latency: 51.25485342927277 ms\n",
      "FLOPs: 35340779904 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::ones\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::expand\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gelu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    }
   ],
   "source": [
    "# load bert-base model\n",
    "\n",
    "model_id = \"sguskin/bert-base-uncased-squad1\"\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=model_id, \n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config\n",
    ")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "bert_trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=bert_train_dataset,\n",
    "        eval_dataset=bert_eval_dataset,\n",
    "        eval_examples=bert_eval_examples,\n",
    "        tokenizer=bert_tokenizer,\n",
    "        data_collator=bert_data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "bert_trainer.run_benchmark()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a8735",
   "metadata": {},
   "source": [
    "# Static Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "32c1e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/sguskin/dynamic-minilmv2-L6-H384-squad1.1/resolve/main/config.json from cache at /nfs/site/home/sguskin/.cache/huggingface/transformers/a44461e990b9565b874f36efa632d10137ea10d6601ed9b20aac30d24f2a9f73.55d058ace8df1ff1081862f63663338c8e826cbc9617c9a914349e35c8de1d98\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"sguskin/dynamic-minilmv2-L6-H384-squad1.1\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForQuestionAnswering\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 384,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 1536,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/sguskin/dynamic-minilmv2-L6-H384-squad1.1/resolve/main/pytorch_model.bin from cache at /nfs/site/home/sguskin/.cache/huggingface/transformers/5a9501e18debd14416f6e474a6727b67024243d5e27a043143713d735cee3a68.61d8609e27c6af89935061ff6c32a2d720c0b1230ddd5ef9f7e65fe8b4553872\n",
      "All model checkpoint weights were used when initializing RobertaForQuestionAnswering.\n",
      "\n",
      "All the weights of RobertaForQuestionAnswering were initialized from the model checkpoint at sguskin/dynamic-minilmv2-L6-H384-squad1.1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForQuestionAnswering for predictions without further training.\n",
      "2022-07-31 04:24:05 [INFO] Pass query framework capability elapsed time: 218.78 ms\n",
      "2022-07-31 04:24:05 [INFO] Get FP32 model baseline.\n",
      "2022-07-31 04:24:05 [INFO] The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10790\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1349' max='1349' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1349/1349 02:45]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200f7806973945c4a0e3c86f26cad82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 04:27:25 [INFO] Saving predictions to ./finetuned_minilm/eval_predictions.json.\n",
      "2022-07-31 04:27:25 [INFO] Saving nbest_preds to ./finetuned_minilm/eval_nbest_predictions.json.\n",
      "2022-07-31 04:27:30 [INFO] {\n",
      "2022-07-31 04:27:30 [INFO]     'eval_exact_match': 81.90160832544939,\n",
      "2022-07-31 04:27:30 [INFO]     'eval_f1': 89.28118310884184\n",
      "2022-07-31 04:27:30 [INFO] }\n",
      "2022-07-31 04:27:30 [INFO] metric: 89.28118310884184\n",
      "2022-07-31 04:27:30 [INFO] Throughput: None samples/sec\n",
      "2022-07-31 04:27:30 [INFO] Save tuning history to /ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/docs/tutorials/pytorch/question-answering/nc_workspace/2022-07-31_03-23-24/./history.snapshot.\n",
      "2022-07-31 04:27:30 [INFO] FP32 baseline is: [Accuracy: 89.2812, Duration (seconds): 205.1094]\n",
      "2022-07-31 04:27:30 [INFO] Fx trace of the entire model failed, We will conduct auto quantization\n",
      "2022-07-31 04:27:31 [WARNING] Please note that calibration sampling size 100 isn't divisible exactly by batch size 8. So the real sampling size is 104.\n",
      "2022-07-31 04:27:34 [INFO] |*********Mixed Precision Statistics********|\n",
      "2022-07-31 04:27:34 [INFO] +---------------------+-------+------+------+\n",
      "2022-07-31 04:27:34 [INFO] |       Op Type       | Total | INT8 | FP32 |\n",
      "2022-07-31 04:27:34 [INFO] +---------------------+-------+------+------+\n",
      "2022-07-31 04:27:34 [INFO] |      Embedding      |   3   |  3   |  0   |\n",
      "2022-07-31 04:27:34 [INFO] |      LayerNorm      |   13  |  0   |  13  |\n",
      "2022-07-31 04:27:34 [INFO] | quantize_per_tensor |   37  |  37  |  0   |\n",
      "2022-07-31 04:27:34 [INFO] |        Linear       |   37  |  37  |  0   |\n",
      "2022-07-31 04:27:34 [INFO] |      dequantize     |   37  |  37  |  0   |\n",
      "2022-07-31 04:27:34 [INFO] |     input_tensor    |   12  |  12  |  0   |\n",
      "2022-07-31 04:27:34 [INFO] |       Dropout       |   12  |  0   |  12  |\n",
      "2022-07-31 04:27:34 [INFO] +---------------------+-------+------+------+\n",
      "2022-07-31 04:27:34 [INFO] Pass quantize model elapsed time: 3921.47 ms\n",
      "2022-07-31 04:27:34 [INFO] The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: offset_mapping, example_id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10790\n",
      "  Batch size = 8\n",
      "2022-07-31 04:27:34 [ERROR] Unexpected exception RuntimeError('Expect weight, indices, and offsets to be contiguous.') happened during tuning.\n",
      "Traceback (most recent call last):\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/neural_compressor/experimental/quantization.py\", line 151, in execute\n",
      "    self.strategy.traverse()\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/neural_compressor/strategy/strategy.py\", line 401, in traverse\n",
      "    self.last_tune_result = self._evaluate(self.last_qmodel)\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/neural_compressor/strategy/strategy.py\", line 505, in _evaluate\n",
      "    val = self.objectives.evaluate(\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/neural_compressor/objective.py\", line 266, in evaluate\n",
      "    acc = eval_func(model)\n",
      "  File \"/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/trainer.py\", line 152, in builtin_eval_func\n",
      "    results = self.evaluate()\n",
      "  File \"<ipython-input-7-1f0d81291661>\", line 15, in evaluate\n",
      "    output = eval_loop(\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\", line 2798, in evaluation_loop\n",
      "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/transformers/trainer.py\", line 3049, in prediction_step\n",
      "    outputs = model(**inputs)\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\", line 1584, in forward\n",
      "    outputs = self.roberta(\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\", line 899, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/utils/models/modeling_roberta_dynamic.py\", line 129, in forward\n",
      "    token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torch/fx/graph_module.py\", line 652, in call_wrapped\n",
      "    return self._wrapped_call(self, *args, **kwargs)\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torch/fx/graph_module.py\", line 277, in __call__\n",
      "    raise e\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torch/fx/graph_module.py\", line 267, in __call__\n",
      "    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"<eval_with_key>.1376\", line 6, in forward\n",
      "    module = self.module(x);  x = None\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torch/nn/quantized/modules/embedding_ops.py\", line 116, in forward\n",
      "    return torch.ops.quantized.embedding_byte(self._packed_params._packed_weight, indices)\n",
      "  File \"/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torch/_ops.py\", line 143, in __call__\n",
      "    return self._op(*args, **kwargs or {})\n",
      "RuntimeError: Expect weight, indices, and offsets to be contiguous.\n",
      "2022-07-31 04:27:34 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n",
      "Configuration saved in ./finetuned_minilm/config.json\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'quantized_state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1ce006c9d439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# run quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mquant_dynamic_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquantization_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/trainer.py\u001b[0m in \u001b[0;36mquantize\u001b[0;34m(self, quant_config, provider, eval_func, train_func, calib_dataloader)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nncf_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provider\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProvider\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inc_quantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprovider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprovider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Unsupport provider:{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_provider\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/trainer.py\u001b[0m in \u001b[0;36m_inc_quantize\u001b[0;34m(self, quant_config, provider)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_inc_quant\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;31m# pylint: disable=E1101\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_inc_int8\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;31m# pylint: disable=E1101\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         logger.info(\n",
      "\u001b[0;32m/ec/pdx/disks/mlp_lab_home_pool_02/sguskin/dynamic/frameworks.ai.intel-extension-for-transformers.intel-intel-extension-for-transformers/intel_extension_for_transformers/optimization/trainer.py\u001b[0m in \u001b[0;36m_save_inc_int8\u001b[0;34m(self, opt_model, output_dir)\u001b[0m\n\u001b[1;32m    308\u001b[0m         weights_file = os.path.join(os.path.abspath(\n\u001b[1;32m    309\u001b[0m           os.path.expanduser(output_dir)), WEIGHTS_NAME)\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         logger.info(\n\u001b[1;32m    312\u001b[0m             \u001b[0;34m\"quantized model and configure file have saved to {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'quantized_state_dict'"
     ]
    }
   ],
   "source": [
    "# load dynamic-minilm model\n",
    "\n",
    "model_id = \"sguskin/dynamic-minilmv2-L6-H384-squad1.1\"\n",
    "\n",
    "model_args = ModelArguments(\n",
    "    model_name_or_path=model_id, \n",
    ")\n",
    "\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "model = RobertaForQuestionAnswering.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config\n",
    ")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "quant_dynamic_trainer = QuestionAnsweringTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=roberta_train_dataset,\n",
    "        eval_dataset=roberta_eval_dataset,\n",
    "        eval_examples=roberta_eval_examples,\n",
    "        tokenizer=roberta_tokenizer,\n",
    "        data_collator=roberta_data_collator,\n",
    "        post_process_function=post_processing_function,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "\n",
    "tune_metric = metrics.Metric(\n",
    "    name=\"eval_f1\", # Metric used for the tuning strategy.\n",
    "    is_relative=True, # Metric tolerance mode, True is for relative, otherwise for absolute.\n",
    "    criterion=\"0.01\", # Performance tolerance when optimizing the model.\n",
    ")\n",
    "quantization_config = QuantizationConfig(\n",
    "    approach=\"PostTrainingStatic\",\n",
    "    max_trials=200,\n",
    "    metrics=[tune_metric],\n",
    ")\n",
    "\n",
    "# lc = \"(269, 253, 252, 202, 104, 34)\" # configure model with best length config\n",
    "\n",
    "# dynamic_length_config = DynamicLengthConfig(\n",
    "#         length_config=lc\n",
    "# )\n",
    "# quant_dynamic_trainer.set_dynamic_config(dynamic_length_config)\n",
    "\n",
    "\n",
    "# run quantization\n",
    "quant_dynamic_trainer.quantize(quant_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8039831",
   "metadata": {},
   "source": [
    "## Run Benchmark after Static Post Training Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf54c034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 01:40:29 [INFO] The following columns in the evaluation set  don't have a corresponding argument in `RobertaForQuestionAnswering.forward` and have been ignored: example_id, offset_mapping.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10790\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ce5b03088748889e24bf9d6bdeaa16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 01:45:48 [INFO] Saving predictions to ./benchmark_results/eval_predictions.json.\n",
      "2022-07-31 01:45:48 [INFO] Saving nbest_preds to ./benchmark_results/eval_nbest_predictions.json.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size = 1\n",
      "Finally Eval eval_f1 Accuracy: 87.26878045524857\n",
      "Latency: 30.149574108521925 ms\n",
      "Throughput: 33.167964376562956 samples/sec\n",
      "Duration: 325.3139046309516 sec\n",
      "Latency: 23.48242442123592 ms\n",
      "FLOPs: 2485456896 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::ones\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::expand\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::unsqueeze\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::cumsum\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::type_as\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scalarimplicit\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::arange\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::repeat\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::permute\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::topk\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gather\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::gelu\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::scatter\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n",
      "/nfs/site/home/sguskin/anaconda3/lib/python3.8/site-packages/torchprofile/profile.py:22: UserWarning: No handlers found: \"aten::split\". Skipped.\n",
      "  warnings.warn('No handlers found: \"{}\". Skipped.'.format(\n"
     ]
    }
   ],
   "source": [
    "quant_dynamic_trainer.run_benchmark()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f54fd8d6160ddfbc370985ee3ad2925997e28943a671b1747496a6859c59cd26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
