{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is used to list steps of introducing [Prune Once For All](https://arxiv.org/abs/2111.05754) examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Follow [installation](https://github.com/intel-innersource/frameworks.ai.nlp-toolkit.intel-nlp-toolkit#installation) to install **nlp-toolkit**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install model dependency\n",
    "!pip install datasets>=1.8.0 torch>=1.10.0 transformers>=4.12.0 wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "import functools\n",
    "import numpy as np\n",
    "import timeit\n",
    "import torch\n",
    "import transformers\n",
    "from intel_extension_for_transformers.optimization import (\n",
    "    metrics,\n",
    "    PrunerConfig,\n",
    "    PruningConfig,\n",
    "    DistillationConfig,\n",
    "    QuantizationConfig,\n",
    "    OptimizedModel,\n",
    "    objectives\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from trainer_qa import QuestionAnsweringTrainer\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizerFast,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "from typing import Optional\n",
    "from utils_qa import postprocess_qa_predictions\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.12.0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/huggingface/pytorch/question-answering/orchestrate_optimizations/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Define arguments =========\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    Using `HfArgumentParser` we can turn this class\n",
    "    into argparse arguments to be able to specify them on\n",
    "    the command line.\n",
    "    \"\"\"\n",
    "\n",
    "    task_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n",
    "    )\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    max_seq_length: int = field(\n",
    "        default=128,\n",
    "        metadata={\n",
    "            \"help\": \"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "            \"than this will be truncated, sequences shorter will be padded.\"\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n",
    "    )\n",
    "    pad_to_max_length: bool = field(\n",
    "        default=True,\n",
    "        metadata={\n",
    "            \"help\": \"Whether to pad all samples to `max_seq_length`. \"\n",
    "            \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n",
    "        },\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    max_predict_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n",
    "            \"value if set.\"\n",
    "        },\n",
    "    )\n",
    "    train_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the training data.\"}\n",
    "    )\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"A csv or a json file containing the validation data.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: str = field(\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class OptimizationArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what type of optimization we are going to apply on the model.\n",
    "    \"\"\"\n",
    "\n",
    "    prune: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether or not to apply prune.\"},\n",
    "    )\n",
    "    pruning_approach: Optional[str] = field(\n",
    "        default=\"BasicMagnitude\",\n",
    "        metadata={\"help\": \"Pruning approach. Supported approach is basic_magnite.\"},\n",
    "    )\n",
    "    target_sparsity_ratio: Optional[float] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Targeted sparsity when pruning the model.\"},\n",
    "    )\n",
    "    metric_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Metric used for the tuning strategy.\"},\n",
    "    )\n",
    "    tolerance_mode: Optional[str] = field(\n",
    "        default=\"absolute\",\n",
    "        metadata={\"help\": \"Metric tolerance model, expected to be relative or absolute.\"},\n",
    "    )\n",
    "    tune: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether or not to apply quantization.\"},\n",
    "    )\n",
    "    quantization_approach: Optional[str] = field(\n",
    "        default=\"PostTrainingStatic\",\n",
    "        metadata={\"help\": \"Quantization approach. Supported approach are PostTrainingStatic, \"\n",
    "                  \"PostTrainingDynamic and QuantizationAwareTraining.\"},\n",
    "    )\n",
    "    metric_name: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"Metric used for the tuning strategy.\"},\n",
    "    )\n",
    "    is_relative: Optional[bool] = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Metric tolerance model, expected to be relative or absolute.\"},\n",
    "    )\n",
    "    perf_tol: Optional[float] = field(\n",
    "        default=0.01,\n",
    "        metadata={\"help\": \"Performance tolerance when optimizing the model.\"},\n",
    "    )\n",
    "    int8: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\":\"run benchmark.\"}\n",
    "    )\n",
    "    distillation: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Whether or not to apply distillation.\"},\n",
    "    )\n",
    "    teacher_model_name_or_path: str = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n",
    "    )\n",
    "    orchestrate_optimizations: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\":\"for one shot.\"}\n",
    "    )\n",
    "    benchmark: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"run benchmark.\"}\n",
    "    )\n",
    "    accuracy_only: bool = field(\n",
    "        default=False,\n",
    "        metadata={\"help\":\"Whether to only test accuracy for model tuned by Neural Compressor.\"}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"distilbert-base-uncased-distilled-squad\",\n",
    ")\n",
    "data_args = DataTrainingArguments(\n",
    "    dataset_name=\"squad\",\n",
    "    max_seq_length=384,\n",
    "    max_eval_samples=5000\n",
    ")\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tmp/squad_output\",\n",
    "    do_eval=True,\n",
    "    do_train=True,\n",
    "    no_cuda=True,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=8,\n",
    ")\n",
    "optim_args = OptimizationArguments(\n",
    "    tune=True,\n",
    "    quantization_approach=\"PostTrainingStatic\"\n",
    ")\n",
    "log_level = training_args.get_process_log_level()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset from the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\n",
    "    data_args.dataset_name, data_args.dataset_config_name, cache_dir=model_args.cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download fp32 model from the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# get fp32 model\n",
    "config = AutoConfig.from_pretrained(model_args.model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_args.model_name_or_path, use_fast=True)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=config,\n",
    "    use_auth_token=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the datasets.\n",
    "# Preprocessing is slighlty different for training and evaluation.\n",
    "if training_args.do_train:\n",
    "    column_names = raw_datasets[\"train\"].column_names\n",
    "elif training_args.do_eval:\n",
    "    column_names = raw_datasets[\"validation\"].column_names\n",
    "else:\n",
    "    column_names = raw_datasets[\"test\"].column_names\n",
    "question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n",
    "\n",
    "# Padding side determines if we do (question|context) or (context|question).\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "\n",
    "if data_args.max_seq_length > tokenizer.model_max_length:\n",
    "    logger.warning(\n",
    "        f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the\"\n",
    "        f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n",
    "    )\n",
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "# Training preprocessing\n",
    "def prepare_train_features(examples, tokenizer=tokenizer):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=data_args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "if training_args.do_train:\n",
    "    if \"train\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_examples = raw_datasets[\"train\"]\n",
    "    if data_args.max_train_samples is not None:\n",
    "        # We will select sample from whole data if argument is specified\n",
    "        train_dataset = train_examples.select(range(data_args.max_train_samples))\n",
    "    # Create train feature from dataset\n",
    "    with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "        train_dataset = train_examples.map(\n",
    "            prepare_train_features,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on train dataset\",\n",
    "        )\n",
    "    if data_args.max_train_samples is not None:\n",
    "        # Number of samples might increase during Feature Creation, We select only specified max samples\n",
    "        train_dataset = train_dataset.select(range(data_args.max_train_samples))\n",
    "\n",
    "# Validation preprocessing\n",
    "def prepare_validation_features(examples, tokenizer=tokenizer):\n",
    "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
    "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
    "    # left whitespace\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=data_args.doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\" if data_args.pad_to_max_length else False,\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "    # corresponding example_id and we will store the offset mappings.\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "if training_args.do_eval:\n",
    "    if \"validation\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    eval_examples = raw_datasets[\"validation\"]\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        # We will select sample from whole data\n",
    "        eval_examples = eval_examples.select(range(data_args.max_eval_samples))\n",
    "    # Validation Feature Creation\n",
    "    with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "        eval_dataset = eval_examples.map(\n",
    "            prepare_validation_features,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on validation dataset\",\n",
    "        )\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        # During Feature creation dataset samples might increase, we will select required samples again\n",
    "        eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n",
    "\n",
    "if training_args.do_predict:\n",
    "    if \"test\" not in raw_datasets:\n",
    "        raise ValueError(\"--do_predict requires a test dataset\")\n",
    "    predict_examples = raw_datasets[\"test\"]\n",
    "    if data_args.max_predict_samples is not None:\n",
    "        # We will select sample from whole data\n",
    "        predict_examples = predict_examples.select(range(data_args.max_predict_samples))\n",
    "    # Predict Feature Creation\n",
    "    with training_args.main_process_first(desc=\"prediction dataset map pre-processing\"):\n",
    "        predict_dataset = predict_examples.map(\n",
    "            prepare_validation_features,\n",
    "            batched=True,\n",
    "            num_proc=data_args.preprocessing_num_workers,\n",
    "            remove_columns=column_names,\n",
    "            load_from_cache_file=not data_args.overwrite_cache,\n",
    "            desc=\"Running tokenizer on prediction dataset\",\n",
    "        )\n",
    "    if data_args.max_predict_samples is not None:\n",
    "        # During Feature creation dataset samples might increase, we will select required samples again\n",
    "        predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n",
    "\n",
    "# Data collator\n",
    "# We have already padded to max length if the corresponding flag is True, otherwise we need to pad in the data\n",
    "# collator.\n",
    "data_collator = (\n",
    "    default_data_collator\n",
    "    if data_args.pad_to_max_length\n",
    "    else DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n",
    ")\n",
    "\n",
    "# Post-processing:\n",
    "def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "    # Post-processing: we match the start logits and end logits to answers in the original context.\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=data_args.version_2_with_negative,\n",
    "        n_best_size=data_args.n_best_size,\n",
    "        max_answer_length=data_args.max_answer_length,\n",
    "        null_score_diff_threshold=data_args.null_score_diff_threshold,\n",
    "        output_dir=training_args.output_dir,\n",
    "        log_level=log_level,\n",
    "        prefix=stage,\n",
    "    )\n",
    "    # Format the result to the format the metric expects.\n",
    "    if data_args.version_2_with_negative:\n",
    "        formatted_predictions = [\n",
    "            {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n",
    "        ]\n",
    "    else:\n",
    "        formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if data_args.version_2_with_negative else \"squad\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare datasets for teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_config = AutoConfig.from_pretrained(\n",
    "    optim_args.teacher_model_name_or_path,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    optim_args.teacher_model_name_or_path,\n",
    "    use_fast=True,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "teacher_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    optim_args.teacher_model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_args.model_name_or_path),\n",
    "    config=teacher_config,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "teacher_model.to(training_args.device)\n",
    "\n",
    "# Prepare datasets for teacher model\n",
    "# Create train feature from dataset\n",
    "with training_args.main_process_first(desc=\"train dataset map pre-processing\"):\n",
    "    teacher_train_dataset = train_examples.map(\n",
    "        functools.partial(prepare_train_features, tokenizer=teacher_tokenizer),\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on train dataset\",\n",
    "    )\n",
    "teacher_train_dataset = teacher_train_dataset.select(range(data_args.max_train_samples))\n",
    "\n",
    "# Validation Feature Creation\n",
    "with training_args.main_process_first(desc=\"validation dataset map pre-processing\"):\n",
    "    teacher_eval_dataset = eval_examples.map(\n",
    "        functools.partial(prepare_validation_features, tokenizer=teacher_tokenizer),\n",
    "        batched=True,\n",
    "        num_proc=data_args.preprocessing_num_workers,\n",
    "        remove_columns=column_names,\n",
    "        load_from_cache_file=not data_args.overwrite_cache,\n",
    "        desc=\"Running tokenizer on validation dataset\",\n",
    "    )\n",
    "teacher_eval_dataset = teacher_eval_dataset.select(range(data_args.max_eval_samples))\n",
    "    \n",
    "# get logits of teacher model\n",
    "if optim_args.run_teacher_logits:\n",
    "    def dict_tensor_to_model_device(batch, model):\n",
    "        device = next(model.parameters()).device\n",
    "        for k in batch:\n",
    "            batch[k] = batch[k].to(device)\n",
    "\n",
    "    def get_logits(teacher_model, train_dataset, teacher_train_dataset):\n",
    "        logger.info(\"***** Getting logits of teacher model *****\")\n",
    "        logger.info(f\"  Num examples = {len(train_dataset) }\")\n",
    "        teacher_model.eval()\n",
    "        npy_file = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n",
    "            '{}.{}.npy'.format(data_args.dataset_name, \n",
    "                            optim_args.teacher_model_name_or_path.replace('/', '.')))\n",
    "        if os.path.exists(npy_file):\n",
    "            teacher_logits = [list(x) for x in np.load(npy_file, allow_pickle=True)]\n",
    "        else:\n",
    "            sampler = None\n",
    "            if training_args.world_size > 1:\n",
    "                from transformers.trainer_pt_utils import ShardSampler\n",
    "                sampler = ShardSampler(\n",
    "                    teacher_train_dataset,\n",
    "                    batch_size=training_args.per_device_eval_batch_size,\n",
    "                    num_processes=training_args.world_size,\n",
    "                    process_index=training_args.process_index,\n",
    "                )\n",
    "                teacher_model = torch.nn.parallel.DistributedDataParallel(\n",
    "                    teacher_model,\n",
    "                    device_ids=[training_args.local_rank] \\\n",
    "                        if training_args._n_gpu != 0 else None,\n",
    "                    output_device=training_args.local_rank \\\n",
    "                        if training_args._n_gpu != 0 else None,\n",
    "                )\n",
    "            train_dataloader = DataLoader(teacher_train_dataset, \n",
    "                                        collate_fn=data_collator,  \n",
    "                                        sampler=sampler,\n",
    "                                        batch_size=training_args.per_device_eval_batch_size)\n",
    "            train_dataloader = tqdm(train_dataloader, desc=\"Evaluating\")\n",
    "            teacher_logits = []\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                dict_tensor_to_model_device(batch, teacher_model)\n",
    "                outputs = teacher_model(**batch).cpu().numpy()\n",
    "                if training_args.world_size > 1:\n",
    "                    outputs_list = [None for i in range(training_args.world_size)]\n",
    "                    torch.distributed.all_gather_object(outputs_list, outputs)\n",
    "                    outputs = np.concatenate(outputs_list, axis=0)\n",
    "                teacher_logits += [[s,e] for s,e in zip(outputs[0::2], outputs[1::2])]\n",
    "            if training_args.world_size > 1:\n",
    "                teacher_logits = teacher_logits[:len(teacher_train_dataset)]\n",
    "            if training_args.local_rank in [-1, 0]:\n",
    "                np.save(npy_file, teacher_logits, allow_pickle=True)\n",
    "        return train_dataset.add_column('teacher_logits', teacher_logits[:data_args.max_train_samples])\n",
    "    with torch.no_grad():\n",
    "        train_dataset = get_logits(QAModel_output_reshaped(teacher_model), train_dataset, teacher_train_dataset)\n",
    "        \n",
    "para_counter = lambda model:sum(p.numel() for p in model.parameters())\n",
    "logger.info(\"***** Number of teacher model parameters: {:.2f}M *****\".format(\\\n",
    "            para_counter(teacher_model)/10**6))\n",
    "logger.info(\"***** Number of student model parameters: {:.2f}M *****\".format(\\\n",
    "            para_counter(model)/10**6))\n",
    "\n",
    "# Trace model\n",
    "from neural_compressor.adaptor.torch_utils.symbolic_trace import symbolic_trace\n",
    "model = symbolic_trace(model, optim_args.quantization_approach==\"QuantizationAwareTraining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrate Optimizations & Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrate Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)\n",
    "# Initialize our Trainer\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset if training_args.do_train else None,\n",
    "    eval_dataset=eval_dataset if training_args.do_eval else None,\n",
    "    eval_examples=eval_examples if training_args.do_eval else None,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    post_process_function=post_processing_function,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metric_name = optim_args.metric_name\n",
    "\n",
    "if optim_args.orchestrate_optimizations:\n",
    "\n",
    "    if not training_args.do_train:\n",
    "        raise ValueError(\"do_train must be set to True for pruning.\")\n",
    "\n",
    "    tune_metric = metrics.Metric(\n",
    "        name=metric_name, is_relative=optim_args.is_relative, criterion=optim_args.perf_tol\n",
    "    )\n",
    "    prune_type = 'PatternLock' \\\n",
    "        if optim_args.pruning_approach else optim_args.pruning_approach\n",
    "    target_sparsity_ratio = optim_args.target_sparsity_ratio \\\n",
    "        if optim_args.target_sparsity_ratio else None\n",
    "    pruner_config = PrunerConfig(prune_type=prune_type, target_sparsity_ratio=target_sparsity_ratio)\n",
    "    pruning_conf = PruningConfig(framework=\"pytorch_fx\",pruner_config=[pruner_config], metrics=tune_metric)\n",
    "    distillation_conf = DistillationConfig(framework=\"pytorch_fx\", metrics=tune_metric)\n",
    "\n",
    "    objective = objectives.performance\n",
    "    quantization_conf = QuantizationConfig(\n",
    "        approach=optim_args.quantization_approach,\n",
    "        max_trials=600,\n",
    "        metrics=[tune_metric],\n",
    "        objectives=[objective]\n",
    "    )\n",
    "    conf_list = [pruning_conf, distillation_conf, quantization_conf]\n",
    "    model = trainer.orchestrate_optimizations(config_list=conf_list, teacher_model=teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark after Orchestrate Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(training_args.seed)\n",
    "start_time = timeit.default_timer()\n",
    "results = trainer.evaluate()\n",
    "evalTime = timeit.default_timer() - start_time\n",
    "max_eval_samples = data_args.max_eval_samples\n",
    "samples = min(max_eval_samples, len(eval_dataset))\n",
    "\n",
    "eval_f1_dynamic = results.get(\"eval_f1\")\n",
    "print('Batch size = {}'.format(training_args.per_device_eval_batch_size))\n",
    "print(\"Finally Eval eval_f1 Accuracy: {}\".format(eval_f1_dynamic))\n",
    "print(\"Latency: {:.3f} ms\".format(evalTime / samples * 1000))\n",
    "print(\"Throughput: {} samples/sec\".format(samples/evalTime))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f54fd8d6160ddfbc370985ee3ad2925997e28943a671b1747496a6859c59cd26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
