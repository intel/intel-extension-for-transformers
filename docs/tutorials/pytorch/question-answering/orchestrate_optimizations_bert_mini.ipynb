{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is used to list steps of introducing [Prune Once For All](https://arxiv.org/abs/2111.05754) examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Follow [installation](https://github.com/intel-innersource/frameworks.ai.nlp-toolkit.intel-nlp-toolkit#installation) to install **intel-extension-for-transformers**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets>=1.8.0 torch>=1.10.0 transformers>=4.12.0 wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from dataclasses import dataclass, field\n",
    "import timeit\n",
    "from datasets import load_dataset, load_metric\n",
    "import functools\n",
    "from trainer_qa import QuestionAnsweringTrainer\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from transformers.utils import check_min_version\n",
    "from transformers.utils.versions import require_version\n",
    "from typing import Optional\n",
    "from utils_qa import postprocess_qa_predictions\n",
    "\n",
    "\n",
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "check_min_version(\"4.12.0\")\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/question-answering/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset from the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download fp32 Model from the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = 'Intel/distilbert-base-uncased-sparse-90-unstructured-pruneofa'\n",
    "config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    config=config,\n",
    "    use_auth_token=None\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = raw_datasets[\"train\"].column_names\n",
    "question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]    \n",
    "\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "max_seq_length = min(384, tokenizer.model_max_length)\n",
    "\n",
    "def prepare_train_features(examples, tokenizer=tokenizer):\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[answer_column_name][sample_index]\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "train_examples = raw_datasets[\"train\"]\n",
    "train_dataset = train_examples.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    num_proc=None,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset for Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = raw_datasets[\"validation\"].column_names\n",
    "question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n",
    "context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n",
    "answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]    \n",
    "\n",
    "pad_on_right = tokenizer.padding_side == \"right\"\n",
    "max_seq_length = min(384, tokenizer.model_max_length)\n",
    "\n",
    "def prepare_validation_features(examples, tokenizer=tokenizer):\n",
    "    examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n",
    "\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[question_column_name if pad_on_right else context_column_name],\n",
    "        examples[context_column_name if pad_on_right else question_column_name],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "eval_examples = raw_datasets[\"validation\"]\n",
    "eval_dataset = eval_examples.map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    num_proc=None,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on validation dataset\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = (\n",
    "    DataCollatorWithPadding(tokenizer, None)\n",
    ")\n",
    "\n",
    "def post_processing_function(examples, features, predictions, stage=\"eval\"):\n",
    "    predictions = postprocess_qa_predictions(\n",
    "        examples=examples,\n",
    "        features=features,\n",
    "        predictions=predictions,\n",
    "        version_2_with_negative=False,\n",
    "        n_best_size=20,\n",
    "        max_answer_length=30,\n",
    "        null_score_diff_threshold=0.0,\n",
    "        output_dir=\"./tmp/squad_output\",\n",
    "        log_level='passive'\n",
    "        prefix=stage,\n",
    "    )\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n",
    "\n",
    "    references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n",
    "    return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n",
    "\n",
    "metric = load_metric(\"squad\")\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    return metric.compute(predictions=p.predictions, references=p.label_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets for Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_name_or_path = \"distilbert-base-uncased-distilled-squad\"\n",
    "teacher_config = AutoConfig.from_pretrained(\n",
    "    teacher_model_name_or_path,\n",
    "    use_auth_token=True\n",
    ")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    teacher_model_name_or_path,\n",
    "    use_fast=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "teacher_model = AutoModelForQuestionAnswering.from_pretrained(\n",
    "    teacher_model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    config=teacher_config,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "teacher_train_dataset = train_examples.map(\n",
    "    functools.partial(prepare_train_features, tokenizer=teacher_tokenizer),\n",
    "    batched=True,\n",
    "    num_proc=None,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on train dataset\",\n",
    ")\n",
    "\n",
    "teacher_eval_dataset = eval_examples.map(\n",
    "    functools.partial(prepare_validation_features, tokenizer=teacher_tokenizer),\n",
    "    batched=True,\n",
    "    num_proc=None,\n",
    "    remove_columns=column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on validation dataset\",\n",
    ")\n",
    "    \n",
    "\n",
    "para_counter = lambda model:sum(p.numel() for p in model.parameters())\n",
    "logger.info(\"***** Number of teacher model parameters: {:.2f}M *****\".format(\\\n",
    "            para_counter(teacher_model)/10**6))\n",
    "logger.info(\"***** Number of student model parameters: {:.2f}M *****\".format(\\\n",
    "            para_counter(model)/10**6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrate Optimizations & Benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrate Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "trainer = QuestionAnsweringTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    eval_examples=eval_examples,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    post_process_function=post_processing_function,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metric_name = \"eval_f1\"\n",
    "\n",
    "tune_metric = metrics.Metric(\n",
    "    name=metric_name, is_relative=True, criterion=0.01\n",
    ")\n",
    "\n",
    "target_sparsity_ratio = None\n",
    "pruner_config = PrunerConfig(prune_type='PatternLock', target_sparsity_ratio=None)\n",
    "pruning_conf = PruningConfig(framework=\"pytorch_fx\",pruner_config=[pruner_config], metrics=tune_metric)\n",
    "distillation_conf = DistillationConfig(framework=\"pytorch_fx\", metrics=tune_metric)\n",
    "\n",
    "objective = objectives.performance\n",
    "quantization_conf = QuantizationConfig(\n",
    "    approach=\"QuantizationAwareTraining\",\n",
    "    max_trials=600,\n",
    "    metrics=[tune_metric],\n",
    "    objectives=[objective]\n",
    ")\n",
    "conf_list = [pruning_conf, distillation_conf, quantization_conf]\n",
    "model = trainer.orchestrate_optimizations(config_list=conf_list, teacher_model=teacher_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark after Orchestrate Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "results = trainer.evaluate()\n",
    "evalTime = timeit.default_timer() - start_time\n",
    "samples = len(eval_dataset)\n",
    "eval_f1_dynamic = results.get(\"eval_f1\")\n",
    "print('Batch size = {}'.format(8))\n",
    "print(\"Finally Eval eval_f1 Accuracy: {}\".format(eval_f1_dynamic))\n",
    "print(\"Latency: {:.3f} ms\".format(evalTime / samples * 1000))\n",
    "print(\"Throughput: {} samples/sec\".format(samples/evalTime))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
