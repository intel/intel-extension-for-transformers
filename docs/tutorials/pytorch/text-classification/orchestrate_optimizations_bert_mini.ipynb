{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial is used to list steps of introducing [Prune Once For All](https://arxiv.org/abs/2111.05754) examples."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Follow [installation](https://github.com/intel-innersource/frameworks.ai.nlp-toolkit.intel-nlp-toolkit#installation) to install **intel-extension-for-transformers**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate torch>=1.10 datasets>=1.1.3 sentencepiece!=0.1.92 transformers>=4.12.0 protobuf wandb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from dataclasses import dataclass, field\n",
    "from datasets import load_dataset, load_metric\n",
    "from intel_extension_for_transformers.optimization import (\n",
    "    metrics,\n",
    "    PrunerConfig,\n",
    "    PruningConfig,\n",
    "    DistillationConfig,\n",
    "    QuantizationConfig,\n",
    "    objectives\n",
    ")\n",
    "from intel_extension_for_transformers.optimization.trainer import NLPTrainer\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    EvalPrediction,\n",
    "    PretrainedConfig\n",
    ")\n",
    "from transformers.utils import check_min_version\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "check_min_version(\"4.12.0\")\n",
    "\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Dataset from the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"glue\", \"sst2\")\n",
    "label_list = raw_datasets[\"train\"].features[\"label\"].names\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download fp32 Model from the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"Intel/distilbert-base-uncased-sparse-90-unstructured-pruneofa\"\n",
    "config = AutoConfig.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=num_labels,\n",
    "    finetuning_task=\"sst2\",\n",
    "    revision=\"main\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    use_fast=True,\n",
    "    revision=\"main\"\n",
    ")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    from_tf=bool(\".ckpt\" in model_name_or_path),\n",
    "    config=config,\n",
    "    revision=\"main\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n",
    "if is_regression:\n",
    "    num_labels = 1\n",
    "else:\n",
    "    label_list = raw_datasets[\"train\"].unique(\"label\")\n",
    "    label_list.sort()\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[\"sst2\"]\n",
    "\n",
    "label_to_id = None\n",
    "if model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id \\\n",
    "    and not is_regression:\n",
    "    label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n",
    "    label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n",
    "\n",
    "if label_to_id is not None:\n",
    "    model.config.label2id = label_to_id\n",
    "    model.config.id2label = {id: label for label, id in config.label2id.items()}\n",
    "\n",
    "max_seq_length = min(128, tokenizer.model_max_length)\n",
    "\n",
    "\n",
    "def preprocess_function(examples, tokenizer=tokenizer):\n",
    "    args = (\n",
    "        (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n",
    "    )\n",
    "    result = tokenizer(*args, padding=False, max_length=max_seq_length, truncation=True)\n",
    "\n",
    "    # Map labels to IDs (not necessary for GLUE tasks)\n",
    "    if label_to_id is not None and \"label\" in examples:\n",
    "        result[\"label\"] = [(label_to_id[l] if l != -1 else -1) for l in examples[\"label\"]]\n",
    "    return result\n",
    "\n",
    "\n",
    "raw_datasets = raw_datasets.map(\n",
    "    preprocess_function, batched=True, load_from_cache_file=False\n",
    ")\n",
    "\n",
    "train_dataset = raw_datasets[\"train\"]\n",
    "eval_dataset = raw_datasets[\"validation\"]\n",
    "metric = load_metric(\"glue\", \"sst2\")\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n",
    "    result = metric.compute(predictions=preds, references=p.label_ids)\n",
    "    \n",
    "    if len(result) > 1:\n",
    "        result[\"combined_score\"] = np.mean(list(result.values())).item()\n",
    "        return result\n",
    "    \n",
    "    if is_regression:\n",
    "        return {\"mse\": ((preds - p.label_ids) ** 2).mean().item()}\n",
    "    else:\n",
    "        return {\"accuracy\": (preds == p.label_ids).astype(np.float32).mean().item()}\n",
    "\n",
    "\n",
    "data_collator = (\n",
    "    DataCollatorWithPadding(tokenizer, None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets for Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model_name_or_path = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "teacher_config = AutoConfig.from_pretrained(teacher_model_name_or_path, \\\n",
    "                        num_labels=num_labels, finetuning_task=\"sst2\")\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name_or_path, \\\n",
    "                    use_fast=True)\n",
    "teacher_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                    teacher_model_name_or_path,\n",
    "                    from_tf=bool(\".ckpt\" in teacher_model_name_or_path),\n",
    "                    config=teacher_config,\n",
    ")\n",
    "teacher_processed_datasets = raw_datasets.map(\n",
    "    functools.partial(preprocess_function, tokenizer=teacher_tokenizer), \n",
    "    batched=True, remove_columns=raw_datasets[\"train\"].column_names\n",
    ")\n",
    "teacher_train_dataset = teacher_processed_datasets[\"train\"]\n",
    "teacher_eval_dataset = teacher_processed_datasets[\"validation\"]\n",
    "\n",
    "\n",
    "def dict_tensor_to_model_device(batch, model):\n",
    "    device = next(model.parameters()).device\n",
    "    for k in batch:\n",
    "        batch[k] = batch[k].to(device)\n",
    "\n",
    "\n",
    "def get_logits(teacher_model, train_dataset, teacher_train_dataset):\n",
    "    logger.info(\"***** Getting logits of teacher model *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset) }\")\n",
    "    teacher_model.eval()\n",
    "    npy_file = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n",
    "        '{}.{}.npy'.format(\"sst2\", teacher_model_name_or_path.replace('/', '.')))\n",
    "    if os.path.exists(npy_file):\n",
    "        teacher_logits = [x for x in np.load(npy_file)]\n",
    "    return train_dataset.add_column('teacher_logits', teacher_logits)\n",
    "\n",
    "\n",
    "class BertModelforLogitsOutputOnly(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(BertModelforLogitsOutputOnly, self).__init__()\n",
    "        self.model = model\n",
    "    def forward(self, *args, **kwargs):\n",
    "        output = self.model(*args, **kwargs)\n",
    "        return output['logits']\n",
    "    \n",
    "\n",
    "with torch.no_grad():\n",
    "    train_dataset = get_logits(BertModelforLogitsOutputOnly(teacher_model), train_dataset, teacher_train_dataset)\n",
    "\n",
    "para_counter = lambda model:sum(p.numel() for p in model.parameters())\n",
    "logger.info(\"***** Number of teacher model parameters: {:.2f}M *****\".format(\\\n",
    "            para_counter(teacher_model)/10**6))\n",
    "logger.info(\"***** Number of student model parameters: {:.2f}M *****\".format(\\\n",
    "            para_counter(model)/10**6))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrate Optimizations & Benchmark"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrate Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = NLPTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "metric_name = \"eval_accuracy\"\n",
    "\n",
    "tune_metric = metrics.Metric(\n",
    "    name=metric_name, is_relative=True, criterion=0.01\n",
    ")\n",
    "\n",
    "target_sparsity_ratio = None\n",
    "pruner_config = PrunerConfig(prune_type='PatternLock', target_sparsity_ratio=None)\n",
    "pruning_conf = PruningConfig(framework=\"pytorch_fx\",pruner_config=[pruner_config], metrics=tune_metric)\n",
    "distillation_conf = DistillationConfig(framework=\"pytorch_fx\", metrics=tune_metric)\n",
    "\n",
    "objective = objectives.performance\n",
    "quantization_conf = QuantizationConfig(\n",
    "    approach=\"QuantizationAwareTraining\",\n",
    "    max_trials=600,\n",
    "    metrics=[tune_metric],\n",
    "    objectives=[objective]\n",
    ")\n",
    "conf_list = [pruning_conf, distillation_conf, quantization_conf]\n",
    "model = trainer.orchestrate_optimizations(config_list=conf_list, teacher_model=teacher_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark after Orchestrate Optimizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = trainer.evaluate()\n",
    "throughput = results.get(\"eval_samples_per_second\")\n",
    "eval_acc = results.get(\"eval_accuracy\")\n",
    "print('Batch size = {}'.format(8))\n",
    "print(\"Finally Eval eval_accuracy Accuracy: {:.5f}\".format(eval_acc))\n",
    "print(\"Latency: {:.5f} ms\".format(1000 / throughput))\n",
    "print(\"Throughput: {:.5f} samples/sec\".format(throughput))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
