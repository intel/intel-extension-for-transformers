{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TangoBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reproduces results from [TangoBERT paper](http://arxiv.org/abs/2204.06271).\n",
    "\n",
    "TangoBERT is a cascaded model architecture in\n",
    "which instances are first processed by an efficient but less accurate first tier model, and\n",
    "only part of those instances are additionally\n",
    "processed by a less efficient but more accurate\n",
    "second tier model. The decision of whether to\n",
    "apply the second tier model is based on a confidence score produced by the first tier model. \n",
    "Instance are first\n",
    "processed by the efficient first tier model. If this model\n",
    "is confident regarding its prediction, it produces the final prediction. Otherwise, the more accurate second\n",
    "tier model processes the instance and produces the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch numpy transformers datasets evaluate argparse-range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run TangoBERT on SST2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "from time import perf_counter\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from argparse_range import range_action\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\"),\n",
    "}\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"TangoBERT\")\n",
    "    parser.add_argument(\n",
    "        \"--task_name\",\n",
    "        type=str,\n",
    "        help=\"Name of the GLUE task.\",\n",
    "        choices=list(task_to_keys.keys()),\n",
    "        default=\"sst2\"\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--small_model_name_or_path\",\n",
    "        type=str,\n",
    "        help=\"Path to the small pretrained model or model identifier from huggingface.co/models.\",\n",
    "        default=\"philschmid/tiny-bert-sst2-distilled\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--big_model_name_or_path\",\n",
    "        type=str,\n",
    "        help=\"Path to the big pretrained model or model identifier from huggingface.co/models.\",\n",
    "        default=\"textattack/roberta-base-SST-2\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_small\",\n",
    "        default=0,\n",
    "        help=\"Defines the device (e.g., \\\"cpu\\\", \\\"cuda:1\\\", \\\"mps\\\", or a GPU ordinal rank like 1) on which  \"\n",
    "             \"small model pipeline will be allocated. \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--device_big\",\n",
    "        default=0,\n",
    "        help=\"Defines the device (e.g., \\\"cpu\\\", \\\"cuda:1\\\", \\\"mps\\\", or a GPU ordinal rank like 1) on which this \"\n",
    "             \"big model pipeline will be allocated. \"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_eval_batch_size-small\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Batch size (per device) for small model inference.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--per_device_eval_batch_size-big\",\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help=\"Batch size (per device) for big model inference.\",\n",
    "    )\n",
    "    parser.add_argument(\"--confidence_threshold\",\n",
    "                        type=float,\n",
    "                        default=0.9,\n",
    "                        action=range_action(0.5, 1.0),\n",
    "                        help=\"Confidence threshold for small model prediction (must be in range 0.5..1.0).\"\n",
    "                        )\n",
    "    args, _ = parser.parse_known_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parse_args()\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "\n",
    "logger.info(args)\n",
    "\n",
    "logger.info(\"Prepare dataset\")\n",
    "eval_dataset = load_dataset(\"glue\", args.task_name)[\n",
    "    \"validation_matched\" if args.task_name == \"mnli\" else \"validation\"]\n",
    "\n",
    "sentence1_key, sentence2_key = task_to_keys[args.task_name]\n",
    "texts = (\n",
    "    (eval_dataset[sentence1_key],) if sentence2_key is None else (\n",
    "        eval_dataset[sentence1_key], eval_dataset[sentence2_key])\n",
    ")\n",
    "\n",
    "logger.info(\"Loading small model\")\n",
    "small_model = AutoModelForSequenceClassification.from_pretrained(args.small_model_name_or_path)\n",
    "small_tokenizer = AutoTokenizer.from_pretrained(args.small_model_name_or_path)\n",
    "pipe_small = pipeline(\"text-classification\", model=small_model, tokenizer=small_tokenizer, device=args.device_small)\n",
    "\n",
    "logger.info(\"Loading big model\")\n",
    "big_model = AutoModelForSequenceClassification.from_pretrained(args.big_model_name_or_path)\n",
    "big_tokenizer = AutoTokenizer.from_pretrained(args.big_model_name_or_path)\n",
    "pipe_big = pipeline(\"text-classification\", model=big_model, tokenizer=big_tokenizer, device=args.device_big)\n",
    "\n",
    "logger.info(\"Inference\")\n",
    "start_time = perf_counter()\n",
    "output_small = pipe_small(*texts, batch_size=args.per_device_eval_batch_size_small)\n",
    "low_confidence_indices = [idx for idx, pred in enumerate(output_small) if\n",
    "                            np.max(pred['score']) < args.confidence_threshold]\n",
    "if len(low_confidence_indices) > 0:\n",
    "    low_confidence_subset = np.asarray(*texts)[low_confidence_indices].tolist()\n",
    "    output_big = pipe_big(low_confidence_subset, batch_size=args.per_device_eval_batch_size_big)\n",
    "end_time = perf_counter()\n",
    "\n",
    "logger.info(\"Evaluation\")\n",
    "high_confidence_indices = [idx for idx, pred in enumerate(output_small) if\n",
    "                            np.max(pred['score']) >= args.confidence_threshold]\n",
    "predictions = np.empty([len(output_small)])\n",
    "predictions[high_confidence_indices] = [small_model.config.label2id[o['label']] for o in\n",
    "                                        np.asarray(output_small)[high_confidence_indices]]\n",
    "if len(low_confidence_indices) > 0:\n",
    "    predictions[low_confidence_indices] = [big_model.config.label2id[o['label']] for o in output_big]\n",
    "\n",
    "metric = evaluate.load(\"glue\", args.task_name)\n",
    "metric.add_batch(predictions=predictions, references=eval_dataset['label'])\n",
    "eval_metric = metric.compute()\n",
    "logger.info(f\"eval_metric: {eval_metric}\")\n",
    "inference_time = round((end_time - start_time) * 1000, 0)\n",
    "logger.info(f\"inference time:  {inference_time} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n",
    "```\n",
    "@inproceedings{mamou2022tangobert,\n",
    "    title={TangoBERT: Reducing Inference Cost by using Cascaded Architecture},\n",
    "    author={Jonathan Mamou and Oren Pereg and Moshe Wasserblat and Roy Schwartz},\n",
    "    booktitle = \"Energy Efficient Training and Inference of Transformer Based Models workshop, AAAI Conference on Artificial Intelligence,\n",
    "    url = \"http://arxiv.org/abs/2204.06271\",\n",
    "    year = {2023}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
