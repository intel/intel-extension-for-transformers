# Docs
These documents provide a detailed description of the functionalities of Intel Extension for Transformers.
[architecture.md](architecture.md) offers an overview of Intel Extension for Transformers, encompassing various architectural levels.

You can fine install guidelines in [installation.md](installation.md).

Optimization docs: [quantization.md](quantization.md), [pruning.md](pruning.md), [autodistillation.md](autodistillation.md), [distillation.md](distillation.md), 

[export.md](export.md) provides PyTorch to onnx including int8 model.

[get_started.md](get_started.md) guides you to use optimization API and sparse inference. We also provide examples in [examples.md](examples.md). P.S, smoothquant, woq and cpp inference are not in these documents.


These utils may help you:
[data_augmentation.md](data_augmentation.md) does NLP datasets augmentation.
[benchmark.md](benchmark.md) is used to measure the model performance. 

You are invited to commit your code; kindly adhere to the guidelines specified in [contributions.md](contributions.md) and maintain a positive demeanor as outlined in [code_of_conduct.md](code_of_conduct.md). The approvers for each component can be found in [component_owner.md](component_owner.md).
